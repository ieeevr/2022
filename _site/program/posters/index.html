<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.19.2 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->


<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Posters | IEEE VR 2022</title>
<meta name="description" content=" ">


  <meta name="author" content="IEEE VR">


<meta property="og:type" content="website">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="IEEE VR 2022">
<meta property="og:title" content="Posters">
<meta property="og:url" content="http://localhost:4000/program/posters/">


  <meta property="og:description" content=" ">





  <meta name="twitter:site" content="@ieeevr">
  <meta name="twitter:title" content="Posters">
  <meta name="twitter:description" content=" ">
  <meta name="twitter:url" content="http://localhost:4000/program/posters/">

  
    <meta name="twitter:card" content="summary">
    
  

  







  

  


<link rel="canonical" href="http://localhost:4000/program/posters/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "IEEE VR 2022",
      "url": "http://localhost:4000/"
    
  }
</script>






<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="IEEE VR 2022 Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



      <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->
<link rel="shortcut icon" type="image/png" href="https://www.ieeevr.org/2022/favicon.png"/> 

<!-- end custom head snippets -->

      
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
      
    <!--<script src="jquery.js"></script>-->
    
      
      <!--<link rel="stylesheet" href="styles.css">-->
      <script src="http://code.jquery.com/jquery-latest.min.js" type="text/javascript"></script>
      <!--<script src="script.js"></script>-->
      
  </head>
    
    

  <body class="layout--ieeevr-default">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    <!-- 

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/"><img src="/2022/favicon.png" alt=""></a>
        
        <a class="site-title" href="/">
          IEEE VR 2022
          <span class="site-subtitle"></span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/about/">About</a>
            </li><li class="masthead__menu-item">
              <a href="/contribute/">Contribute</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>
-->
      

      
      
    <div class="initial-content">
      <style>

    p {
        text-align: justify;
        text-justify: auto;
    }

    .topnav {
        #position: fixed;
        #top: 0px;
        #width: 100%;
        overflow: hidden;
        background-color: white;
        #border-bottom: 1px solid #00aeef;
    }

    .topnav a {
        float: left;
        display: block;
        color: black;
        text-align: center;
        padding: 14px 16px;
        text-decoration: none;
        font-size: 17px;
    }

    .active {
        background-color: white;
        color: black;
    }

    .topnav .icon {
        display: none;
    }

    .dropdown {
        float: left;
        overflow: hidden;
    }

    .dropdown .dropbtn {
        font-size: 17px;
        border: none;
        outline: none;
        color: black;
        padding: 16px 16px;
        background-color: inherit;
        font-family: inherit;
        margin: 0;
    }

    .dropdown-content {
        display: none;
        position: absolute;
        background-color: #fffbed;
        min-width: 160px;
        box-shadow: 0px 8px 16px 0px rgba(0, 0, 0, 0.2);
        z-index: 1;
    }

    .dropdown-content a {
        float: none;
        color: #363636;
        padding: 12px 16px;
        text-decoration: none;
        display: block;
        text-align: left;
    }

    .topnav a:hover,
    .dropdown:hover .dropbtn {
        background-color: #fec10d;
        color: #103b62;
    }

    .dropdown:hover .dropdown-content {
        display: block;
    }

    .dropdown:checked .dropdown-content {
        display: block;
    }

    @media screen and (max-width: 600px) {

        .topnav a:not(:first-child),
        .dropdown .dropbtn {
            display: none;
        }

        .topnav a.icon {
            float: right;
            display: block;
        }
    }

    @media screen and (max-width: 600px) {
        .topnav.responsive {
            position: relative;
        }

        .topnav.responsive .icon {
            position: absolute;
            right: 0;
            top: 0;
            height: inherit;
        }

        .topnav.responsive a {
            float: none;
            display: block;
            text-align: left;
        }

        .topnav.responsive .dropdown {
            float: none;
        }

        .topnav.responsive .dropdown-content {
            position: relative;
        }

        .topnav.responsive .dropdown .dropbtn {
            display: block;
            width: 100%;
            text-align: left;
        }
    }

    .ieeevrbanner {
        padding: 0px;
        border-radius: 7px;
    }

    .column {
        float: left;
        padding: 10px;
    }

    .left {
        width: 75%;
    }

    .center {
        width: 2%;
    }

    .right {
        width: 22%;
    }

    .row:after {
        content: "";
        display: table;
        clear: both;
    }

    .sponsorsend {
        display: block;
    }

    .ieeevrfooter {
        position: static;
        padding-top: 10px;
        bottom: 0;
        width: 100%;
        color: black;
        text-align: center;
        font-size: 16px;
    }

    .social-icons {
        text-align: center;
    }

    .social-icons li {
        display: inline-block;
        list-style-type: none;
        -webkit-user-select: none;
        -moz-user-select: none;
    }

    .social-icons li a {
        border-bottom: none;
    }

    .social-icons li img {
        width: 40px;
        height: 40px;
        margin-right: 30px;
    }

    .ieeevrmsgbox { // Any boxes with the theme-primary color as background
      background-color: #fec10d;
      border-radius: 7px;
    }

    .ieeevrmsgbox b {
        color: #363636;
    }

    .notice--info {
        background-color: $fffbed ! important;
        color: #363636 ! important;
    }

    .notice--text {
        background-color: $fffbed ! important;
        color: #363636 ! important;
    }

    @media screen and (max-width: 900px) {
        .left {
            width: 100%;
        }

        .center {
            display: none;
        }

        .right {
            width: 100%;
        }

        .sidebar-header-icon {
            display: none;
            padding: 0px 0px;
        }

        

        .conf-icon {
            width: 200px;
        }
    }

</style>



<button onclick="topFunction()" id="myBtnTop" title="Go to top">Top</button>

<script>
    var mybutton = document.getElementById("myBtnTop");

    window.onscroll = function() {
        scrollFunction()
    };

    function scrollFunction() {
        if (document.body.scrollTop > 100 || document.documentElement.scrollTop > 100) {
            mybutton.style.display = "block";
        } else {
            mybutton.style.display = "none";
        }
    }

    function topFunction() {
        document.body.scrollTop = 0;
        document.documentElement.scrollTop = 0;
    }


    function myFunction() {
        var x = document.getElementById("myTopnav");
        if (x.className === "topnav") {
            x.className += " responsive";
        } else {
            x.className = "topnav";
        }
    }

</script>



<div id="main" role="main">
    <article class="splash" style="margins:0px;" itemscope itemtype="https://schema.org/CreativeWork">



        <!-- navbar -->
        <div class="topnav" id="myTopnav">
            <a href=/ class="active">IEEE VR</a>
            <a href=/about>About</a>
            <!-- <a href="#">Online</a>  -->

            <div class="dropdown">
                <button class="dropbtn">Attend &#9662;</button>
                <div class="dropdown-content">
                    <a href=/attend/code-of-conduct/>Code of Conduct</a>
                    <a href=/attend/why-attend/>Why Attend</a>
                    <a href=/attend/registration/>Registration</a>
                    <a href=/attend/satellite-events>Satellite Events</a>
                    <a href=/attend/diversity-and-inclusion-scholarship/>Diversity and Inclusion Scholarship</a>
                    <a href=/attend/bridge-to-vr/>Bridge to VR</a>
                    <a href=/attend/mentorship-program/>Mentorship Program</a>
                    <a href=/attend/virbela-instructions/>Virbela Instructions</a>
                    <a href=/attend/zoom-instructions/>Zoom Instructions</a>
                    <!--<a href=/attend/accessibility-faq/>Accessibility FAQ</a>
                    <a href="http://www.lindeman.com/vr2021/live.shtml" target="_blank">Ready Player 21</a>-->
                </div>
            </div>
            
            <div class="dropdown">
                <button class="dropbtn">Program &#9662;</button>
                <div class="dropdown-content">
                    <a href=/program/overview/>Overview</a>
                    <a href=/program/keynote-speakers/>Keynote Speakers</a>
                    <a href=/program/papers/>Papers</a>
                    <a href=/program/workshops/>Workshops</a>
                    <a href=/program/tutorials/>Tutorials</a>
                    <a href=/program/panels/>Panels</a>
                    <a href=/program/3dui-contest/>3DUI Contest Entries</a>
                    <a href=/program/doctoral-consortium/>Doctoral Consortium</a>
                    <!--<a href=/program/exhibitors/>Exhibitors and Sponsors</a>
                    <a href=/program/plenary-sessions/>Plenary Sessions</a>
                    <a href=/program/posters/>Posters</a>
                    <a href=/program/demos/>Demos</a>
                    <a href=/program/videos/>Videos</a>
                    <a href=/program/bofs/>Birds of a Feather</a>-->

                </div>
            </div>
            
            <div class="dropdown">
                <button class="dropbtn">Contribute &#9662;</button>
                <div class="dropdown-content">
                    <a href=/contribute/exhibitors>Call for Exhibitors and Sponsors</a>
                    <a href=/contribute/>Call for Journal Papers</a>
                    <a href=/contribute/conference-papers/>Call for Conference Papers</a>
                    <a href=/contribute/posters/>Call for Posters</a>
                    <a href=/contribute/workshops/>Call for Workshops</a>
                    <a href=/contribute/3dui-contest/>Call for 3DUI Contest Entries</a>
                    <a href=/contribute/demos/>Call for Research Demos</a>
                    <a href=/contribute/doctoral-consortium/>Call for Doctoral Consortium</a>
                    <a href=/contribute/tutorials/>Call for Tutorials</a>
                    <a href=/contribute/workshoppapers/>Call for Workshop Papers</a>
            <!--    <a href=/contribute/panels/ style="color:#919191">Call for Panels</a> -->
            <!--    <a href=/contribute/videos/ style="color:#919191">Call for Videos</a> -->
                    <a href=/contribute/studentVolunteers/>Call for Student Volunteers</a>
            <!--    <a href=/contribute/presenterInfo>Presenter Guidelines</a>                  -->
            <!--    <a href=/contribute/videoInstructions/" | relative_url }}>Video Guidelines</a>   -->

                </div>
            </div>
            
            <div class="dropdown">
                <button class="dropbtn">Awards &#9662;</button>
                <div class="dropdown-content">
                    <a href=/awards/vgtc>VGTC Awards</a>
                    <!--<a href=/awards/vgtc-award-winners/>2022 Awards Winners</a>
                    <a href=/awards/conference-awards/>Conference Awards</a>-->
                </div>
            </div>

            <div class="dropdown">
                <button class="dropbtn">Committees &#9662;</button>
                <div class="dropdown-content">
                    <a href=/committees/conference-committee/>Conference Committee</a>
                    <a href=/committees/steering-committee/>Steering Committee</a> 
                    <a href=/committees/program-committee/>Program Committee</a>
                </div>
            </div>

            <div class="dropdown">
                <button class="dropbtn">Resources &#9662;</button>
                <div class="dropdown-content">
                    <a href=/resources#slide-templates>Slide Templates</a>
                    <a href=/resources#zoom-backgrounds>Zoom Backgrounds</a>
                    <a href=/resources#banner>Banners</a>
                    <a href=/resources#logos>Conference Logos</a>
                </div>
            </div>

            <a href=/past-conferences/>Past Conferences</a>

            <a href="javascript:void(0);" style="font-size:16px;" class="icon" onclick="myFunction()">&#9776;</a>
        </div>


        <!-- banner -->
        <img class="ieeevrbanner" src=/assets/images/logos/IEEE%20VR%202020%20Logo%20BANNER.jpg alt="The official banner for the IEEE Conference on Virtual Reality + User Interfaces, comprised of a Kiwi wearing a VR headset overlaid on an image of Mount Cook and a braided river.">

        <!-- content -->
        <section class="page__content" itemprop="text">
            <div class="row">

                <!-- left sidebar -->
                <div class="column left" style="">


                    <style>
    .styled-table {
        border-collapse: collapse;
        margin: 25px 0;
        font-size: 0.9em;
        font-family: sans-serif;
        /*min-width: 400px;*/
        box-shadow: 0 0 20px rgba(0, 0, 0, 0.15);
        display: table;
    }

    .styled-table thead tr {
        background-color: #fec10d;
        color: #ffffff;
        text-align: left;
    }

    .styled-table th,
    .styled-table td {
        padding: 12px 15px;
    }

    .styled-table tbody tr {
        border-bottom: 1px solid #dddddd;
    }

    .styled-table tbody tr:nth-of-type(even) {
        background-color: #fffbed;
    }

    .styled-table tbody tr:last-of-type {
        border-bottom: 2px solid #fec10d;
    }

    .styled-table tbody tr.active-row {
        font-weight: bold;
        color: #fec10d;
    }

    /* Collapsible */
    input[type='checkbox'] {
        display: none;
    }

    .wrap-collabsible {
        margin: 1rem 0;
    }

    .lbl-toggle {
        display: block;
        font-weight: bold;
        /* font-family: monospace; */
        font-size: 0.8rem;
        text-align: left;
        padding: 0rem;
        color: #fec10d;
        background: #ffffff;
        cursor: pointer;
        border-radius: 7px;
        transition: all 0.25s ease-out;
    }

    .lbl-toggle:hover {
        /*color: #FFF;*/
    }

    .lbl-toggle::before {
        content: ' ';
        display: inline-block;
        border-top: 5px solid transparent;
        border-bottom: 5px solid transparent;
        border-left: 5px solid currentColor;
        vertical-align: middle;
        margin-right: .7rem;
        transform: translateY(-2px);
        transition: transform .2s ease-out;
    }

    .toggle:checked+.lbl-toggle::before {
        transform: rotate(90deg) translateX(-3px);
    }

    .collapsible-content {
        max-height: 0px;
        overflow: hidden;
        transition: max-height .25s ease-in-out;
    }

    .toggle:checked+.lbl-toggle+.collapsible-content {
        max-height: 1500px;
    }

    .toggle:checked+.lbl-toggle {
        border-bottom-right-radius: 0;
        border-bottom-left-radius: 0;
    }

    .collapsible-content .content-inner {
        background: white;
        /* rgba(0, 105, 255, .2);*/
        border-bottom: 1px solid white;
        border-bottom-left-radius: 7px;
        border-bottom-right-radius: 7px;
        padding: .5rem 1rem;
    }

    .collapsible-content p {
        margin-bottom: 0;
    }
    
    /* video container */
    .video-container {
        overflow: hidden;
        position: relative;
        width: 100%;
    }

    .video-container::after {
        padding-top: 56.25%;
        /* 75% if 4:3*/
        display: block;
        content: '';
    }

    .video-container iframe {
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
    }

</style>

<div>
    <table class="styled-table">

        <tr>
            <th>Posters</th>
            <th>Location</th>
            <th>Poster Session</th>
        </tr>
        <tr>
            <td><a href="#DCA">Doctoral Consortium Posters (Session 1)</a></td>
            <td>Expo Hall A</td>
        </tr>
        <tr>
            <td><a href="#DCB">Doctoral Consortium Posters (Session 2)</a></td>
            <td>Expo Hall B</td>
        </tr>
        <tr>
            <td><a href="#DCC">Doctoral Consortium Posters (Session 3)</a></td>
            <td>Expo Hall C</td>
        </tr>
        <tr>
            <td><a href="#PA">Posters (Session 1)</a></td>
            <td>Expo Hall A</td>
        </tr>
        <tr>
            <td><a href="#PB">Posters (Session 2)</a></td>
            <td>Expo Hall B</td>
        </tr>
        <tr>
            <td><a href="#PC">Posters (Session 3)</a></td>
            <td>Expo Hall C</td>
        </tr>
    </table>
</div>

<div>
<!-- TAKE ME TO THE EVENT START -->
    <!--
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <div class="notice--info" style="background-color: $theme-yellow ! important; color: $theme-text ! important;">
        <strong style="padding-bottom: 5px;">Take me to the event:</strong>
        <p>
            <strong style="color: black;">Virbela Location:</strong> Hall A and Hall B (<a href="/2022/attend/virbela-instructions/#map">MAP</a>)

            
            
            <br />
            <strong style="color: black;">Discord Channel:</strong> <a href="https://discord.com/channels/785628120471699507/823244142611791942" target="_blank">Open in Browser</a>, <a href="discord://discord.com/channels/785628120471699507/823244142611791942">Open in App</a> (Participants only)
            
            
        </p>
    </div> 
    
    
    
    
    
    
    
    
    
    
    
    -->
    <!-- TAKE ME TO THE EVENT END-->
</div>

<div>
    
    <h2 id="DCA"> Doctoral Consortium - Expo Hall A</h2>
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="DC1032">Immersive Analytics for Understanding Ecosystem Services Tradeoffs</h3>
    <p><strong>Doctoral Consortium</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D28 </strong></small> <br /> </p>
    
    <p><i>Benjamin Powley</i></p>
    
    
    
    <div id="abstractDC1032" class="wrap-collabsible"> <input id="collapsibleabstractDC1032" class="toggle" type="checkbox" /> <label for="collapsibleabstractDC1032" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Existing immersive systems for analysing geospatial data relating to ecosystem services are not designed for all groups involved with land use decision making. Land management scientists have different requirements compared to non-experts as the tasks they perform are different. Land use decision making needs better tools for assisting the analysis and exploration of land use decisions, and their effect on ecosystem services. In this research, a user centred design process is applied for developing and evaluating an immersive VR visualization tool to assist with better decision making around land use. Interviews with experts found issues with how their current tool presents analysis results, and problems with communicating their results to stakeholders. A literature review found no pre-existing immersive VR systems specifically for analysing tradeoffs among ecosystem services.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="DC1050"> Exploration of Context and Physiological Cues for Personalized Emotion-Adaptive Virtual Reality</h3>
    <p><strong>Doctoral Consortium</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D31 </strong></small> <br /> </p>
    
    <p><i>Mr Kunal Gupta</i></p>
    
    
    
    <div id="abstractDC1050" class="wrap-collabsible"> <input id="collapsibleabstractDC1050" class="toggle" type="checkbox" /> <label for="collapsibleabstractDC1050" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Immersive Virtual Reality (VR) can create compelling context-specific emotional experiences, but very few studies explore the importance of emotion-relevant contextual cues in VR. In this thesis, I investigate how to use combined contextual and physiological cues to improve emotion recognition in VR and enhance shared VR experiences. The main novelty is the creation of the first Personalized Real-time Emotion-Adaptive Context-Aware VR (PerAffectly VR) system which provides significant insight into how to create, measure, and share emotional VR experiences.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="DC1037">Improving Multi-User Interaction for Mixed Reality Telecollaboration</h3>
    <p><strong>Doctoral Consortium</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D27 </strong></small> <br /> </p>
    
    <p><i>Faisal Zaman</i></p>
    
    
    
    <div id="abstractDC1037" class="wrap-collabsible"> <input id="collapsibleabstractDC1037" class="toggle" type="checkbox" /> <label for="collapsibleabstractDC1037" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Mixed reality approaches offer merging of real and virtual worlds to create new environments and visualizations for real-time interaction. However, existing systems do not utilise user real environment, lack detail in dynamic environments, and often lack multi-user capabilities. This research focuses on exploring this multi-user aspect of immersive collaboration, where an arbitrary number of co-located and remotely located users can immerse into a single or merged collaborative mixed reality space. The aim is to enable users to experience VR/AR together, irrespective of the type of HMD, and facilitate users with their collaborative tasks. The main goal is to develop an immersive collaboration platform in which users can utilize the space around them and at the same time can collaborate and switch between different perspectives of other co-located and remote users.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="DC1038"> A Mobile Intervention to Promote Social Skills in Children with Autism Spectrum Disorder Using AR Face Masks</h3>
    <p><strong>Doctoral Consortium</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D32 </strong></small> <br /> </p>
    
    <p><i>Ms. Hiroshika Nadeeshani Premarathne</i></p>
    
    
    
    <div id="abstractDC1038" class="wrap-collabsible"> <input id="collapsibleabstractDC1038" class="toggle" type="checkbox" /> <label for="collapsibleabstractDC1038" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Autism Spectrum Disorder is a lifelong neuro-developmental disorder characterized by several behaviors including the deficits in emotion recognition and face-directed eye gaze. Although, behavioral therapists are there to help children with ASD to improve the skills, due to both high demand for services and lack of resources, there is a need for providing alternative interventions to therapy sessions. My PhD aims to develop an Augmented Reality (AR) based mobile application to provide an alternative method and assist children with ASD in identifying emotions and communicating with their close family members using face-directed eye gaze.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="DC1048">Using Multimodal Input in Augmented Virtual Teleportation</h3>
    <p><strong>Doctoral Consortium</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D26 </strong></small> <br /> </p>
    
    <p><i>Prasanth Sasikumar</i></p>
    
    
    
    <div id="abstractDC1048" class="wrap-collabsible"> <input id="collapsibleabstractDC1048" class="toggle" type="checkbox" /> <label for="collapsibleabstractDC1048" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Augmented (AR) and Virtual Reality (VR) can create compelling emotional collaborative experiences, but very few studies have explored the importance of sharing a user's live environment and their physiological cues. In this Ph.D. thesis, I am investigating how to use scene reconstruction and emotion recognition to enhance shared collaborative AR/VR experiences. I have developed a framework that can be broadly classified into two sections: 1) Live scene capturing for real-time environment reconstruction, 2) Sharing multimodal input such as gaze, gesture, and physiological cues. The main novelty of the research is that it is one of the first systems for real-time sharing of environment and emotion cues. It provides significant insight into how to create, measure, and share remote collaborative experiences. The research will be helpful in multiple application domains such as remote assistance, tourism, training, and entertainment. It will also enable the creation of interfaces that automatically adapt to the user's emotional needs and environment and provide a better collaborative experience.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="DC1049">A Tangible Augmented Reality Programming Learning Environment (TARPLE) for Active, Guided Learning</h3>
    <p><strong>Doctoral Consortium</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D33 </strong></small> <br /> </p>
    
    <p><i>Dmitry Resnyansky</i></p>
    
    
    
    <div id="abstractDC1049" class="wrap-collabsible"> <input id="collapsibleabstractDC1049" class="toggle" type="checkbox" /> <label for="collapsibleabstractDC1049" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This PhD project aims to bring together technological and educational perspectives by understanding how objectives of programming learning and principles of active learning and embodied learning can be supported and enhanced with AR and TUI technologies. This work presents an approach to the design and evaluation of a TARPLE prototype with enhanced functionality to encourage active guided learning of a text-based OOP language. The system supports natural interaction with learning material, and embodiment and contextualisation of information in 3D space. One of the goals of this thesis has been to understand how empirical studies can inform the design of a TARPLE that supports learning of text-based programming languages and development of a basic debugging skillset.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="DC1031">Designing and Optimizing Daily-wear Photophobic Smart Sunglasses</h3>
    <p><strong>Doctoral Consortium</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D25 </strong></small> <br /> </p>
    
    <p><i>Xiaodan Hu</i></p>
    
    
    
    <div id="abstractDC1031" class="wrap-collabsible"> <input id="collapsibleabstractDC1031" class="toggle" type="checkbox" /> <label for="collapsibleabstractDC1031" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Photophobia, also known as light sensitivity, is a condition in which there is a fear of light. Traditional sunglasses and tinted glasses typically worn by individuals with photophobia only provide linear dimming, leading to difficulty to see the contents in the dark region of a high-contrast environment (e.g., indoors at night). This paper presents a smart dimming sunglass that uses a spatial light modular (SLM) to flexibly dim the user's field of view based on scene detection from an HDR camera. To address the problem when the user views a distant object, the occlusion mask displayed on the SLM becomes blurred due to out-of-focus, thus providing an insufficient modulation. An optimization model is designed to dilate the occlusion mask appropriately. The optimized dimming effect is verified by the camera and preliminary test by real users to be able to filter the desired amount of incoming light through a blurred mask.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="DC1040">The impact of the Informational load of Presence Illusions on Users Attention and Memory</h3>
    <p><strong>Doctoral Consortium</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D34 </strong></small> <br /> </p>
    
    <p><i>Daniel A. Mu&ntilde;oz Daniel A. Mu&ntilde;oz</i></p>
    
    
    
    <div id="abstractDC1040" class="wrap-collabsible"> <input id="collapsibleabstractDC1040" class="toggle" type="checkbox" /> <label for="collapsibleabstractDC1040" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Presence has become an expected outcome for most VR experiences due to its positive relationship with engagement, task performance, behavior change, and other experiences. However, current literature reports contradictory results regarding presence, cognitive load, working memory, and attention. This study explores the cognitive load of presence under the framework of illusions to understand how these illusions impact users' attention and memory. Quantify and hierarchize the information built by presence (Place and Plausibility of illusion), attempt to contribute knowledge to further design, and effectively manage attention and distraction on Virtual reality experiences. This document discussed our theoretical direction and a proposed psychophysiological study.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
</div>

<div>
    
    <h2 id="DCB"> Doctoral Consortium - Expo Hall B</h2>
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="DC1023">Gamified VR for Socially Isolated Adolescents with Significant Illness</h3>
    <p><strong>Doctoral Consortium</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D25 </strong></small> <br /> </p>
    
    <p><i>Ms Udapola Balage Hansi Shashiprabha Udapola</i></p>
    
    
    
    <div id="abstractDC1023" class="wrap-collabsible"> <input id="collapsibleabstractDC1023" class="toggle" type="checkbox" /> <label for="collapsibleabstractDC1023" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Adolescents with significant illness face various psychosocial and mental wellbeing challenges during hospitalisation. Social isolation from family and peers is identified as a significant concern for this group. Several digital interventions have been proposed to connect these young people with others, such as video conferencing, social media, social robots, and online games. Research so far has found those to be beneficial for adolescents' wellbeing. Social VR is a novel social interaction mechanism that allows users to interact socially within an immersive 3D virtual environment with embodiment experience. Playing in the social VR space is generally identified as a motivational factor for adolescents to engage in social interactions. Therefore, integrating game technologies into social VR space would encourage and motivate socially isolated adolescents to engage socially intrinsically. The main goal of this research project is to enhance the social engagement of socially isolated adolescents by fostering positive interactions with their peers within a safe gamified virtual environment. In order to achieve that, this research will design and develop an intervention using game and VR technologies. Then, the designed intervention will be evaluated with the target user group to investigate the impact of implemented game mechanics on social engagement and connectedness.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="DC1030">XR for Improving Cardiac Catheter Ablation Procedure</h3>
    <p><strong>Doctoral Consortium</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D34 </strong></small> <br /> </p>
    
    <p><i>Mr. Nisal Manisha Udawatta Kankanamge Don</i></p>
    
    
    
    <div id="abstractDC1030" class="wrap-collabsible"> <input id="collapsibleabstractDC1030" class="toggle" type="checkbox" /> <label for="collapsibleabstractDC1030" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Cardiac arrhythmia refers to abnormalities of heart rhythm, and cardiac catheter ablation procedure provides the best therapeutic outcomes to cure this life-threatening pathology. An electrophysiologist clinically performs the procedure that involves navigating 'catheters' into the chambers of the heart through peripheral blood vessels, studying the cardiac electrophysiology and performing ablations. An electrophysiologist must possess a comprehensive understanding of cardiac electrophysiology and precise instrument handling due to the sensitiveness of the procedure. In the conventional approach, electroanatomical mapping systems and fluoroscopic visualizations are utilized to assist the procedure&#59; however, their limitations reduce the procedure's effectiveness. Two main scenarios have been identified to improve the effectiveness of the procedure: intraoperative guidance and procedure training. This study aims to examine how extended reality technologies (eg. AR/VR) can be used to improve the effectiveness of the cardiac catheter ablation procedure.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="DC1027">Mixed Reality Interaction for Mobile Knowledge Work</h3>
    <p><strong>Doctoral Consortium</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D26 </strong></small> <br /> </p>
    
    <p><i>Verena Biener</i></p>
    
    
    
    <div id="abstractDC1027" class="wrap-collabsible"> <input id="collapsibleabstractDC1027" class="toggle" type="checkbox" /> <label for="collapsibleabstractDC1027" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Knowledge workers typically work on some kind of computer and other than an internet connection, they rarely need access to specific work environments or devices. This makes it feasible for them to also work in different environments or in mobile settings, like public transportation. In such spaces comfort and productivity could be decreased due to hardware limitations like small screen sizes or input devices and environmental clutter. Mixed reality (MR) has the potential to solve such issues. It can provide the user with additional display space that can even include the third dimension. It can open up new possibilities for interacting with virtual content using gestures or spatially tracked devices. And it can allow the users to modify the work environment according to their personal preferences. This doctoral thesis aims at exploring the challenges of using MR for mobile knowledge work and how to effectively support knowledge worker tasks through appropriate interaction techniques.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="DC1024">Dynamic facial expressions on virtual humans to facilitate virtual reality (VR) mental health therapy</h3>
    <p><strong>Doctoral Consortium</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D31 </strong></small> <br /> </p>
    
    <p><i>Miss Shu Wei</i></p>
    
    
    
    <div id="abstractDC1024" class="wrap-collabsible"> <input id="collapsibleabstractDC1024" class="toggle" type="checkbox" /> <label for="collapsibleabstractDC1024" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This ongoing project aims to utilize dynamic facial expressions on virtual humans to enhance the effectiveness and efficiency of virtual reality (VR) mental health therapy. A systematic review of virtual humans in mental health VR indicated that only around 10&#37; applications used dynamic facial expressions. The potentials of virtual characters' emotion richness is understudied and it is unclear how the facial expressions affect the individuals differently in the virtual environment. Therefore, we will focus on understanding people's behavioural, physiological, and psychological reactions toward facial-animated characters in VR experimental studies. The first study examines whether particular non-verbal behaviours can enhance people's therapy engagement, by applying warmness facial expressions and head nod on a virtual coach. Future experiments will further look at mixed facial expressions, and assess people's visual attention (through eye-tracking) and interpretation of the emotional faces. This research will explore how best to use facial expressions to facilitate VR therapy through the practice of psychiatric research, VR programming and 3D animation.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="DC1000">Robust Redirected Walking in the Wild</h3>
    <p><strong>Doctoral Consortium</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D27 </strong></small> <br /> </p>
    
    <p><i>Niall L. Williams</i></p>
    
    
    
    <div id="abstractDC1000" class="wrap-collabsible"> <input id="collapsibleabstractDC1000" class="toggle" type="checkbox" /> <label for="collapsibleabstractDC1000" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Locomotion is a fundamental component of experiences in virtual reality (VR). However, locomotion in VR is often difficult because the layouts of the physical and virtual environments are often different, which may cause unobstructed paths in the virtual world to correspond to obstructed paths in the physical world. Thus, in order to deliver a comfortable and immersive virtual experience to users, it is important that the user can explore the virtual world using techniques that help them avoid collisions with unseen physical objects. Redirected walking (RDW) is one such technique that enables collision-free locomotion in VR using real walking. Although RDW shows promise as an effective locomotion interface, it has seen relatively little adoption in the consumer market due to the difficulty in deploying effective RDW algorithms that are robust to different environment layouts and different users' perceptual thresholds. For my thesis, I am focused on developing RDW methods that are capable of enabling collision-free locomotion in arbitrary physical and virtual environments for a wide range of users.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="DC1033">Context-Aware Inference and Adaptation in Augmented Reality</h3>
    <p><strong>Doctoral Consortium</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D32 </strong></small> <br /> </p>
    
    <p><i>Shakiba Davari</i></p>
    
    
    
    <div id="abstractDC1033" class="wrap-collabsible"> <input id="collapsibleabstractDC1033" class="toggle" type="checkbox" /> <label for="collapsibleabstractDC1033" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Augmented Reality(AR) offers the potential for easy and efficient information access, reinforcing the wide belief that AR Glasses are the next-generation of personal computing devices. However, to realize this all-day AR vision, the AR interface must be able to address the challenges that constant and pervasive presence of virtual content can cause for the user. The optimal interface, that is the most efficient yet least intrusive, in one context may be the worst interface for another context. Throughout the day, as the user switches context, an optimal all-day interface must adapts its virtual content display and interactions as well. This work aims to propose a research agenda to design and validate different adaptation techniques and context-aware AR interfaces and introduce a framework for the design of such intelligent interfaces.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="DC1039">Balancing Realities by Smoothing Cross-Reality Interactions</h3>
    <p><strong>Doctoral Consortium</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D28 </strong></small> <br /> </p>
    
    <p><i>Matt Gottsacker</i></p>
    
    
    
    <div id="abstractDC1039" class="wrap-collabsible"> <input id="collapsibleabstractDC1039" class="toggle" type="checkbox" /> <label for="collapsibleabstractDC1039" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Virtual reality (VR) devices have a demonstrated capability to make users feel present in a virtual world. Research has shown that, at times, users desire a less immersive system that provides them awareness of and the ability to interact with elements from the real world and with a variety of devices. Understanding such cross-reality interactions is an under-explored research area that will become increasingly important as immersive devices become more ubiquitous. The planned focus of my dissertation is to investigate the social norms that are complicated by these interactions and design solutions that lead to meaningful interactions. As a second-year PhD student, I am excited about the possibility of discussing my research at the IEEE VR 2022 Doctoral Consortium and getting feedback from peers and mentors about the direction of my dissertation.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="DC1042">Designing Immersive Tools for Supporting Cognition in Remote Scientific Collaboration</h3>
    <p><strong>Doctoral Consortium</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D33 </strong></small> <br /> </p>
    
    <p><i>Monsurat Olaosebikan</i></p>
    
    
    
    <div id="abstractDC1042" class="wrap-collabsible"> <input id="collapsibleabstractDC1042" class="toggle" type="checkbox" /> <label for="collapsibleabstractDC1042" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Scientists collaborate remotely across institutions, countries and continents. However, collaborating remotely is challenging. Video-conferencing tools used for meetings limit the cognitive practices that collaborators can partake in. In virtual reality (VR) users can gain back spatial affordances present in collocated collaboration and we can design interactions that would not be possible in the real world. My research aims to investigate how VR can support cognition in remote scientific collaboration through the design, development, and study of Embodied Notes: a cognitive support tool designed to be used in a collaborative virtual environment.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
</div>

<div>
    
    <h2 id="DCC"> Doctoral Consortium - Expo Hall C</h2>
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="DC1047">Improving presence of virtual humans through paralinguistics</h3>
    <p><strong>Doctoral Consortium</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D25 </strong></small> <br /> </p>
    
    <p><i>Andrew H Maxim</i></p>
    
    
    
    <div id="abstractDC1047" class="wrap-collabsible"> <input id="collapsibleabstractDC1047" class="toggle" type="checkbox" /> <label for="collapsibleabstractDC1047" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Social presence and plausibility are two of the most important constructs being studied in IEEE VR to improve user experience in virtual environments. However, virtual humans that are used in these virtual environments lack the capability of adaptive speech that directly influences social presence and plausibility of the virtual humans that are used since humans use adaptive speech during conversations. Virtual humans lack the adaptability of their speech in the manner that humans do. Research has been explored in how pitch, rate of speech, and intensity can be manipulated in human discourse. However, little research has been done in using adaptable speech to affect dialog with virtual humans. This dissertation attempts to create a framework and a system that enables adaptive virtual human speech. This includes using machine learning and user modeling to manipulate paralinguistic features such as pitch, rate of speech, pause duration, and intensity using Speech Synthesis Markup Language (SSML). Understanding the interplay of paralinguistic features between virtual human and human will improve the social presence of virtual humans. By achieving this, virtual interactions can be moved toward the dynamic level of human-to-human interactions, thus increasing the co-presence and plausibility of these virtual human characters.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="DC1029">Leveraging AR Cues towards New Navigation Assistant Paradigm</h3>
    <p><strong>Doctoral Consortium</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D34 </strong></small> <br /> </p>
    
    <p><i>Yu Zhao</i></p>
    
    
    
    <div id="abstractDC1029" class="wrap-collabsible"> <input id="collapsibleabstractDC1029" class="toggle" type="checkbox" /> <label for="collapsibleabstractDC1029" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Extensive research has shown that the knowledge required to navigate an unfamiliar environment has been greatly reduced as many of the planning and decision-making tasks can be supplanted by the use of automated navigation systems. The progress in augmented reality (AR), particularly AR head-mounted displays (HMDs) foreshadows the prevalence of such devices as computational platforms of the future. AR displays open a new design space on navigational aids for solving this problem by superimposing virtual imagery over the environment. This dissertation abstract proposes a research agenda that investigates how to effectively leverage AR cues to help both navigation efficiency and spatial learning in walking scenarios.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="DC1045">Annotation in Asynchronous Collaborative Immersive Analytic Environments using Augmented Reality</h3>
    <p><strong>Doctoral Consortium</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D26 </strong></small> <br /> </p>
    
    <p><i>Zahra Borhani</i></p>
    
    
    
    <div id="abstractDC1045" class="wrap-collabsible"> <input id="collapsibleabstractDC1045" class="toggle" type="checkbox" /> <label for="collapsibleabstractDC1045" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Immersive Analysts(IA) and Augmented Reality (AR) head-mounted displays provide a different paradigm for people to analyze multidimensional data and externalize their thoughts by utilizing the stereoscopic nature of headsets. However, using annotation in IA-AR is challenging and not well-understood. In addition, IA collaborative environments add another complexity level for users operating on complex visualized datasets. Current AR systems focus mainly on synchronized collaboration, while asynchronous collaboration has remained unexplored. This project investigates annotation in IA for asynchronous collaborative environments. We present our research studies on virtual annotation types and introduce a new filtering annotation technique for IA.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="DC1046">Effects of Asymmetric Locomotion Methods on Collaborative Navigation and Wayfinding in Shared Virtual Environments</h3>
    <p><strong>Doctoral Consortium</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D33 </strong></small> <br /> </p>
    
    <p><i>Soumyajit Chakraborty</i></p>
    
    
    
    <div id="abstractDC1046" class="wrap-collabsible"> <input id="collapsibleabstractDC1046" class="toggle" type="checkbox" /> <label for="collapsibleabstractDC1046" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Navigation and wayfinding can be accomplished either by single person or a group of people. Using the help of immersive virtual reality technology, significant research has been conducted to find out how a person can navigate and wayfind in a virtual world. How- ever, there has been little work done that asks how multiple people can collaboratively navigate and wayfind in a virtual world. In this proposal, we investigate this question with a specific interest on how different locomotion methods can affect the acquired knowledge of a group of individuals in a distributed, shared virtual environment.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
</div>

<div>
    
    <h2 id="PA"> Posters - Expo Hall A</h2>
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1066">CV-Mora Based Lip Sync Facial Animations for Japanese Speech</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: B11 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Ryoto Kato</i></p>
    
    
        <p>Teaser Video: <a href="https://youtu.be/HB52DVsXrkg" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1066" class="wrap-collabsible"> <input id="collapsibleabstractC1066" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1066" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>To generate authentic real-time facial animations using face mesh data, which corresponds to fifty-six consonant and vowel (CV) types of morae that form the basis of Japanese speech, we propose a new method. Our method produces facial expressions by the weighted addition of fifty-three face meshes based on the real-time mapping of voice streaming to registered morae. In the user study, results showed that facial expressions produced during Japanese speech were more natural using our method than those using popular methods to generate real-time English-based Oculus lip sync and volume intensity-based facial animations.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1088">Gaze Capture based Considerate Behaviour Control of Virtual Guiding Agent</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: B12 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Hironori Mitake</i></p>
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=h4-93eQURWM" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1088" class="wrap-collabsible"> <input id="collapsibleabstractC1088" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1088" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Agents in VR have wide application like guidance. Most current agents are passive, so that people should suspend their current tasks and request agents with explicit demand. It is necessary to make agent more actively open the interaction naturally but without being bothering.
We propose a virtual guidance agent which provide voice explanation in appropriate timing, using gaze tracking, attention amount estimation and attention driven state machine. We used time-decayed moving average of angle between gaze direction and face front direction.
We implemented the method in VR and evaluated effectiveness in virtual guiding tour experimentally.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="C1091">Perceptions of Colour Pickers and Companions in Virtual Reality Art-Making</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: B13 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Marylyn Alex</i></p>
    
    
        <p>Teaser Video: <a href="https://youtu.be/RPcqwawFn60" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1091" class="wrap-collabsible"> <input id="collapsibleabstractC1091" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1091" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Virtual reality art is reshaping digital art experiences but may elicit different first impressions across disparate age groups. We investigate first impressions of VR colour pickers and the impact of a virtual companion via an online survey with 63 adults and 24 older adults. The colour pickers differed significantly in perceived hedonic qualities. We found no statistical differences between perceptions of adults and older adults. The virtual companion had no significant effect on participants' overall experiences. However, we found statistical trends where older adults rated the virtual companion higher in terms of companionship and making VR art more engaging.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1103">Augmenting VR Ski Training using Time Distortion</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: B14 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Takashi Matsumoto</i></p>
    
    
        <p>Teaser Video: <a href="https://youtu.be/rUNxRoenp7o" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1103" class="wrap-collabsible"> <input id="collapsibleabstractC1103" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1103" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Virtual reality-based sports simulators are widely developed, which makes training in a virtual environment possible. On the other hand, methods using temporal features are also introduced to realize an adaptive training. In this paper, we study the effect of time distortion on alpine ski training to find out how modifying the temporal space can affect sports training. Experiments are conducted to investigate how a fast/slow and a static/dynamic time distortion-based training, respectively, can impact the performance of users.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1164">Investigating Display Position of a Head-Fixed Augmented Reality Notification for Dual-task</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: B35 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Hyunjin Lee</i></p>
    
    
        <p>Teaser Video: <a href="https://youtu.be/ALXzQpHXWt8" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1164" class="wrap-collabsible"> <input id="collapsibleabstractC1164" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1164" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Providing additional information in the proper position of augmented reality (AR) head-mounted display (HMD) can help increase AR performance and usability for dual-task. Therefore, our study investigated how to place notifications for the dual-task to address this. We compared eight display positions and two tasks (single and dual tasks) to identify the appropriate area for displaying notifications. We confirmed that the middle-right reduces response time and task load. In contrast, the top-left is the location, which should avoid providing any notification in AR dual-task. Our study contributes to designing AR notifications on HMDs to enhance everyday AR experiences.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1181">Augmented Reality Fitts' Law Input Comparison Between Touchpad, Pointing Gesture, and Raycast</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: B21 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Domenick Mifsud</i></p>
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=c3rabmgy_Hc" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1181" class="wrap-collabsible"> <input id="collapsibleabstractC1181" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1181" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>With the goal of exploring the impact of transparency on selection in augmented reality (AR), we present a Fitts' law experiment with 18 participants, comparing three different input methods (finger based Pointing Gesture, controller using the Touchpad, and controller using Raycast), across 4 different target transparency levels (0&#37;, 30&#37;, 60&#37;, and 90&#37;) in an optical see-through AR head-mounted display. The results indicate that transparency has little effect on selection throughput and error rates. Overall, the Raycast input method performed significantly better than the pointing gesture and Touchpad inputs in terms of error rate and throughput in all opacity conditions.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1190">High-speed Gaze-oriented Projection by Cross-ratio-based Eye Tracking with Dual Infrared Imaging</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: C11 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Ayumi Matsumoto</i></p>
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=0wTEgvkwA1I" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1190" class="wrap-collabsible"> <input id="collapsibleabstractC1190" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1190" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>While gaze-oriented projection can be high-resolution and wide-area display, conventional methods have difficulties in handling quick human eye movements. In this paper, we propose a high-speed gaze-oriented projection system using a synchronized high-speed tracking projector and cross-ratio-based eye tracking. The tracking projector with a high-speed projector and rotational mirrors enables temporal geometric consistency of the projection. The eye tracking uses high-speed cameras and infrared lightings of different wavelengths, and can achieve fast and almost calibration-free due to the cross-ratio algorithm. We have experimentally validated the eye tracking speed and accuracy, system latency, and demonstrated gaze-oriented projection.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="C1210">VR Wayfinding Training for People with Visual Impairment using VR Treadmill and VR Tracker</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: C12 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Sangsun Han</i></p>
    
    
        <p>Teaser Video: <a href="https://youtu.be/6ri75t3PAFA" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1210" class="wrap-collabsible"> <input id="collapsibleabstractC1210" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1210" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>There are virtual reality (VR) wayfinding training systems for people with visual impairment, but there is a lack of studies about how training environments can affect spatial information acquisition of people with visual impairment. Using a VR treadmill and a VR tracker, we studied how walk-in-place and actual walking can affect the acquisition of spatial information with regard to paths and obstacles. Our results show that people with visual impairment remember routes better when trained with VR treadmill, but they remember obstacles better when trained with VR tracker. We evaluate the respective efficacies of these approaches on spatial information memorization.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1265">HoloInset: 3D Biomedical Image Data Exploration through Augmented Hologram Insets</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: C13 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>JunYoung Choi</i></p>
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=g1FrGxDQrAQ" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1265" class="wrap-collabsible"> <input id="collapsibleabstractC1265" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1265" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>The extended reality (XR) provides realistic depth perception and huge visualization spaces, which can serve as a powerful workspace for 3D data exploration and analysis. However, a direct adaptation of XR to conventional 3D data exploration tasks is less feasible due to several hardware limitations, such as low screen resolution, dizziness, narrow field of view, etc. In this paper, we propose a novel mixed reality visualization scheme, HoloInset, which combines a conventional visual analytics system and a virtual environment to effectively explore 3D biomedical image data. We also demonstrate the usability of the proposed visualization through a real-world analysis case.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1356">Augmenting Sculpture with Immersive Sonification</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: C14 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Yichen Wang</i></p>
    
    
        <p>Teaser Video: <a href="https://youtu.be/MlBKWVlkEDc" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1356" class="wrap-collabsible"> <input id="collapsibleabstractC1356" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1356" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We present an artistic Mixed Reality (MR) system that remixes a sculptural element of a building and its aesthetic context to provide an on-site augmented art experience. Mainstream MR systems, particularly art-related, focus on the use of visuals in presenting additional information, whereas the use of audio as the main information channel has rarely been considered. In this work, we explore two different versions of a sonic experience for walking through an artistic staircase to enhance its public's engagement. Our user evaluation reveals the effectiveness of sonic design for a rewarding MR experience. With this, we emphasise the importance of sonic design in MR applications.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1405">Feasibility of Training Elite Athletes for Improving their Mental Imagery Ability Using Virtual Reality</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: C21 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Yuanjie Wu</i></p>
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=dfpbJw3mVFY" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1405" class="wrap-collabsible"> <input id="collapsibleabstractC1405" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1405" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>The goal of imagery training for athletes is to create realistic images in their minds and to familiarize them with certain procedures, environments, and other aspects related to competition. Traditional imagery training methods use still images or videos, and athletes study the pictures or watch the videos in order to mentally rehearse. However, factors such as distractions and low realism can affect the training quality. In this paper, we present a VR solution and a study that explores our hypotheses that 1) high-fidelity VR systems improve mental imagery skills, and that 2) the presence of elements such as an audience or photographers in the VR environment result in better mental imagery skill improvement.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1438">Emotional Avatars: Facial Emotion Identification Methodology for Avatar Based Systems</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: C22 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Dilshani Kumarapeli</i></p>
    
    
        <p>Teaser Video: <a href="https://youtu.be/sEhP59USpWk" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1438" class="wrap-collabsible"> <input id="collapsibleabstractC1438" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1438" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This work analyses the effect of uncanniness behaviour in identifying emotions from different humanoid avatar representations. Expressive avatars play a vital role in immersive environments. However, technical limitations in replicating subtle emotional cues using real-time expression conversion techniques create an uncanniness to the viewers. Hence, achieving the desired emotional awareness is arguable. Therefore, using an avatar representation resistant to uncanniness in systems sensitive to emotional changes is vital. Therefore, here we analyse the level of uncanniness noticed by people for different avatars exhibiting various emotions using expressive faces and the behavioural trends of people in catching emotion-related uncanniness.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1449">Prototyping a Virtual Agent for Pre-school English Teaching</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: C23 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Eduardo Benitez Sandoval</i></p>
    
    
        <p>Teaser Video: <a href="https://youtu.be/iPKHRcGrQOs" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1449" class="wrap-collabsible"> <input id="collapsibleabstractC1449" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1449" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This paper describes a case study and the insights gained from prototyping an Intelligent Virtual Agent (IVA) for English vocabulary building for Spanish-speaking preschool children. After an initial exploration to evaluate the feasibility of developing an IVA, we followed a Human-Centered Design (HCD) approach to create a prototype. We report on the multidisciplinary process used that incorporated two well-known educative concepts: gamification and story-telling as the main components for engagement. Our results suggest that a multidisciplinary approach to developing an educational IVA is effective. We report on the relevant aspects of the ideation and design processes that informed the vision and mission of the project.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1489">A Tangible Augmented Reality Programming Learning Environment for textual languages</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: C24 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Dmitry Resnyansky</i></p>
    
    
        <p>Teaser Video: <a href="https://youtu.be/Ta0ufvfVPf4" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1489" class="wrap-collabsible"> <input id="collapsibleabstractC1489" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1489" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We present a novel Tangible Augmented Reality Programming Learning Environment system using head-mounted display (HMD) and physical manipulatives for teaching programming. The system supports student understanding/recollection of terms, and statement construction through access to terminology, explanations, and programming hints. It is designed to provide a virtual workspace for natural interaction with learning material using affordances of Augmented Reality (AR) and Tangible User Interfaces (TUIs). An AR code template provides a building and testing environment for learners to practice statement construction and computational skills. The system bolsters active learning with localised AR program visualisations and HMD-anchored AR glossary.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1577">How Late is Too Late? Effects of Network Latency on Audio-Visual Perception During AR Remote Musical Collaboration</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: C28 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Torin Hopkins</i></p>
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=69ewcIKQWdY" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1577" class="wrap-collabsible"> <input id="collapsibleabstractC1577" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1577" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Networked Musical Collaboration requires near-instantaneous network transmission for successful real-time collaboration. We studied the way changes in network latency affect participants' auditory and visual perception in latency detection, as well as latency tolerance in AR. Twenty-four participants were asked to play a hand drum with a prerecorded remote musician rendered as an avatar in AR at different levels of audio-visual latency. We analyzed the subjective responses of the participants from each session. Results suggest a minimum noticeable delay value between 160 milliseconds (ms) and 320 ms, as well as no upper limit to audio-visual delay tolerance.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="C1578">Toward Using Multi-Modal Machine Learning for User Behavior Prediction in Simulated Smart Home for Extended Reality</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: C31 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Powen Yao</i></p>
    
    
        <p>Teaser Video: <a href="https://youtu.be/nKgjwGd8ZOs" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1578" class="wrap-collabsible"> <input id="collapsibleabstractC1578" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1578" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In this work, we propose a multi-modal approach to manipulate smart home devices in a smart home environment simulated in virtual reality (VR). We determine the user's target device and the desired action by their utterance, spatial information (gestures, positions, etc.), or a combination of the two. Since the information contained in the user's utterance and the spatial information can be disjoint or complementary to each other, we process the two sources of information in parallel using our array of machine learning models. We use ensemble modeling to aggregate the results of these models and enhance the quality of our final prediction results. We present our preliminary architecture, models, and findings.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1590">A Live-Coded Add-On System for Video Conferencing in Virtual Reality</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: C38 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Septian Razi</i></p>
    
    
        <p>Teaser Video: <a href="https://youtu.be/HdTGUFWILqk" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1590" class="wrap-collabsible"> <input id="collapsibleabstractC1590" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1590" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Despite our increasing reliance on video conferencing, it has inherent limitations in engagement and effective communication. We explore a VR add-on system that supplements traditional video conferencing where a host live-codes data visualisations for stakeholders. It combines VR visualisation techniques with libraries present in popular data analytics tools such as Python, allowing real-time changes by utilising a RESTful real-time database. An application has been built to explore pedigree node-link graphs, and its feasibility has been analysed with a few domain experts. Trials have demonstrated that our system appears to enhance engagement and communication above traditional video conferencing for data exploration.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1607">Seamless Walk: Novel Natural Virtual Reality Locomotion Method with a High-Resolution Tactile Sensor</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: C37 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Yunho Choi</i></p>
    
    
        <p>Teaser Video: <a href="https://youtu.be/h2pAaVl5PUQ" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1607" class="wrap-collabsible"> <input id="collapsibleabstractC1607" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1607" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Natural movement is a challenging problem in virtual reality locomotion. However, existing foot-based locomotion methods lack naturalness due to physical limitations caused by wearing equipment. Therefore, in this study, we propose Seamless-walk, a novel virtual reality (VR) locomotion technique to enable locomotion in the virtual environment by walking on a high-resolution tactile carpet. The proposed Seamless-walk moves the user's virtual character by extracting the users' walking speed and orientation from raw tactile signals using machine learning techniques. We demonstrate that the proposed Seamless-walk is more natural and effective than existing VR locomotion methods by comparing them in VR game-playing tasks.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="C1614">Understanding the Capabilities of the HoloLens 1 and 2 in a Mixed Reality Environment for Direct Volume Rendering with a Ray-casting Algorithm</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: C36 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Hoijoon Jung</i></p>
    
    
        <p>Teaser Video: <a href="https://youtu.be/mqHb7gZn-fs" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1614" class="wrap-collabsible"> <input id="collapsibleabstractC1614" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1614" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Direct volume rendering (DVR) is a standard technique for visualizing scientific volumetric data in three-dimension (3D). Utilizing current mixed reality head-mounted displays (MR-HMDs), the DVR can be displayed as a 3D hologram that can be superimposed on the original 'physical' object, offering supplementary x-ray visions showing its interior features. These MR-MHDs are stimulating innovations in a range of scientific application fields, yet their capabilities on DVR have yet to be thoroughly investigated. In this study, we explore a key requirement of rendering latency capability for MR-HMDs by proposing a benchmark application with 5 volumes and 30 rendering parameter variations.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1630">The Virtual-Augmented Reality Simulator: Evaluating OST-HMD AR calibration algorithms in VR</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: C35 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Danilo Gasques</i></p>
    
    
        <p>Teaser Video: <a href="https://youtu.be/uhn3jzH2iP0" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1630" class="wrap-collabsible"> <input id="collapsibleabstractC1630" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1630" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>When developing AR applications for high-precision domains such as surgery, we face a common problem: how can the system guarantee that the end-user will see a virtual object aligned with its real-world counterpart? Alignment, or registration, is a crucial feature of AR displays, but achieving accurate alignment between real and virtual objects is not trivial. With hundreds of calibration approaches available, we need better tools to understand how and when calibration algorithms fail as well as understand what can be done to improve alignment. This poster introduces a novel AR simulator in VR that facilitates experimentation with different calibration algorithms.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1039">Relationship Between the Sensory Processing Patterns and the Detection Threshold of Curvature Gain</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: C41 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Keigo Matsumoto: The University of Tokyo&#59; Takuji Narumi: the University of Tokyo</i></p>
    
    
        <p>Teaser Video: <a href="https://youtu.be/_IWc7ftJ8x8" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1039" class="wrap-collabsible"> <input id="collapsibleabstractPO1039" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1039" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This study examines the relationship between sensory processing patterns (SPPs) and the effects of redirected walking (RDW). Research efforts have been devoted to identifying the detection threshold (DT) of the RDW techniques, and various DTs have been reported in different studies. Recently, age, sex, and spatial ability have been found to be associated with the DTs of RDW techniques. A preliminary examination was conducted on the relationship between SSPs, as measured by the Adolescents/Adult Sensory Profile, and the DT of curvature gains, one of the fundamental RDW techniques, and it was suggested that the higher sensory sensitivity tendencies were associated with lower DT, i.e., participants were more likely to notice the RDW technique.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="PO1061">Predicting Blendshapes of Virtual Humans for Low-Delay Remote Rendering using LSTM</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: C42 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Haruhisa Kato: KDDI Research Inc.&#59; Tatsuya Kobayashi: KDDI Research Inc.&#59; Sei Naito: KDDI Research Inc.</i></p>
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=_6Z9bqDXLvQ" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1061" class="wrap-collabsible"> <input id="collapsibleabstractPO1061" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1061" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This paper proposes a novel framework to reduce the perceptual delay in rendering the facial expressions of virtual humans. The proposed method reduces the delay by predicting the future blend coefficients that represent facial expressions. The prediction accuracy of the proposed method is improved by 27&#37; on average over a conventional
LSTM. We also subjectively confirmed that the proposed method achieves natural facial expressions.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1103">AIR-range: Arranging optical systems to present mid-AIR images with continuous luminance on and above a tabletop</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: C43 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Tomoyo Kikuchi: The University of Tokyo&#59; Yuchi Yahagi: The University of Tokyo&#59; Shogo Fukushima: The University of Tokyo&#59; Saki Sakaguchi: Tokyo Metropolitan University&#59; Takeshi Naemura: The University of Tokyo</i></p>
    
    
        <p>Teaser Video: <a href="https://youtu.be/OOtvkFNTKT0" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1103" class="wrap-collabsible"> <input id="collapsibleabstractPO1103" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1103" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We propose &quot;AIR-range&quot;- a system that seamlessly connects mid-air images from the surface of a table to mid-air space. This system can display tall mid-air images in the three-dimensional (3D) space beyond the screen. AIR-range is implemented using a symmetrical mirror structure that displays a large image by integrating multiple imaging paths. The mirror arrangement in previous research had a problem in that the luminance was discontinuous. In this study, we theorize the relationship between the parameters of optical elements and the appearance of mid-air images and optimize an optical system to minimize the difference in luminance between image paths.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1111">Third-Person Perspective Avatar Embodiment in Augmented Reality: Examining the Proteus Effect on Physical Performance</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: C44 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Riku Otono: Nara Institute of Science and Technology&#59; Naoya Isoyama: Nara Institute of Science and Technology&#59; Hideaki Uchiyama: Nara Institute of Science and Technology&#59; Kiyoshi Kiyokawa: Nara Institute of Science and Technology</i></p>
    
    
        <p>Teaser Video: <a href="https://youtu.be/RBcTPqbDxhs" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1111" class="wrap-collabsible"> <input id="collapsibleabstractPO1111" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1111" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Embodiment in augmented reality (AR) is applicable to various fields such as exercise and education. However, full-body embodiment in AR is still challenging to implement due to technical problems such as low body tracking accuracy. Therefore, the study on the impact of an avatar in AR on user performance is limited. We implemented an AR embodiment system and investigated its impact on user physical performance. The system allows users to see their avatar instead of their real body from a third-person perspective. The results show that a muscular avatar improves user physical performance during and after controlling the avatar.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1121">Omnidirectional Neural Radiance Field for Immersive Experience</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: D11 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Qiaoge Li: University of Tsukuba&#59; Itsuki Ueda: University of Tsukuba&#59; Chun Xie: University of Tsukuba&#59; Hidehiko Shishido: University of Tsukuba&#59; Itaru Kitahara: University of Tsukuba</i></p>
    
    
        <p>Teaser Video: <a href="https://youtu.be/B989wn84DxY" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1121" class="wrap-collabsible"> <input id="collapsibleabstractPO1121" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1121" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This paper proposes a method using only RGB information from multiple captured panoramas to provide an immersive observing experience for real scenes. We generated an omnidirectional neural radiance field by adopting the Fibonacci sphere model for sampling rays and several optimized positional encoding approaches. We tested our method on synthetic and real scenes and achieved satisfying empirical performance. Our result makes the immersive continuous free-viewpoint experience possible.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1134">Depth Reduction in Light-Field Head-Mounted Displays by Generating Intermediate Images as Virtual Images</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: D12 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Yasutaka Maeda: Japan Broadcasting Corporation (NHK)&#59; Daiichi Koide: Japan Broadcasting Corporation (NHK)&#59; Hisayuki Sasaki: Japan Broadcasting Corporation (NHK)&#59; Kensuke Hisatomi: Japan Broadcasting Corporation (NHK)</i></p>
    
    
        <p>Teaser Video: <a href="https://youtu.be/HNNd_3NLoL0" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1134" class="wrap-collabsible"> <input id="collapsibleabstractPO1134" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1134" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Light-field head-mounted displays (HMDs) can resolve vergence-accommodation conflicts, which may cause visual discomfort and fatigue. However, light-field HMDs have a narrow field of view (FOV), owing to their small display size and optical configuration. We increased the FOV by adopting a large display and shifting the elemental image from the back of the corresponding microlens. In this study, we proposed a method to generate intermediate images as virtual images to reduce the distance between the microlens array and the eyepiece. We also designed a microlens array suitable for the proposed method and successfully reduced the device depth by 44&#37;.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1141">Perceptually-Based Optimization for Radiometric Projector Compensation</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: D13 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Ryo Akiyama: NTT&#59; Taiki Fukiage: NTT&#59; Shin'ya Nishida: Kyoto University</i></p>
    
    
        <p>Teaser Video: <a href="https://youtu.be/50TtJgfIyvI" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1141" class="wrap-collabsible"> <input id="collapsibleabstractPO1141" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1141" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Radiometric compensation techniques have been proposed to manipulate the appearance of arbitrarily textured surfaces using projectors. However, due to the limited dynamic range of the projectors, these compensation techniques often fail under bright environmental lighting or when the projection surface contains high contrast textures, resulting in clipping artifacts. To address this issue, we propose to apply a perceptually-based tone mapping technique to generate compensated projection images. The experimental results demonstrated that our approach minimizes the clipping artifacts and contrast degradation under challenging conditions.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="PO1145">Effects of Mirrors on User Behavior in Social Virtual Reality Environments</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: D14 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Takayuki Kameoka: The University of Electro-Communications&#59; Seitaro Kaneko: The University of Electro-Communications</i></p>
    
    
        <p>Teaser Video: <a href="https://youtu.be/_ndl4GEVsk4" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1145" class="wrap-collabsible"> <input id="collapsibleabstractPO1145" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1145" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>The authors have observed that users gather in front of mirrors on VRSNS such as VRChat. Based on these observations, we hypothesized that mirrors attracted users and conducted an experiment in a controlled environment. The participants were requested to converse in pairs in a VR space with mirrors and posters, and their behavior was recorded. Results showed that, although a certain number of users gathered in front of the mirror, it did not significantly increase their chance of staying. Conversely, we received comments such as &quot;I feel relaxed when I go in front of the mirror&quot;.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="PO1151">Implementation of an Authoring Tool for Wheelchair Simulation with Visual and Vestibular Feedback</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: D21 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Takumi Okawara: Nihon University&#59; Kousuke Motooka: Graduate School of integrated Basic Sciences, Nihon University&#59; Kazuki Okugawa: Nihon University&#59; Akihiro Miyata: Nihon University</i></p>
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=Zi1uw93XmjY" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1151" class="wrap-collabsible"> <input id="collapsibleabstractPO1151" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1151" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In this study, we develop a wheelchair simulator that provides visual and vestibular feedback at a low cost by leveraging the combination of vection-inducing movies displayed on a head-mounted display and vestibular feedback provided by an electric-powered wheelchair. However, this simulator requires users to manually create a synchronized pair of a VR movie and a motion scenario (chronological control commands for the wheelchair), which necessitates considerable effort and experience. In this paper, we introduce a novel authoring tool that generates a pair of a VR movie and a motion scenario based on a few parameters entered by the user.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="PO1154">Robust Tangible Projection Mapping with Multi-View Contour-Based Object Tracking</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: D22 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Yuta Halvorson: The University of Electro-Communications&#59; Takumi Saito: The University of Electro-Communications&#59; Naoki Hashimoto: The University of Electro-Communications</i></p>
    
    
        <p>Teaser Video: <a href="https://youtu.be/C6BhwqSlHsE" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1154" class="wrap-collabsible"> <input id="collapsibleabstractPO1154" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1154" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Dynamic projection mapping for moving objects can greatly enhance an augmented reality representation by using projected images because the target object can be freely moved. However, the interaction between the target object and the user has not been sufficiently considered. In this research, we propose tangible projection mapping, with which a user can grasp an object with their hands and move it freely. It uses a simple configuration of two infrared cameras to realize dynamic projection mapping that is robust to hand occlusion.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1161">Design of a VR Action Observation Tool for Rhythmic Coordination Training</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: D23 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>James Jonathan Pinkl: University of Aizu&#59; Michael Cohen: University of Aizu</i></p>
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=l209jE22g5A" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1161" class="wrap-collabsible"> <input id="collapsibleabstractPO1161" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1161" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Motor learning applications, particularly those with first-person virtual reality Action Observation features, have achieved positive results in a variety of fields. This project entails development of a first-person VR application designed to help musicians learn rhythm and the body coordination needed to express rhythm. Utilizing realtime tracking of user hand position, various accompanying visual media, and spatial audio with outside-the-head localization, the tool provides a new way to learn and improve rhythm.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1180">Interpersonal Distance to a Speaking Avatar: Loudness Matters Irrespective of Contents</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: D24 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Kota Takahashi: Toyohashi University of Technology&#59; Yasuyuki Inoue: Toyohashi University of Technology&#59; Michiteru Kitazaki: Toyohashi University of Technology</i></p>
    
    
        <p>Teaser Video: <a href="https://youtu.be/DV1v8-KbTXw" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1180" class="wrap-collabsible"> <input id="collapsibleabstractPO1180" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1180" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>It is important for us to maintain appropriate interpersonal distance depending on situations in effective and safe communications. We aimed to investigate the effects of speech loudness and clarity on the interpersonal distance towards an avatar in a virtual environment. We found that the louder speech of the avatar made the distance between the participants and the avatar larger than the quiet speech, but the clarity of the speech did not significantly affect the distance. These results suggest that the perception of loudness modulates the interpersonal distance towards the virtual avatar to maintain the intimate equilibrium.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1182">A Skin Pressure-type Grasping Device to Reproduce Impulse Force for Virtual Ball Games</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: D38 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Kazuma Yoshimura: Nara Institute of Science and Technology&#59; Naoya Isoyama: Nara Institute of Science and Technology&#59; Hideaki Uchiyama: Nara Institute of Science and Technology&#59; Nobuchika Sakata: Ryukoku University&#59; Kiyoshi Kiyokawa: Nara Institute of Science and Technology&#59; Yoshihiro Kuroda: University of Tsukuba</i></p>
    
    
        <p>Teaser Video: <a href="https://youtu.be/V_xgCKtYs44" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1182" class="wrap-collabsible"> <input id="collapsibleabstractPO1182" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1182" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Tactile feedback is crucial to improve the realism of virtual sports, which are popular applications of virtual reality (VR). However, few wearable tactile feedback devices can produce an impulse force in midair. We propose a graspable impulse force presentation device by using a voice coil motor. In the evaluation experiment, the result suggests that the proposed device improves the sense of realism compared to a conventional VR controller with vibration.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1187">Virtual Touch Modulates Perception of Pleasant Touch</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: D37 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Gakumaru HARAGUCHI: Toyohashi University of Tecknology&#59; Michiteru Kitazaki: Toyohashi University of Technology</i></p>
    
    
        <p>Teaser Video: <a href="https://youtu.be/oa9gJ7Hq9tw" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1187" class="wrap-collabsible"> <input id="collapsibleabstractPO1187" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1187" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Pleasant touch by gentle stroking is processed by C-tactile afferents, and important for emotional and social touch in human communication. We aimed to investigate the effects of visual touch in a virtual environment on the perception of pleasantness of tactile touch. Five velocities were used for the tactile brushing and virtual brushing (0.3 - 30 cm/s), and participants answered the perceived pleasantness of the tactile touch irrespective of visual stimulus. We found that the perception of pleasant touch was significantly modulated by the velocity of visual brushing. Thus, the pleasant touch would be perceived by integrating vision and tactile perception.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1189">An Examination on Reduction of Displayed Character Shake while Walking in Place with AR Glasses</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: D36 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Hiromu Koide: Utsunomiya University&#59; Kei Kanari: college&#59; Mie Sato: Utsunomiya University</i></p>
    
    
        <p>Teaser Video: <a href="https://youtu.be/9OwTyee6uME" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1189" class="wrap-collabsible"> <input id="collapsibleabstractPO1189" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1189" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In recent years, augmented reality (AR) has started to be used in our daily lives. AR glasses are used when walking, which is a normal part of daily life, but walking causes the text displayed on the glasses to shake. This reduces both readability and our attention to what is in front of us, and increases discomfort. We propose a method of fixing the text to take account of shaking while walking to reduce these adverse effects. Experiments revealed the effectiveness of our reduction method and its influence on the distance of the text display.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1207">Knowing the Partner's Objective Increases Embodiment towards a Limb Controlled by the Partner</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: D35 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Harin Manujaya Hapuarachchi: Toyohashi University of Technology&#59; Michiteru Kitazaki: Toyohashi University of Technology</i></p>
    
    
        <p>Teaser Video: <a href="https://youtu.be/Ht86aJO09Vo" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1207" class="wrap-collabsible"> <input id="collapsibleabstractPO1207" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1207" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We have developed a joint avatar of which two participants simultaneously control left and right-side limbs in first-person view in a virtual environment. We aimed to investigate whether having a common objective with a partner affects sense of embodiment towards a limb controlled by the partner. Participants performed reaching tasks using the joint avatar. We found that the embodiment towards the arm controlled by the partner was significantly higher when the participant dyads shared a common objective or when they were allowed to see their partner's goal, compared to when their partner's goal was unknown to them.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1215">On the Effectiveness of Conveying BIM Metadata in VR Design Reviews for Healthcare Architecture</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: D41 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Emma Buchanan: University of Canterbury&#59; Giuseppe Loporcaro: University of Canterbury&#59; Stephan Lukosch: University of Canterbury</i></p>
    
    
        <p>Teaser Video: <a href="https://youtu.be/MQkaOB4BlaQ" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1215" class="wrap-collabsible"> <input id="collapsibleabstractPO1215" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1215" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This research seeks to assess whether Virtual Reality (VR) can be used to convey Building Information Modelling (BIM) metadata alongside geometric and spatial data in a virtual environment, and by doing so, determine if this increases the understanding of the design by the stakeholder. A user study assessed participants performance and preference for conducting design reviews in VR or using a traditional design review system of PDF drawings and a 3D model. Early results indicate VR was preferred with fewer errors made during assessment and a higher System Usability Scale SUS score.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="PO1218">Towards a Virtual Reality Math Game for Learning In Schools - A User Study</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: D42 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Meike Belter Belter: University of Canterbury&#59; Heide Lukosch: University of Canterbury</i></p>
    
    
        <p>Teaser Video: <a href="https://youtu.be/D1_w_q8Xur4" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1218" class="wrap-collabsible"> <input id="collapsibleabstractPO1218" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1218" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In recent years, immersive Virtual Reality (VR) has gained popularity among young users as a new technology for entertainment gaming. While VR remains majorly used for entertainment purposes, 3D desktop games are already used in schools. This study takes a closer look at the suitability for VR games to be used in a formal educational environment, and its potential to enrich existing game based learning approaches. Based on learning needs of in particular easily distracted and inattentive children, an immersive VR math game was created and tested on 15 children aged 11-12.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="PO1219">Motion Correction of Interactive CG Avatars Using Machine Learning</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: D43 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Ko Suzuki: Utsunomiya University&#59; Hiroshi Mori: Utsunomiya University&#59; Fubito Toyama: Utsunomiya University</i></p>
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=7mX7fx44ljM" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1219" class="wrap-collabsible"> <input id="collapsibleabstractPO1219" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1219" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Motion capture allows users to control their CG avatar via their own movements. However, the composed avatar motion fails to deliver the actual input movements if the user's motion information is not accurately captured due to measurement errors. In this paper, we propose a method that complements a user's motion according to the motion of another person for a two-party motion with interaction. This method is expected to compose avatar motions that look natural to the other person while emphasizing the actual motions of the user.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="PO1220">Adding Difference Flow between Virtual and Actual Motion to Reduce Sensory Mismatch and VR Sickness while Moving</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: D44 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Kwan Yun: Korea University&#59; Gerard Jounghyun Kim: Korea University</i></p>
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=R7cAmW5iyMU&amp;ab_channel=kwanyun" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1220" class="wrap-collabsible"> <input id="collapsibleabstractPO1220" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1220" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Enjoying Virtual Reality in vehicles presents a problem because of the sensory mismatch and sickness. While moving, the vestibular sense perceives actual motion in one direction, and the visual sense, visual motion in another. We propose to zero out such physiological mismatch by mixing in motion information as computed by the difference between those of the actual and virtual, namely, &quot;Difference&quot; flow. We present the system for computing and visualizing the difference flow and validate our approach through a small pilot field experiment. Although tested only with a low number of subjects, the initial results are promising.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1225">Event Synthesis for Light Field Videos using Recurrent Neural Networks</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: E31 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Zhicheng Lu: The University of Sydney&#59; Xiaoming Chen: Beijing Technology and Business University&#59; Yuk Ying Chung: The University of Sydney&#59; Sen Liu: University of Science and Technology of China</i></p>
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=lfTb8FNg-wc" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1225" class="wrap-collabsible"> <input id="collapsibleabstractPO1225" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1225" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Light field videos (LFVs) yield higher complexity in performing computer vision tasks. The emerging event cameras offer new means for light-weight processing of LFVs, but it is infeasible to build an event camera array due to their high costs. In this poster, we propose a novel &quot;event synthesis for light field videos&quot; (ES4LFV) model by using recurrent neural networks and build a preliminary dataset. The ES4LFV can synthesize events for light field videos from a LFV camera array and single event camera. The experimental results show that ES4LFV outperforms the traditional method by 3.1dB in PSNR.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="PO1227">Towards Controlling Whole Body Avatars with Partial Body-Tracking and Environmental Information</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: E28 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Koji Yamada: Utsunomiya University&#59; Hiroshi Mori: Utsunomiya University&#59; Fubito Toyama: Utsunomiya University</i></p>
    
    
        <p>Teaser Video: <a href="https://youtu.be/qabKBsbDVZI" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1227" class="wrap-collabsible"> <input id="collapsibleabstractPO1227" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1227" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In body-tracking-based avatar manipulation, the user's motion is reflected in their avatar, which creates a high level of immersion. Conversely, the user is required to give a detailed performance similar to that of the avatar to be composed.<br />In this research, we aim to produce avatar motion that reflects the user's intended actions and maintains consistency with the VR environment by inputting the user's posture. The avatar's body motion was generated by inputting the user's motion and VR environmental information into the motion configuration network, which was developed using machine learning.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1240">Geometric Calibration with Multi-Viewpoints for Multi-Projector Systems on Arbitrary Shapes Using Homography and Pixel Maps</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: E21 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Atsuya Ueno: Wakayama University &#59; Toshiyuki Amano: Wakayama University&#59; Chisato Yamauchi: Misato astronomical observatory</i></p>
    
    
        <p>Teaser Video: <a href="https://youtu.be/OnK5vSUdDUw" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1240" class="wrap-collabsible"> <input id="collapsibleabstractPO1240" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1240" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We propose a geometrical calibration method for the MISATO astronomical observatory in Japan. The primary objective of our projection system is enabling a large-scale geometrical projection calibration for near-planner arbitrarily-shaped ground projection with a multi-projector system via temporally-placed camera capturing. The obtained results exhibited an average distortion reduction of 84.7&#37; less than the simple homography-based method.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1247">Bouncing Seat: An Immersive Virtual Locomotion Interface with LSTM Based Body Gesture Estimation</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: E22 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Yoshikazu Onuki: Digital Hollywood University</i></p>
    
    
        <p>Teaser Video: <a href="https://youtu.be/QyitgAwqEJc" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1247" class="wrap-collabsible"> <input id="collapsibleabstractPO1247" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1247" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>A pilot study of the bouncing seat system is presented. This system consists of an air cushion and gravicorder. The former enables users to move easier and feel lively motion. The latter and its computing units detect users' body sway and recognize its gestures. Four commands (walk, right turn, left turn, and jump) and associated intuitive body gestures were designed. The estimator based on the multi-timescale LSTM was trained using newly created body gesture dataset and achieved 88&#37; inference accuracy. Results suggest that our system has potential for providing a higher sense of immersion and enjoyment than the joysticks.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="PO1249">Hype Live: Biometric-based Sensory Feedback for Improving the Sense of Unity in VR Live Performance</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: E23 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Masashi Abe: Nara Institute of Science and Technology&#59; Akiyoshi Takuto: Nara Institute of Science and Technology&#59; Isidro Mendoza Butaslac III: Nara Institute of Science and Technology&#59; Zhou Hangyu: Nara Institute of Science and Technology&#59; Taishi Sawabe: Nara Institute of Science and Technology</i></p>
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=mu7IPN9rk8o" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1249" class="wrap-collabsible"> <input id="collapsibleabstractPO1249" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1249" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We propose Hype Live, a system to improve the sense of unity by sharing responses through visual, auditory, and haptic stimuli based on the biometrics of the participants in VR live performances. In this field, the sharing of reactions among the participants is one of the most important factors in improving the sense of unity. However, not many past studies have provided feedback to the participants' senses. Therefore, as a prototype of Hype Live, we used a vibration device to provide haptic feedback and recreated moshing scenes where the participants violently collide with each other like a real live performance.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="PO1250">Sense of Agency on Handheld AR for Virtual Object Translation</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: E14 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Wenxin Sun: Xi'an Jiaotong-Liverpool University&#59; Mengjie Huang: Xi'an Jiaotong-Liverpool University&#59; Chenxin Wu: Xi'an Jiaotong-Liverpool University&#59; Rui Yang: Xi'an Jiaotong-Liverpool University</i></p>
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=LI5yXT10jdo" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1250" class="wrap-collabsible"> <input id="collapsibleabstractPO1250" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1250" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Handheld augmented reality (AR) interfaces are applied in various programs nowadays, and the degrees of freedom (DoF) of translation modes have become significant on these interfaces. Sense of agency (SoA), emphasizing one's feeling of control, has emerged as an essential index of user experience. However, little was known about users' feelings of control with different translation modes in literature. Hence, this paper focuses on users' SoA in different translation modes by assessing subjective and objective measures. Correlations between SoA and translation modes were explored on handheld AR interfaces, revealing that the 1DoF translation mode was associated with higher SoA.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1252">User-Defined Interaction Using Everyday Objects for Augmented Reality First Person Action Games</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: E24 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Mac Greenslade: University of Canterbury&#59; Adrian James Clark: University of Canterbury&#59; Stephan Lukosch: University of Canterbury</i></p>
    
    
        <p>Teaser Video: <a href="https://youtu.be/szgK1-omCeM" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1252" class="wrap-collabsible"> <input id="collapsibleabstractPO1252" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1252" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In this paper, we present an elicitation study to explore how people use everyday objects for augmented reality first person games. 24 participants were asked to select items from a range of everyday objects to use as controllers for three different classes of virtual object. Participants completed tasks using their selected items and rated the experience using the Augmented Reality Immersion (ARI) questionnaire. Results indicate no strong consensus linking any specific everyday object to any virtual object across our testing population. Based on these findings, we recommend developers provide the ability for users to choose the everyday objects they prefer.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1256">AmbientTransfer: Presence Enhancement by Converting Video Ambient to Users' Somatosensory Feedback</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: E27 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Xunshi Li: School of Computer Science and Engineering, Beijing Technology and Business University&#59; Xiaoming Chen: Beijing Technology and Business University&#59; Yuk Ying Chung: The University of Sydney&#59; Qiang Qu: The University of Sydney</i></p>
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=xSy7YdeFN6o" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1256" class="wrap-collabsible"> <input id="collapsibleabstractPO1256" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1256" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Haptic feedback can improve users' presence when watching immersive videos. In this work, we present our &quot;AmbientTransfer&quot; framework for demonstrating how to &quot;embed&quot; users in the video ambient with somatosensory feedback. Particularly, AmbientTransfer can obtain and convert video ambient factors, e.g., rain intensities, into dynamic haptic feedback by slight electrostimulation in various levels. Then AmbientTransfer maps various levels of haptic feedback to different body parts of users wearing the emerging electrostimulation suit. Preliminary experimental results show that AmbientTransfer can considerably enhance the users' presence. We believe that AmbientTransfer is worth of further exploration for exploiting its full potentials.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="PO1257">Comparing Physiological and Emotional Effects of Happy and Sad Virtual Environments Experienced in Video and Virtual Reality</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: E26 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Yuankun Zhu: University of Queensland&#59; Arindam Dey: University of Queensland</i></p>
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=s8TwrhjWBmk" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1257" class="wrap-collabsible"> <input id="collapsibleabstractPO1257" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1257" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Virtual Reality (VR) could give users a more immersive experience than other non-immersive mediums. In this study, we explored differences in emotional and physiological effects between videos and VR using two different sets of contents to evoke happy and sad emotions. In this within-subjects controlled experiment we collected real-time heart rate and positive and negative affect schedule (PANAS) to measure physiological and emotional effects. Our results showed that VR triggers stronger emotions and higher heart rate than videos.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="PO1259">Toward Understanding the Effects of Visual and Tactile Stimuli to Reduce the Sensation of Movement with XR Mobility Platform</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: E25 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Taishi Sawabe: Nara Institute of Science and Technology&#59; Masayuki Kanbara: Nara Institute of Science and Technology&#59; Yuichiro Fujimoto: Nara Institute of Science and Technology&#59; Hirokazu Kato: Nara Institute of Science and Technology</i></p>
    
    
        <p>Teaser Video: <a href="https://youtu.be/nPjwXP84bmQ" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1259" class="wrap-collabsible"> <input id="collapsibleabstractPO1259" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1259" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This paper investigates a reduction method for passenger's movement sensation with the XR mobility platform mounted on an autonomous vehicle to improve passenger comfort during auto-driving. We investigate a reduction method that controls passenger's sense of movement by controlling visual and tactile perception using a multimodal XR mobility platform which consists of an immersive display and a motion platform with a tilting seat. The result of 30 subjects shows the sense of movement perceived by the passenger was reduced significantly when both visual acceleration and tactile acceleration control method was activated inside a moving autonomous vehicle.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1265">Creating 3D Personal Avatars with High Quality Facial Expressions for Telecommunication and Telepresence</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: E32 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Michal Joachimczak: NICT UCRI&#59; Juan Liu: NICT UCRI&#59; Hiroshi Ando: NICT UCRI</i></p>
    
    
        <p>Teaser Video: <a href="https://youtu.be/fN5D6lwXfLc" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1265" class="wrap-collabsible"> <input id="collapsibleabstractPO1265" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1265" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This study aims at providing a low-cost solution for telepresence where people are reconstructed as 3D avatars using an ordinary webcam, while still exhibiting abundant facial information (such as micro-expressions) that are critical for face-to-face communication. We estimate the basic 3D shape and texture of the body from a set of video frames, and then subsequently update its body pose, facial expression, and facial texture in each frame. Our method is expected to reduce the entry barrier of VR systems and create an embodied telecommunication that conveys rich information and subtle emotional changes to deepen mutual understanding at a distance.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1290">Jitsi360: Using 360 Images for Live Tours</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: E33 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Alaeddin Nassani: University of Auckland&#59; Huidong Bai: The University of Auckland&#59; Mark Billinghurst: University of South Australia</i></p>
    
    
        <p>Teaser Video: <a href="https://youtu.be/lLSCJWu4ncU" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1290" class="wrap-collabsible"> <input id="collapsibleabstractPO1290" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1290" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In this poster, we present a system for sharing immersive 360-degree images for live tours. While the sharing of prerecorded 360 video and images is becoming commonplace, there have been fewer systems presented that support live-sharing of 360 images. We customised a video conferencing platform to enable dozens of people to see the same 360-degree content together while having a live call. We describe our system and pilot user study results from using it for a virtual guided tour. Compared to sharing non 360-degree images on Zoom, our system was felt to be more immersive, enjoyable, and easy to use.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="PO1293">Apparent shape manipulation by light-field projection onto a retroreflective surface</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: E34 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>jion kanaya: Wakayama University&#59; Toshiyuki Amano: Wakayama University</i></p>
    
    
        <p>Teaser Video: <a href="https://youtu.be/IULjfYJMfrE" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1293" class="wrap-collabsible"> <input id="collapsibleabstractPO1293" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1293" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In order to optically correctly present metallic luster and structural color, it is necessary to reproduce the changes in brilliance and color that accompany the movement of the viewpoint.<br />Light-field projection onto a retroreflective surface can optically present texture depending on the viewpoint. By applying this, it is thought that the apparent shape can be manipulated depending on the viewpoint. This research proposes an optical illusion that manipulates the apparent shape of the 3D object with lightfield projection based on a perceptual normal map transformation.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="PO1296">Enabling Augmented Reality Incorporate with Audio on Indoor Navigation for People with Low Vision</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: E13 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Zihao Chi: Nara Institute of Science and Technology&#59; Zhaofeng Niu: Nara Institute of Science and Technology&#59; Taishi Sawabe: Nara Institute of Science and Technology</i></p>
    
    
        <p>Teaser Video: <a href="https://youtu.be/-rkDuUTwOFY" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1296" class="wrap-collabsible"> <input id="collapsibleabstractPO1296" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1296" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Indoor navigation is difficult for low vision people, even they can benefit from visual cue. However, visual rating has been neglected, which can provide assistant for different visual impairments. In this paper, we propose an Augmented Reality (AR) application that measures the visual rating firstly. According to visual rating, different navigation service, including visual and audio cue, is provided for users. Object detection and depth estimation are utilized for avoiding obstacles. We conducted an exploratory design study to investigate our idea. In experiment, Snellen chart is displayed on Hololens2 and a pilot study has been conducted. The strategy on both visual aid and audio cue will be tested by a user study for the next step</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1303">Flick Typing: Toward A New XR Text Input System Based on 3D Gestures and Machine Learning</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: E12 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Tian Yang: University of Southern California&#59; Powen Yao: University of Southern California&#59; Michael Zyda: USC</i></p>
    
    
        <p>Teaser Video: <a href="https://youtu.be/3U6LZ25O-xc" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1303" class="wrap-collabsible"> <input id="collapsibleabstractPO1303" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1303" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We propose a new text entry input method in Extended Reality that we call Flick Typing. Flick Typing utilizes the user's knowledge of a QWERTY keyboard layout, but does not explicitly provide visualization of the keys, and is agnostic to user posture or keyboard position. To type with Flick Typing, users will move their controller to where they think the target key is with respect to the controller's starting position and orientation, often with a simple flick of their wrists. Machine learning model is trained and used to adapt to the user's mental map of the keys in 3D space.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="PO1304">Feasibility of mapping engagement ratios to levels of task complexity within VR environments</h3>
    <p><strong>Poster</strong></p> 
    
<p> <small><strong style="color: black;"> Booth: E11 
    
<!--     --> 
        
</strong></small> <br /> </p>

    

    <p><i>Yobbahim J Vite: University of Calgary&#59; Yaoping Hu: University of Calgary</i></p>
    
    
        <p>Teaser Video: <a href="https://youtu.be/u5ZrHZnFDrI" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1304" class="wrap-collabsible"> <input id="collapsibleabstractPO1304" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1304" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This paper studied the feasibility of mapping an engagement ratio onto a level of task complexity, when human participants undertook interactive tasks within a virtual reality (VR) environment. Each human participant used a haptic device to push a ball-shaped object through a pipe. There were a total of three pipes, which had three different shapes corresponding to the levels of task complexity mathematically. An electroencephalogram (EEG) device recorded the brain activity of the participant while undertaking the task. The outcomes of the study confirmed the feasibility of mapping the engagement ratio with the levels of task complexity.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
</div>

<div>
    
    <h2 id="PB"> Posters - Expo Hall B</h2>
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1023">Taming Cyclops: Mixed Reality Head-Mounted Displays as Laser Safety Goggles for Advanced Optics Laboratories</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: B14 </strong></small> <br /> </p>

<!---->
    
    <p><i>Ke Li</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/wv7lltZVv30" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1023" class="wrap-collabsible"> <input id="collapsibleabstractC1023" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1023" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In this poster paper, we present a mixed reality application for laser eye protection based on a video see-through head-mounted display. With our setup, laser lab users perceive the real environment through the head-mounted display, using it as a substitute for laser safety goggles required by health and safety regulations. We designed and evaluated our prototype with a human-centered computing approach at the Deutsches Elektronen-Synchrotron where there exists some of the most advanced and extreme optics laboratory working conditions. We demonstrated that virtual reality headsets can be an attractive future alternative to conventional laser safety goggles.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1036">Absence Agents: Mitigating Interruptions in Extended Reality Remote Collaboration</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: B13 </strong></small> <br /> </p>

<!---->
    
    <p><i>Huyen Nguyen</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/m80kWoJKXdg" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1036" class="wrap-collabsible"> <input id="collapsibleabstractC1036" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1036" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Although dealing with interruptions in remote collaboration has been studied in general, few works have done this for Extended Reality (XR) collaboration. With the current explosion of interest in XR collaboration, we explore the negative impacts of interruptions in synchronous distributed XR environments and propose a novel concept for dealing with them: Absence Agents. We present their requirements analysis, design, and a prototype implementation. We believe that our concept and design of Absence Agents are important for practitioners and researchers alike, as they highlight avenues for future research.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="C1043">Group WiM: A Group Navigation Technique for Collaborative Virtual Reality Environments</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: B12 </strong></small> <br /> </p>

<!---->
    
    <p><i>Vuthea Chheang</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/ZLbcZU4sQ7Y" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1043" class="wrap-collabsible"> <input id="collapsibleabstractC1043" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1043" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In this work, we present a group World-in-Miniature (WiM) navigation technique that allows a guide to navigate a team in collaborative virtual reality (VR) environments. We evaluated the usability, discomfort, and user performance of the technique compared to state-of-the-art group teleportation in a user study. The results show that the proposed technique induces less discomfort for the guide and has slight usability advantages. The group WiM technique seems superior regarding task completion time for obstructed target destination. The group WiM technique provides potential benefits for effective group navigation in complex virtual environments and harder-to-reach target locations.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1072">Impact of Parameter Disentanglement on Collaborative Alignment</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: B11 </strong></small> <br /> </p>

<!---->
    
    <p><i>Tianyu Song</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/eulUFXdOrD0" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1072" class="wrap-collabsible"> <input id="collapsibleabstractC1072" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1072" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>The interactive alignment of real and virtual content in AR is often non-trivial. Positional errors along the user's view direction frequently lead to the misjudgment of the object's depth. This work takes advantage of alternative users' viewpoints in collaborative settings to mitigate these errors. Furthermore, we systematically restrict the parameters used to control the virtual content's pose and investigate the impact of sharing and disentangling such parameters. Results from this work show that alignment schemes that disentangle the control parameters improve overall alignment accuracy with a similar workload for the users and no significant increase in execution time.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1094">Light VR Client for Point Cloud Navigation with 360&deg; Images</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: B21 </strong></small> <br /> </p>

<!---->
    
    <p><i>Cl&eacute;ment Dluzniewski</i></p>
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=DCQTostPJV0" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1094" class="wrap-collabsible"> <input id="collapsibleabstractC1094" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1094" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Since point clouds require a large amount of data to be visually pleasing, they tend to be voluminous. Hence, hardware with limited computational and memory capabilities may not be able to handle such large data structures. Here, we propose a light VR client to explore a static point cloud, stored in a remote server, through 360&deg; images. The client visualizes in an HMD the omnidirectional rendering of the point cloud and moves to another position with a teleportation metaphor. The main advantage of our proposition is the ability to work on modest hardware without a continuous high bandwidth.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="C1099">Vibrating tilt platform enhancing immersive experience in VR</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: B22 </strong></small> <br /> </p>

<!---->
    
    <p><i>Dorota Kami&nacute;ska</i></p>
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=JMPiNTJlYBY" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1099" class="wrap-collabsible"> <input id="collapsibleabstractC1099" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1099" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Virtual reality systems, since their initial development, suffered some disadvantages. One of them was the fact that they had only visual interfaces. This limitation, however, has been successfully overcome with the development of haptic technology. Peripheral solutions reinforcing and enriching VR experience are now commonplace, and many haptic systems are being developed for deepening VR immersion. This paper discusses a new peripheral solution - a vibrating tilt platform with three angles of inclination to be used for enhancing the VR experience.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1113">Enabling Virtual Reality Interactions in Confined Spaces by Re-Associating Finger Motions</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: B28 </strong></small> <br /> </p>

<!---->
    
    <p><i>Wen-Jie Tseng</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/l2ugJVlk45w" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1113" class="wrap-collabsible"> <input id="collapsibleabstractC1113" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1113" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>As Virtual Reality (VR) headsets become mobile, people can interact in public places with applications often requiring large arm movements. However, using these open gestures is often uncomfortable and sometimes impossible in confined and public spaces (e.g., commuting in a vehicle). We introduce the concept of finger mapping, re-associating small-scale finger motions onto virtual arms in a larger VR space. Finger mapping supports various interactions (e.g., arms swinging movement, selection, manipulation, and locomotion) when the environment is constrained and does not allow large gestures. Finally, we discuss the opportunities and challenges of using finger mapping for VR interactions.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="C1125">Understanding Shoulder Surfer Behavior Using Virtual Reality</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: B27 </strong></small> <br /> </p>

<!---->
    
    <p><i>Yasmeen Abdrabou</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/HHgy8bDRPXc" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1125" class="wrap-collabsible"> <input id="collapsibleabstractC1125" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1125" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We explore how attackers behave during shoulder surfing. Such behavior is difficult to study as it is often opportunistic and can occur wherever potential attackers can observe other people's private screens. Therefore, we investigate shoulder surfing using virtual reality (VR). We recruited 24 participants and observed their behavior in two virtual waiting scenarios: at a bus stop and in an open office space. In both scenarios, avatars interacted with private screens displaying different content, thus providing opportunities for shoulder surfing. From the results, we derive an understanding of factors influencing shoulder surfing behavior.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1175">Priority-Dependent Display of Notifications in the Peripheral Field of View of Smart Glasses</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: B26 </strong></small> <br /> </p>

<!---->
    
    <p><i>Anja Faulhaber</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/yqvPWwrtNm0" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1175" class="wrap-collabsible"> <input id="collapsibleabstractC1175" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1175" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We propose a concept for displaying notifications in the peripheral field of view of smart glasses aiming to achieve a balance between perception and distraction depending on the priority of the notification. We designed three different visualizations for notifications of low, medium, and high priority. To evaluate this concept, we conducted a study with 24 participants who reacted to the notifications while performing a primary task. Reaction times for the low-priority notification were significantly higher. The medium- and high-priority notifications did not show a clear difference.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="C1179">Studying the User Adaptability to Hyperbolic Spaces and Delay Time Scenarios</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: B25 </strong></small> <br /> </p>

<!---->
    
    <p><i>Ana Rita Rebelo</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/tPqrSp3VBCU" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1179" class="wrap-collabsible"> <input id="collapsibleabstractC1179" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1179" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>To create immersive virtual experiences, it is crucial to understand how users perceive Virtual Environments (VEs) and which interaction techniques are most appropriate for their tasks.
We created a tangible VE - the VR Lab - where it is possible to study space and time conditions to analyse the user's adaptability to different forms of interaction.
As a case study, we restricted the scope of the investigation to two morphing scenarios. The space morphing scenario compares the adaptability of users to Euclidean versus hyperbolic spaces. The time morphing scenario aims to establish from which values the visual delay affects performance.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1218">&quot;What a Mess!&quot;': Traces of Use to Increase Asynchronous Social Presence in Shared Virtual Environments</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: B31 </strong></small> <br /> </p>

<!---->
    
    <p><i>Linda Hirsch</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/YTsGGwH8Fdo" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1218" class="wrap-collabsible"> <input id="collapsibleabstractC1218" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1218" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Shared virtual environments (VEs) are challenged conveying and triggering users' feelings of social presence. 
Traces of use are implicit evidence of prior interactions that support social awareness in the real environment (RE). However, they have not been explored in VEs so far.
We investigate the traces' effect on users' perception of asynchronous social presences in a within-subject study (N=26) by comparing the users' experience with and without traces. 
The traces significantly increased the feeling of social presence. We contribute an initial exploration of the \emph{traces of use} concept in VE to design shared social spaces for long-term use.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1250">Beyond Flicker, Beyond Blur: View-coherent Metameric Light Fields for Foveated Display</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: B32 </strong></small> <br /> </p>

<!---->
    
    <p><i>Prithvi Kohli</i></p>
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=uCWhjDCPIWY" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1250" class="wrap-collabsible"> <input id="collapsibleabstractC1250" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1250" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Ventral metamers, pairs of images which may differ substantially in the periphery, but are perceptually identical, offer exciting new possibilities in foveated rendering and image compression, as well as offering insights into the human visual system. However, existing literature has mainly focused on creating metamers of static images. In this work, we develop a method for creating sequences of metameric frames, specifically light fields, with enforced consistency along the temporal, or angular, dimension. This greatly expands the potential applications for these metamers, and expanding metamers along the third dimension offers further new potential for compression.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1284">KARLI: Kid-friendly Augmented Reality for Primary School Health Education</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: B33 </strong></small> <br /> </p>

<!---->
    
    <p><i>Mariella Seel</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/6JG7izsWXh8" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1284" class="wrap-collabsible"> <input id="collapsibleabstractC1284" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1284" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Acquiring health knowledge is essential and starts already in primary school. Augmented Reality (AR) helps to convey complex topics in a more understandable way. In this work, we present the prototype of KARLI, the &quot;Kid-friendly Augmented Reality Learning Interface&quot;. This AR smartphone app for in-school use is designed for age level 8 to 10, enabling pupils to explore a 3D model of the human body based on the primary school curriculum. Underlining the importance of kid-friendly app development and testing, our evaluation results of 38 pupils and 3 teachers indicate that KARLI is suitable and helpful for health education in primary schools.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1306">Comparing principally imagination and interaction versions of a play anywhere mobile AR location-based story</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: B34 </strong></small> <br /> </p>

<!---->
    
    <p><i>Gideon Raeburn</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/TPuO4tyOOv8" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1306" class="wrap-collabsible"> <input id="collapsibleabstractC1306" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1306" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Augmented Reality (AR) allows virtual elements to be overlaid on the real world, providing new opportunities for location-based storytelling, by offering blended environments to more closely resemble those in a story. However, there is limited research considering how different interaction opportunities with such augmented surroundings, affect user engagement and immersion in such a story. A mobile AR app, Map Story 2, was developed to investigate this, offering a guided story-walk around a user's chosen location, either interacting with overlaid virtual objects to progress the story, or being asked to imagine the same events playing out at each augmented location.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1323">Design of Mentally and Physically Demanding Tasks as Distractors of Rotation Gains</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: B23 </strong></small> <br /> </p>

<!---->
    
    <p><i>Eike Langbehn</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/DZ--H0NIzlQ" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1323" class="wrap-collabsible"> <input id="collapsibleabstractC1323" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1323" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Rotation gains decouple real and virtual head turning. When reaching a boundary of the tracking space, the user's orientation can be reset by applying a rotation gain and forcing the user to do a certain task that requires head rotations. To identify what kind of tasks are best suited to mask the redirection, four tasks were designed that differentiate in their amount of mentally and physically demand: a spatial memory task, a verbal memory task, a physically exhausting task and a task which requires physical skill. A first pilot study was conducted to evaluate the influence on the redirection awareness.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="C1325">Minimaps for Impossible Spaces: Improving Spatial Cognition in Self-Overlapping Virtual Rooms</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: B24 </strong></small> <br /> </p>

<!---->
    
    <p><i>Eike Langbehn</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/ulTp3mtP230" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1325" class="wrap-collabsible"> <input id="collapsibleabstractC1325" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1325" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Natural walking in virtual reality is constrained by the physical boundaries of the tracking space.
Impossible spaces enlarge the virtual environment by creating overlapping architecture and letting multiple locations occupy the same physical space.
Minimaps, which are small representations of the environment, are a common method to assist with wayfinding and navigation.
Unfortunately, in a naive implementation of such minimaps for an environment with impossible spaces, the overlap would be obvious.
We investigated approaches to displaying impossible spaces on minimaps without attracting users' attention to the overlapping parts.
We conducted a study that investigated effects of minimaps on spatial cognition.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1370">STARE: Semantic Augmented Reality Decision Support in Smart Environments</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: E24 </strong></small> <br /> </p>

<!---->
    
    <p><i>Mengya Zheng</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/siY94Q4jFVw" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1370" class="wrap-collabsible"> <input id="collapsibleabstractC1370" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1370" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>The Internet of Things (IoT) facilitates real-time decision support within smart environments. Augmented Reality (AR) allows for the ubiquitous visualization of IoT-derived data, and AR visualization will simultaneously permit the cognitive and visual binding of information to the physical object(s) to which they pertain. Essential questions exist about efficiently filtering, prioritizing, determining relevance, and adjudicating individual information needs in real-time decision-making. To this end, this paper proposes a novel AR decision support framework (STARE) to support immediate decisions within a smart environment by augmenting the user's focal objects with assemblies of semantically relevant IoT data and corresponding suggestions.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1377">Emotional Support Companions in Virtual Reality</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: E26 </strong></small> <br /> </p>

<!---->
    
    <p><i>Linda Graf</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/VDEfJL2oqxc" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1377" class="wrap-collabsible"> <input id="collapsibleabstractC1377" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1377" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>According to social psychological models, the presence of another person or even a virtual character in stressful situations can have stress-reducing effects. Thereby, the outcome can depend on the congruency between one's mood and the perceived mood of the other person. This dependence guides the design of VR applications for stress reduction, which use different virtual companion designs depending on the emotional state of individual users. This paper describes an ongoing design and development process towards such an emotionally supportive companion and shares initial results concerning the perception and stress-reducing effects of positively- and negatively-minded companions.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="C1385">Heuristic Short-term Path Prediction for Spontaneous Human Locomotionin Virtual Open Spaces</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: C11 </strong></small> <br /> </p>

<!---->
    
    <p><i>Christian Hirt</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/2cF50ABMYlI" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1385" class="wrap-collabsible"> <input id="collapsibleabstractC1385" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1385" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Redirected Walking (RDW) shrinks large virtual environments to fit small physical tracking spaces while supporting natural locomotion. In predictive RDW, algorithms rely on predicting users' paths to adjust the induced redirection. Current predictors assume drastic simplifications or build on complex locomotion models. Further, also adapting existing predictive RDW algorithms to unconstrained open spaces exponentially increases their computational complexity. In this paper, we propose simple yet flexible path prediction models supporting dynamic virtual open spaces. Our proposed prediction models consist of a drop and a sector shape, defining an area, in which linear and clothoidic walking trajectories will be investigated.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1402">Towards Eye-Perspective Rendering for Optical See-Through Head-Mounted Displays</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: C12 </strong></small> <br /> </p>

<!---->
    
    <p><i>Gerlinde Emsenhuber</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/dADsQIE64n4" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1402" class="wrap-collabsible"> <input id="collapsibleabstractC1402" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1402" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Optical see-through (OST) HMDs are a typical platform for Augmented Reality and allows users to experience augmentations. Utilizing information of the real-world background, visualization algorithms adapt the layout and representation of content to improve legibility. Typically, background information is captured via built-in HMD cameras. However, HMD camera views of the real-world scene are distinctively different to the user's view through the OST HMD. We propose eye-perspective rendering as a solution to synthesize high fidelity renderings of the user's view for OST HMDs to enable adaptation algorithms to utilize visual information as seen from the user's perspective to improve placement, rendering and, thus, legibility of content.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1497">Improved Offset Handling in Hand-centered Object Manipulation Extending Ray-casting</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: C13 </strong></small> <br /> </p>

<!---->
    
    <p><i>Karljohan Lundin Palmerius</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/hWx0UxcCtS4" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1497" class="wrap-collabsible"> <input id="collapsibleabstractC1497" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1497" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>One of the most common types of interaction in Virtual Reality (VR) is pose manipulation. The simultaneous manipulation of both object position and orientation, 6 DoF interaction, that can be complex in desktop environments, can become simple in VR, by mapping the natural motion with a 3D interaction controller (wand) to the motion of the object. In this paper we acknowledge the importance of an offset variable in HOMER, present a way of handling the offset consistently when the user faces different directions during interaction, compare this to using naive static offset with respect to task completion time and number of re-grabs, and measure their respective System Usability Scale (SUS) score and Raw NASA Task Load Index (RTLX).</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="C1498">If I Share with you my Perspective, Would you Share your Data with me?</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: C14 </strong></small> <br /> </p>

<!---->
    
    <p><i>Tianyu Song</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/Trd9Y1AOYvU" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1498" class="wrap-collabsible"> <input id="collapsibleabstractC1498" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1498" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Real-time 3D reconstruction using multiple RGBD cameras and their online transmission facilitates the adoption of mixed reality telepresence. However, such a system can only cover a limited volume, and increasing the number of RGBD cameras is unfeasible due to setup complexity and space constraints. To address this issue, we present the concept of Dynamic 3D View Sharing, which complements the views of a 3D reconstruction system by the dynamic view of the user's HMD. Here, we present a markerless calibration method integrating these two seamlessly into the mixed reality telepresence systems without disrupting the current workflow.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1518">Proximity in VR: The Importance of Character Attractiveness and Participant Gender</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: C21 </strong></small> <br /> </p>

<!---->
    
    <p><i>Katja Zibrek</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/MorQ88_xyWk" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1518" class="wrap-collabsible"> <input id="collapsibleabstractC1518" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1518" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In this study, we expand upon recent evidence that the motion of virtual characters affects the proximity of users who are immersed with them in virtual reality (VR). Characters with attractive motions decrease proximity, while females prefer further distances from characters than males. No effect of character gender on proximity was found. We designed a similar experiment where users observed walking motions in VR which were displayed on male and female virtual characters. Our results show similar patterns found in previous studies, while some differences due to the specifics of the characters emerged.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="C1530">A Comparison of Input Devices for Precise Interaction Tasks in VR-based Surgical Planning and Training</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: C22 </strong></small> <br /> </p>

<!---->
    
    <p><i>Mareen Allgaier</i></p>
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=ixza7NYrZ_8" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1530" class="wrap-collabsible"> <input id="collapsibleabstractC1530" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1530" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We present a comparison of input devices for common interaction tasks in medical VR training and planning based on two relevant applications. The chosen devices, VR controllers, VR Ink, data gloves, and a real medical instrument, differ in their degree of specialization and their grip.
The conducted user study shows that the controllers and VR Ink performed significantly better than the other devices regarding precision. Concerning questionnaire results, no device stands out but most participants preferred the VR Ink for both applications.
These results can serve as a guide to identify an appropriate device for future medical VR applications.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1557">Interacting with a Torque-Controlled Virtual Human in Virtual Reality for Ergonomics Studies</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: C23 </strong></small> <br /> </p>

<!---->
    
    <p><i>Jacques Zhong</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/6gfz9rzWXE8" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1557" class="wrap-collabsible"> <input id="collapsibleabstractC1557" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1557" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This paper presents a new tool to help ergonomists conduct studies on digital human models (DHM) in a more intuitive and physically consistent way. To do so, a virtual reality setup was combined with a DHM in a real-time physics simulation. Therefore, the user is able to directly manipulate the DHM within the virtual workplace and quickly experiment with a variety of scenarios.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1572">Cloud-Based Cross-Platform Collaborative AR in Flutter</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: C24 </strong></small> <br /> </p>

<!---->
    
    <p><i>Christian Eichhorn</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/Ia6EJzKj3Jw" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1572" class="wrap-collabsible"> <input id="collapsibleabstractC1572" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1572" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We present a novel collaborative AR framework aimed at lowering the entry barriers and operating expenses of AR applications. It includes a cross-platform and cloud-based Flutter plugin combined with a web-based content management system allowing non-technical staff to take over operational tasks such as providing 3D models. To provide a SOA feature set, the AR Flutter plugin builds upon ARCore and ARKit and unifies the two frameworks using an abstraction layer written in Dart. Our contribution closes a gap by providing an AR framework seamlessly integrating with the familiar development process of cross-platform apps. With the accompanying content management system, AR can be used as a tool to achieve business objectives.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1634">Mixed Reality Support for Bridge Inspectors</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: C28 </strong></small> <br /> </p>

<!---->
    
    <p><i>Urs Riedlinger</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/3vw9nlg3_4M" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1634" class="wrap-collabsible"> <input id="collapsibleabstractC1634" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1634" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Bridge inspectors work for the safety of our infrastructure and mobility. In regular intervals, they conduct structural inspections - a manual task with a long-lasting and firmly normed analogue tradition. We propose a mixed analogue and digital workflow that includes Mixed Reality views that can be ready-to-hand for bridge inspectors during their work at and in a bridge. Our presented demonstrator was iteratively designed in a collaborative research project and turned into a tablet-based application to digitally support that work. It employs BIM data that contains 3D geometry-data and additional data about the structure, such as previous damage reports.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="C1645">Study of communication modalities for teaching distance information</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: C31 </strong></small> <br /> </p>

<!---->
    
    <p><i>Cassandre Simon</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/tpwFHLdRCbI" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1645" class="wrap-collabsible"> <input id="collapsibleabstractC1645" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1645" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We present an exploratory study to compare the haptic, visual, and verbal modalities for communicating distance information in a shared virtual environment. The results show that the visual modality decreased the distance estimation error while the haptic modality decreased the completion time. The verbal modality increased the sense of copresence but was the least preferred modality. These results suggest that a combination of modalities could improve communication of distance information to a partner. These findings can contribute to improve the design of collaborative VR systems and open new research perspectives on studying the effectiveness of multimodal interaction.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1081">Using 3D Reconstruction to create Pervasive Augmented Reality Experiences: A comparison</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: C38 </strong></small> <br /> </p>

<!---->
    
    <p><i>Miguel Neves: University of Aveiro&#59; Bernardo Marques: Universidade de Aveiro&#59; Tiago Madeira: Universidade de Aveiro&#59; Paulo Dias: University of Aveiro&#59; Beatriz Sousa Santos: University of Aveiro</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/YUP0r-UvQ74" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1081" class="wrap-collabsible"> <input id="collapsibleabstractPO1081" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1081" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This paper presents a prototype for configuration and visualization of Pervasive Augmented Reality (AR) experiences using two versions: desktop and mobile. It makes use of 3D scans from physical environments to provide a reconstructed digital representation of such spaces to the desktop version and enable positional tracking for the mobile. While the desktop presents a non-immersive setting, the mobile provides continuous AR in the physical environment. Both versions can be used to place virtual content and ultimately configure an AR experience. The authoring capabilities of the proposed solution were compared by conducting a user study focused on evaluating their usability.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="PO1082">Does Remote Expert Representation really matters: A comparison of Video and AR-based Guidance</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: C37 </strong></small> <br /> </p>

<!---->
    
    <p><i>Bernardo Marques: Universidade de Aveiro&#59; Samuel Silva: Universidade de Aveiro&#59; Paulo Dias: University of Aveiro&#59; Beatriz Sousa Santos: University of Aveiro</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/SPju4spZht4" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1082" class="wrap-collabsible"> <input id="collapsibleabstractPO1082" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1082" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This work describes a user study aimed at understanding how the remote expert representation affects the sense of social presence in scenarios of remote guidance. We compared a traditional video chat solution with an Augmented Reality (AR) annotation tool. These were selected due to ongoing research with partners from the industry sector, following the insights of a participatory design process. A well defined-problem was used, i.e., a synchronous maintenance task with 4 completion stages that required a remote expert using a computer to guide 26 on-site participants wielding a handheld device. The results of the study are described and discussed.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="PO1083">Whac-A-Mole: Exploring Virtual Reality (VR) for Upper-Limb Post-Stroke Physical Rehabilitation based on Participatory Design and Serious Games</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: C36 </strong></small> <br /> </p>

<!---->
    
    <p><i>Helder Paraense Serra: University of Aveiro&#59; Bernardo Marques: Universidade de Aveiro&#59; Paula Amorim: University of Beira Interior&#59; Paulo Dias: University of Aveiro&#59; Beatriz Sousa Santos: University of Aveiro</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/2hmMowFxLiU" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1083" class="wrap-collabsible"> <input id="collapsibleabstractPO1083" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1083" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This paper describes a Human-Centered Design methodology aimed at understanding how Virtual Reality (VR) can assist in post-stroke physical rehabilitation. Based on insights from stroke survivors and healthcare members, a serious game prototype is proposed. We focused on upper-limb rehabilitation, which inspired the game narrative and the movements users must perform. The game supports two modes: 1- normal version - users can use any arm to pick a virtual hammer and hit objects; 2- mirror version - converts a traditional approach to VR, providing the illusion that the arm affected by the stroke is moving. These were evaluated through a user study.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1095">Distinguishing Visual Fatigue, Mental Workload and Acute Stress in Immersive Virtual Reality with Physiological Data: pre-test results</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: C35 </strong></small> <br /> </p>

<!---->
    
    <p><i>Alexis Souchet: CNRS&#59; Weifei Xie: CNRS&#59; Domitile Lourdeaux: Sorbonne University Association, University of Technology of Compi&egrave;gne, CNRS</i></p>
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=SZOrf_7zrsc" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1095" class="wrap-collabsible"> <input id="collapsibleabstractPO1095" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1095" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Virtual Reality Induced Symptoms and Effects (VRISE) can arise. Experimental paradigms are heterogeneous to assess them, and various factors can induce physiological variations. Therefore, we developed a Stroop task to study and distinguish VRISE. We use eye tracking, ECG, EDA, and VRSQ, NASA-TLX, and STAI-6 questionnaires. Pre-tests have been conducted with 6 subjects exposed to 4 experimental conditions: control, dual task, stressful, and stereoscopy. Subjects report different subjective visual fatigue and mental workload but not stress between conditions. Several physiological features are different between conditions. A VRISE detector can be envisioned based on physiological data and questionnaires as an index.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1104">Towards Scalable and Real-time Markerless Motion Capture</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: C41 </strong></small> <br /> </p>

<!---->
    
    <p><i>Georgios Albanis: University of Thessaly&#59; Anargyros Chatzitofis: AC CODEWHEEL LTD&#59; Spyridon Thermos: AC Codewheel Ltd&#59; Nikolaos Zioulis: Independent Researcher&#59; Kostas Kolomvatsos: University of Thessaly</i></p>
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=f8lWahAPwUA" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1104" class="wrap-collabsible"> <input id="collapsibleabstractPO1104" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1104" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Human motion capture and perception without the need for complex systems with specialized cameras or wearable equipment is the holy grail for many human-centric applications. Here, we present a scalable markerless motion capture method that estimates 3D human poses in real-time using low-cost hardware. We do so by replacing the inefficient 3D joint reconstruction techniques, such as learnable triangulation and feature splatting, with a novel uncertainty-driven approach that exploits the available depth information and the edge sensors' spatial alignment to fuse the per viewpoint estimates into final 3D joint positions.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="PO1105">A Mixed Reality Guidance System for Blind and Visually Impaired People</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: C42 </strong></small> <br /> </p>

<!---->
    
    <p><i>Hannah Schieber: Friedrich Alexander University Erlangen-N&uuml;rnberg&#59; Constantin Kleinbeck: Friedrich-Alexander Universit&auml;t Erlangen-N&uuml;rnberg&#59; Charlotte Pradel: Friedrich Alexander University Erlangen-N&uuml;rnberg&#59; Luisa Theelke: Friedrich Alexander University Erlangen-N&uuml;rnberg&#59; Daniel Roth: Friedrich Alexander University Erlangen-N&uuml;rnberg</i></p>
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=gj1QBMwGdQE" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1105" class="wrap-collabsible"> <input id="collapsibleabstractPO1105" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1105" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Persons affected by blindness or visual impairments are challenged by spatially understanding unfamiliar environments. To obtain such understanding, they have to sense their environment closely and carefully. Especially objects outside the sensing area of analog assistive devices, such as a white cane, are simply not perceived and can be the cause of collisions. This project proposes a mixed reality guidance system that aims at preventing such problems. We use object detection and the 3D sensing capabilities of a mixed reality head mounted device to inform users about their spatial surroundings.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="PO1107">Holding Hands for Short-Term Group Navigation in Social Virtual Reality</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: C43 </strong></small> <br /> </p>

<!---->
    
    <p><i>Tim Weissker: Bauhaus-Universitaet Weimar&#59; Pauline Bimberg: Bauhaus-Universitaet Weimar&#59; Ankith Kodanda: Bauhaus-Universitaet Weimar&#59; Bernd Froehlich: Bauhaus-Universit&auml;t Weimar</i></p>
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=UPtn18b-jIE" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1107" class="wrap-collabsible"> <input id="collapsibleabstractPO1107" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1107" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Prior research has shown that social interactions in VR benefit from techniques for group navigation that bring multiple users to a common destination together. In this work, we propose the metaphor of holding onto another user's virtual hand for the ad-hoc formation of a navigational group and report on the positive results of an initial usability study in an exploratory two-user scenario.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1113">Stay Safe! Safety Precautions for Walking on a Conventional Treadmill in VR</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: C44 </strong></small> <br /> </p>

<!---->
    
    <p><i>Sandra Birnstiel: University of W&uuml;rzburg&#59; Sebastian Oberd&ouml;rfer: University of W&uuml;rzburg&#59; Marc Erich Latoschik: Department of Computer Science, HCI Group</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/P2Dvf64aeps" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1113" class="wrap-collabsible"> <input id="collapsibleabstractPO1113" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1113" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Conventional treadmills are used in virtual reality (VR) applications, such as for rehabilitation training or gait studies. However, using the devices in VR poses risks of injury. Therefore, this study investigates safety precautions when using a conventional treadmill for a walking task. We designed a safety belt and displayed parts of the treadmill in VR. The safety belt was much appreciated by the participants and did not affect the walking behavior. However, the participants requested more visual cues in the user's field of view.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="PO1117">Exploring How, for Whom and in Which Contexts Extended Reality Training 'Works' in Upskilling Healthcare Workers: A Realist Review</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D11 </strong></small> <br /> </p>

<!---->
    
    <p><i>Norina Gasteiger: University of Manchester&#59; Sabine N van der Veer: University of Manchester&#59; Paul Wilson: University of Manchester&#59; Dawn Dowding: University of Manchester</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/dgssW35R8LM" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1117" class="wrap-collabsible"> <input id="collapsibleabstractPO1117" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1117" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Extended reality (XR), including virtual reality (VR) and augmented reality (AR) may overcome barriers to training healthcare workers, such as resource constraints. However, the effectiveness of XR training is disputed and not well understood. Our realist review explores how, for whom and in what contexts AR and VR training 'works' in upskilling healthcare workers. Eighty papers informed our program theory, while 46 empirical studies tested/refined it. We conclude that XR triggers perceptions of realism and deep immersion, and enables visualization, interactive learning, enhancement of skills and repeated practice within a safe learning environment, consequently bettering skills, learning/knowledge and learner satisfaction.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="PO1118">ARTFM: Augmented Reality Visualization of Tool Functionality Manuals in Operating Rooms</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D12 </strong></small> <br /> </p>

<!---->
    
    <p><i>Constantin Kleinbeck: Friedrich-Alexander Universit&auml;t Erlangen-N&uuml;rnberg&#59; Hannah Schieber: Friedrich-Alexander University&#59; Sebastian Andress: Ludwig-Maximilians-Universit&auml;t M&uuml;nchen&#59; Christian Krautz: Universit&auml;tsklinikum Erlangen&#59; Daniel Roth: Friedrich-Alexander-Universit&auml;t Erlangen-N&uuml;rnberg</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/muAa74zvmh0" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1118" class="wrap-collabsible"> <input id="collapsibleabstractPO1118" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1118" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Error-free surgical procedures are crucial for a patient's health. However, with the increasing complexity and variety of surgical instruments, it is difficult for clinical staff to acquire detailed assembly and usage knowledge leading to errors in process and preparation steps. Yet, the gold standard in retrieving necessary information when problems occur is to get the paper-based manual. Reading through the necessary instructions is time-consuming and decreases care quality. We propose ARTFM, a process integrated manual, highlighting the correct parts needed, their location, and step-by-step instructions to combine the instrument using an augmented reality head-mounted display.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="PO1120">Comparing Controller with the Hand Gestures Pinch and Grab for Picking Up and Placing Virtual Objects</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D13 </strong></small> <br /> </p>

<!---->
    
    <p><i>Alexander Sch&auml;fer: TU Kaiserslautern&#59; Gerd Reis: German Research Center for Artificial Intelligence&#59; Didier Stricker: German Research Center for Artificial Intelligence</i></p>
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=czur92F6vMg" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1120" class="wrap-collabsible"> <input id="collapsibleabstractPO1120" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1120" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Grabbing virtual objects is one of the essential tasks for Augmented, Virtual, and Mixed Reality applications. Modern applications usually use a simple pinch gesture for grabbing and moving objects. However, picking up objects by pinching has disadvantages. It can be an unnatural gesture to pick up objects and prevents the implementation of other gestures which would be performed with thumb and index. Therefore it is not the optimal choice for many applications. In this work, different implementations for grabbing and placing virtual objects are proposed and compared. Performance and accuracy of the proposed techniques are measured and compared.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1122">Social Presence in VR Empathy Game for Children: Empathic Interaction with the Virtual Characters</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D14 </strong></small> <br /> </p>

<!---->
    
    <p><i>Ekaterina Muravevskaia: University of Florida&#59; Christina Gardner-McCune: University of Florida</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/hCM3QpLRxO0" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1122" class="wrap-collabsible"> <input id="collapsibleabstractPO1122" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1122" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This paper discusses children's empathic interactions with the VR characters. The study and findings in this paper were derived from a larger research study that explored the design and evaluation of empathic experiences for young children (6-9 years old) in VR environments. We found similarities between how children interacted with the VR characters and with real people (i.e., cognitive empathy and emotional contagion). This provides initial insight into children's experience of social presence with the VR characters. We suggest follow-up research on connection between empathy and social presence to explore the ways to create empathic VR experiences for children.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1137">Supervised Machine Learning Hand Gesture Classification in VR for Immersive Training</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D24 </strong></small> <br /> </p>

<!---->
    
    <p><i>Ozkan Cem Bahceci: BT&#59; Anasol Pena-Rios: BT&#59; Gavin Buckingham: University of Exeter&#59; Anthony Conway: BT</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/_vqOF-bg4No" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1137" class="wrap-collabsible"> <input id="collapsibleabstractPO1137" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1137" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>The fast adoption of immersive wearables evidences the need for more intuitive interaction methods between virtual environments and their users. Advances in wearables are making in-built real-time hand tracking mechanisms more common. However, wearable providers only include limited gestures available for developer use. This limits their use for VR-based training solutions, where users need to complete tasks that mimic real-world activities as much as possible to gain valuable insights on those tasks. We present a virtual reality application that collects data on hand characteristics and analyses the collected data to identify hand gestures towards achieving a more natural interaction.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1158">MeasVRe: Measurement Tools for Unity VR Applications</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D23 </strong></small> <br /> </p>

<!---->
    
    <p><i>Jolly Chen: University of Amsterdam&#59; Robert G. Belleman: University of Amsterdam</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/e2MZfG5DNPU" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1158" class="wrap-collabsible"> <input id="collapsibleabstractPO1158" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1158" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>An indispensable facility that is often missing in VR applications for studying complex 3D models, is the ability to take measurements. This paper describes a toolkit for Unity VR applications that allows for taking distance, angle, trace, surface area, and bounding box volume measurements by placing markers. We show that markers can be efficiently snapped to a model by ray casting. Moreover, we validated taking distance measurements with our toolkit, by performing an experiment where we measure inter-branch distances of corals. Lastly, the toolkit can save measurements locally or upload them to a logging server.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1167">Automatic 3D Avatar Generation from a Single RBG Frontal Image</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D22 </strong></small> <br /> </p>

<!---->
    
    <p><i>Alejandro Beacco: Universitat de Barcelona&#59; Jaime Gallego: University of Barcelona&#59; Mel Slater: University of Barcelona</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/JzsghODuZBg" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1167" class="wrap-collabsible"> <input id="collapsibleabstractPO1167" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1167" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We present a complete automatic system to obtain a realistic 3D avatar reconstruction of a person using only a frontal RGB image. Our proposed workflow first determines the pose, shape and semantic information from the input image. All this information is processed to create the skeleton and the 3D skinned textured mesh that conform the final avatar. We use a specific head reconstruction method to correctly match our final mesh to a realistic avatar. Our pipeline focuses on three main aspects: automation of the process, identification of the person, and usability of the avatar.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="PO1168">MR-RIEW: An MR Toolkit for Designing Remote Immersive Experiment Workflows</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D21 </strong></small> <br /> </p>

<!---->
    
    <p><i>Daniele Giunchi: University College London&#59; Riccardo Bovo: Imperial College London&#59; Anthony Steed: University College London&#59; Thomas Heinis: Imperial College</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/Ra9uwnenAxE" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1168" class="wrap-collabsible"> <input id="collapsibleabstractPO1168" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1168" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We present MR-RIEW, a toolkit for virtual and mixed reality that provides researchers with a dynamic way to design an immersive experiment workflow including instructions, environments, sessions, trials and questionnaires. It is implemented in Unity via scriptable objects, allowing simple customisation. The graphic elements, the scenes and the questionnaires can be selected and associated without code. MR-RIEW can save locally into the headset and remotely the questionnaire's answers. MR-RIEW is connected to Google Firebase service for the remote solution requiring a minimal configuration.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1191">Virtual Human Coherence and Plausibility - Towards a Validated Scale</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D35 </strong></small> <br /> </p>

<!---->
    
    <p><i>David Mal: University of W&uuml;rzburg, Department of Computer Science, HCI Group&#59; Erik Wolf: University of W&uuml;rzburg, Department of Computer Science, HCI Group&#59; Nina D&ouml;llinger: Human-Technology-Systems Group&#59; Mario Botsch: TU Dortmund University&#59; Carolin Wienrich: University W&uuml;rzburg&#59; Marc Erich Latoschik: Department of Computer Science, HCI Group</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/qG6L4mweT0w" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1191" class="wrap-collabsible"> <input id="collapsibleabstractPO1191" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1191" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Virtual humans contribute to users' state of plausibility in various XR applications. We present the development and preliminary evaluation of a self-assessment questionnaire to quantify virtual human's plausibility in virtual environments based on eleven concise items. A principal component analysis of 650 appraisals collected in an online survey revealed two highly reliable components within the items. We interpret the components as possible factors, i.e., appearance and behavior plausibility and match to the virtual environment, and propose future work aiming towards a standardized virtual human plausibility scale by validating the structure and sensitivity of both sub-components in XR environments.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="PO1194">Democratic Video Pass-Through for Commercial Virtual Reality Devices</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D36 </strong></small> <br /> </p>

<!---->
    
    <p><i>Diego Gonz&aacute;lez Mor&iacute;n: Nokia&#59; Francisco Pereira: Bell Labs&#59; Ester Gonzalez-Sosa: Bell Labs&#59; Pablo Perez: Bell Labs&#59; Alvaro Villegas: Bell Labs</i></p>
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=F_u-sJZ5ONc" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1194" class="wrap-collabsible"> <input id="collapsibleabstractPO1194" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1194" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Video pass-through Extended Reality (XR) is rapidly gaining interest from developers and researchers. However, video pass-through enabled XR devices' ecosystem is still bounded to expensive hardware. In this paper, we describe our custom hardware and software setup for providing effective video pass-through capabilities to inexpensive commercial Virtual Reality (VR) devices. The proposed hardware setup incorporates a low-cost HD stereo camera rigidly hooked to the VR device using a custom 3D printed attachment. Our software solution, implemented in Unity, overcomes hardware-specific limitations, such as cameras' delays, in a simple yet successful manner.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1199">Bringing Real Body as Self-Avatar into Mixed Reality: A Gamified Volcano Experience</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D37 </strong></small> <br /> </p>

<!---->
    
    <p><i>Diego Gonz&aacute;lez Mor&iacute;n: Nokia&#59; Ester Gonzalez-Sosa: Bell Labs&#59; Pablo Perez: Bell Labs&#59; Alvaro Villegas: Bell Labs</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/XiMmD1UzDiI" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1199" class="wrap-collabsible"> <input id="collapsibleabstractPO1199" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1199" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In this work we showcase a Mixed Reality experience that bring users' own body as a self-avatar. This is mainly based on an algorithm that segment in real time the egocentric full body placing a regular stereo camera in front of a commercial Virtual Reality device. To the best of our knowledge, this is the first work that integrates real body as self-avatar both meeting real-time and segmentation quality in real-life conditions. We conclude the paper with some preliminary subjective evaluation measuring Presence and Embodiment Factors, that suggests the potential of using your own body as self-avatar in Mixed Reality.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="PO1200">A Replication Study to Measure the Perceived Three-Dimensional Location of Virtual Objects in Optical See Through Augmented Reality</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: E26 </strong></small> <br /> </p>

<!---->
    
    <p><i>Farzana Alam Khan: Mississippi State University&#59; Mohammed Safayet Arefin: Mississippi State University&#59; Nate Phillips: Mississippi State University&#59; J. Edward Swan II: Mississippi State University</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/w94ak9ukRKA" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1200" class="wrap-collabsible"> <input id="collapsibleabstractPO1200" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1200" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>An important research question in optical see-through (OST) augmented reality (AR) is, how accurately and precisely can a virtual object's real world location be perceived? Previously, a method was developed to measure the perceived three-dimensional location of virtual objects in OST AR. In this research, a replication study is reported, which examined whether the perceived location of virtual objects are biased in the direction of the dominant eye. The successful replication analysis suggests that perceptual accuracy is not biased in the direction of the dominant eye. Compared to the previous study's findings, overall perceptual accuracy increased, and precision was similar.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1224">Who will Trust my Digital Twin? Maybe a Clerk in a Brick and Mortar Fashion Shop</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D38 </strong></small> <br /> </p>

<!---->
    
    <p><i>Lorenzo Stacchio: University of Bologna&#59; Michele Perlino: University of Bologna&#59; Ulderico Vagnoni: University of Bologna&#59; Federica Sasso: Independent Artist&#59; Claudia Scorolli: University of Bologna&#59; Gustavo Marfia: University of Bologna</i></p>
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=R2LmGI1hLuA&amp;list=PL9DFRaAYy0LCb964qGPmndEr3QYqvSofk" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1224" class="wrap-collabsible"> <input id="collapsibleabstractPO1224" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1224" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Digital Twin (DT) models mirror the life of physical entities and are adopted to optimize several industrial processes. Although well-established in the industrial fields, one of the most exciting examples of where DTs may be employed is in the MetaVerse, with Human Digital Twins (HDTs). We present a preliminary study that examines the efficacy of HDT-human interactions in the context of a fashion shop. Based on the results obtained involving thirty-two participants in our experiments, we begin a discussion related to the pros and cons of this approach on x-commerce.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1230">Measuring Virtual Object Location with X-Ray Vision at Action Space Distances</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D44 </strong></small> <br /> </p>

<!---->
    
    <p><i>Nate Phillips: Mississippi State University&#59; Farzana Alam Khan: Mississippi State University&#59; Mohammed Safayet Arefin: Mississippi State University&#59; Cindy Bethel: Mississsippi State University&#59; J. Edward Swan II: Mississippi State University</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/8MX2051CyV4" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1230" class="wrap-collabsible"> <input id="collapsibleabstractPO1230" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1230" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Accurate and usable x-ray vision is a significant goal in augmented reality (AR) development. X-ray vision, or the ability to comprehend location and object information when it is presented through an opaque barrier, needs to successfully convey scene information to be a viable use case for AR. Further, this investigation should be performed in an ecologically valid context in order to best test x-ray vision. This research seeks to experimentally evaluate the perceived object location of stimuli presented with x-ray vision, as compared to real-world perceived object location through a window, at action space distances of 1.5 to 15 meters.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="PO1231">Preliminary evaluation of an IVR user experience design model using eye-tracking attention measurements</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D43 </strong></small> <br /> </p>

<!---->
    
    <p><i>Elena Dzardanova: University of the Aegean&#59; Vlasios Kasapakis: University of the Aegean</i></p>
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=c1KOXyLNtJM" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1231" class="wrap-collabsible"> <input id="collapsibleabstractPO1231" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1231" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>The present study drafts a simplified IVR user experience design model to guideline a preliminary evaluation of attention variance for semantically distinct elements. 27 participants freely explored an interactive state of the art virtual setting, whilst equipped with eye-tracking technology which procured attention duration measurements. Initial results confirm significant element attention discrepancy and provide the first indication toward a more detailed categorical organization of experience components for follow-up experimentation.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1254">Proposing the RecursiVerse Overlay Application for the MetaVerse</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D42 </strong></small> <br /> </p>

<!---->
    
    <p><i>Lorenzo Donatiello: University of Bologna&#59; Gustavo Marfia: University of Bologna</i></p>
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=W96qFXljEBg&amp;list=PL9DFRaAYy0LCb964qGPmndEr3QYqvSofk&amp;index=4" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1254" class="wrap-collabsible"> <input id="collapsibleabstractPO1254" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1254" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In a still uncertain future for the MetaVerse, we dare to envision one of its possible future developments, the RecursiVerse. The RecursiVerse is an overlay application that may be built upon the MetaVerse, amounting to symmetrical virtual-real space where a human being may rely on human digital twins which may move, operate and recursively replicate to collaboratively perform multiple tasks. The RecursiVerse may hence extend what will eventually be possible thanks to the MetaVerse, providing a service to a society that poses increasing cognitive and perceptual challenges due to growing work-life imbalances and increasing cognitive loads.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1263">Augmented Reality In-Field Observation Creation and Visualization in Underperforming Areas</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D41 </strong></small> <br /> </p>

<!---->
    
    <p><i>Mengya Zheng: University College Dublin&#59; Nestor Velasco Bermeo: University College Dublin&#59; Abraham G. Campbell: University College Dublin</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/9x0qwL9lPvs" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1263" class="wrap-collabsible"> <input id="collapsibleabstractPO1263" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1263" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>To precisely diagnose and address farming issues that led to crop yield underperformance, agronomists need to conduct field trips at the targeted underperforming areas and record the observed issues to support crop treatment decisions. Traditional field data visualization tools may not provide intuitive visualizations to support field trip investigations at targeted underperforming areas. This paper presents an Augmented Reality (AR) in-field observation tool to navigate agronomists to annotate the observed issues at precisely targeted underperforming areas. These recorded observation AR annotations are then synchronized in a Web-based Interactive Multi-Layer Mapping Tool for a complete precision farming decision-making process.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="PO1264">Jamming in MR: Towards Real-Time Music Collaboration in Mixed Reality</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: E23 </strong></small> <br /> </p>

<!---->
    
    <p><i>Ruben Schlagowski: University of Augsburg&#59; Kunal Gupta: The University of Auckland&#59; Silvan Mertes: University of Augsburg&#59; Mark Billinghurst: University of Auckland&#59; Susanne Metzner: University of Augsburg&#59; Elisabeth Andr&eacute;: University of Augsburg</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/4yoZ0RHB1p8" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1264" class="wrap-collabsible"> <input id="collapsibleabstractPO1264" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1264" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Recent pandemic-related contact restrictions have made it difficult for musicians to meet in person to make music. As a result, there has been an increased demand for applications that enable remote and real-time music collaboration. One desirable goal here is to give musicians a sense of social presence, to make them feel that they are &quot;on site&quot; with their musical partners. We conducted a focus group study to investigate the impact of remote jamming on users' affect. Further, we gathered user requirements for a Mixed Reality system that enables real-time jamming and developed a prototype based on these findings.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1267">Effects of the Level of Detail on the Recognition of City Landmarks in Virtual Environments</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: E22 </strong></small> <br /> </p>

<!---->
    
    <p><i>Achref Doula: TU Darmstadt&#59; Philipp Kaufmann: TU Darmstadt&#59; Alejandro Sanchez Guinea: TU Darmstadt&#59; Max M&uuml;hlh&auml;user: TU Darmstadt</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/OZIaR3Ha4fM" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1267" class="wrap-collabsible"> <input id="collapsibleabstractPO1267" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1267" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>The reconstruction of city landmarks is central to creating recognizable virtual environments representing real cities. Despite the recent advances, it is still not clear what level of detail (LOD) to adopt when reconstructing those landmarks for their correct recognition, and if particular architectural styles represent specific challenges in this respect. In this paper, we investigate the effect of LOD on landmark recognition, generally, and on some architectural styles, specifically. The results of our user study show that higher LOD lead to a better landmark identification. Particularly, Neoclassical-style buildings need more details to be individually distinguished from similar ones.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="PO1268">Facial emotion recognition analysis using deep learning through RGB-D imagery of VR participants through partially occluded facial types</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: E21 </strong></small> <br /> </p>

<!---->
    
    <p><i>Ian Mills: Walton Institute for Information and Communication Systems Science&#59; Frances cleary: Walton Institute for Information and Communication Systems Science</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/8aUmRQV0QH8" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1268" class="wrap-collabsible"> <input id="collapsibleabstractPO1268" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1268" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This research poster outlines the initial development of a facial emotion recognition (FER) evaluation system based on RGB-D imagery captured via a mobile based device. The study outlined features control group of non-occluded facial types and a set of participants wearing a head mounted display (HMD) in order to demonstrate an occluded facial type. We explore an architecture to develop a FER system that is suitable for occluded facial analysis. The paper details the methodology, experimental design and future work to be carried out to deliver such a system.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1273">Irish Sign Language in a Virtual Reality Environment</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: E27 </strong></small> <br /> </p>

<!---->
    
    <p><i>Ryan McCloskey: Walton Institute for Information and Communication Systems Science</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/zTb1iuJt-5w" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1273" class="wrap-collabsible"> <input id="collapsibleabstractPO1273" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1273" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This paper describes a study whereby groups of users are tasked with learning Irish Sign Language (ISL) gestures in distinct environments and performing a comparison of outcomes to determine the general effectiveness of each method. One groups of users are tasked with learning via traditional tuition methods, and the second groups is placed in a virtual reality environment with a custom training module for teaching ISL gestures. This paper discusses such a study and details the processes, motivations and potential future work.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="PO1274">A validation study to trigger nicotine craving in virtual reality</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: E28 </strong></small> <br /> </p>

<!---->
    
    <p><i>Chun-Jou Yu: Goldsmiths&#59; Aitor Rovira: Oxford Health NHS Foundation Trust&#59; Xueni Pan: Goldsmiths&#59; Daniel Freeman: University of Oxford</i></p>
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=7BhQdzFND8s&amp;ab_channel=celinethesavage" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1274" class="wrap-collabsible"> <input id="collapsibleabstractPO1274" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1274" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We built a virtual beer garden that contained various smoking cues (both verbal and non-verbal) using a motion capture system to record the realistic smoking behaviour related animations. Our 3-min long VR experience was optimized for Oculus Quest 2 with the hand tracking function enabled. We conducted a pilot study with 13 non-treatment-seeking nicotine-dependent cigarette smokers. The preliminary results indicate that this VR experience led to high levels of presence, and that there is a significant increase of nicotine craving - but only for those who reported a high level of immersion.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="PO1280">X-Ray Device Positioning with Augmented Reality Visual Feedback</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: E32 </strong></small> <br /> </p>

<!---->
    
    <p><i>Kartikay Tehlan: Technical University of Munich&#59; Alexander Winkler: Technical University of Munich&#59; Daniel Roth: Human-Centered Computing and Extended Reality&#59; Nassir Navab: Technische Universit&auml;t M&uuml;nchen</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/jJ7q7rMvJFg" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1280" class="wrap-collabsible"> <input id="collapsibleabstractPO1280" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1280" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In minimally invasive surgeries one common way to verify progress is the use of an intraoperative X-ray device (due to its characteristic shape called a C-arm). Its control, however, remains challenging owing to its complex movements. We propose the use of an Augmented Reality Head-Mounted Display (AR-HMD) to let the surgeon choose a desired X-ray view interventionally providing the corresponding C-arm configuration as visual feedback. The study participants' feedback, despite being critical of the HMD hardware limitations, suggests an inclination towards using AR for orthopaedic surgeries on especially complex or unusual anatomies.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1286">HoloCMDS: Investigating Around Field of View Glanceable Commands Selection in AR-HMDs</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: E33 </strong></small> <br /> </p>

<!---->
    
    <p><i>Rajkumar Darbar: INRIA Bordeaux&#59; Arnaud Prouzeau: Inria&#59; Martin HACHET: Inria</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/Ye-UDkTqNpo" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1286" class="wrap-collabsible"> <input id="collapsibleabstractPO1286" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1286" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Augmented reality merges the real and virtual worlds seamlessly in real-time. However, we need contextual menus to manipulate virtual objects rendered in our physical space. Unfortunately, designing a menu for augmented reality head-mounted displays (AR-HMDs) is challenging because of their limited display field of view (FOV). In this paper, we propose HoloCMDS to support quick access of contextual commands in AR-HMDs and conduct an initial experiment to get users' feedback about this technique.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1312">Rereading the Narrative Paradox for Virtual Reality Theatre</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: E34 </strong></small> <br /> </p>

<!---->
    
    <p><i>Xiaotian Jiang: Goldsmiths, University of London&#59; Xueni Pan: Goldsmiths&#59; Jonathan Freeman: Goldsmiths University</i></p>
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=EjciBNYMV5M" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1312" class="wrap-collabsible"> <input id="collapsibleabstractPO1312" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1312" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We examined several key issues around audience autonomy in VR theatre. Informed by a literature review and a qualitative user study (grounded theory), we developed a conceptual model that enables a quantifiable evaluation of audience experience in VR theatre. A second user study inspired by the 'narrative paradox', investigates the relationship between spatial exploration and narrative comprehension in two VR performances. Our results show that although navigation distracted the participants from following the full story, they were more engaged, attached and had a better overall experience as a result of their freedom to move and interact.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="PO1318">Investigation of the potential use of Virtual Reality for Agoraphobia Exposure therapy</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: E33 </strong></small> <br /> </p>

<!---->
    
    <p><i>Sinead Barnett: Walton Institute&#59; Ian Mills: Walton Institute&#59; Frances cleary: Walton Institute</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/D91eAlPDeB4" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1318" class="wrap-collabsible"> <input id="collapsibleabstractPO1318" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1318" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Preliminary research study to evaluate the potential and need for virtual reality in the mental health sector specifically focusing on the treatment of agoraphobia. A survey was sent to numerous participants that have been diagnosed and currently receiving treatment for agoraphobia. Results have concluded there is a demand for virtual reality treatment for agoraphobia and this in turn can lead to future studies into the VR therapy</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
</div>

<div>
    
    <h2 id="PC"> Posters - Expo Hall C</h2>
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1012">Effects of Clutching Mechanism on Remote Object Manipulation Tasks</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: B11 </strong></small> <br /> </p>

<!---->
    
    <p><i>Zihan Gao</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/coWStqGOQG0" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1012" class="wrap-collabsible"> <input id="collapsibleabstractC1012" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1012" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Remote object manipulation in practice is often an iterative process that needs clutching. However, while many interaction techniques have been designed for manipulating remote objects, clutching mechanism is often an important but overlooked aspect in manipulation tasks. In this paper, we evaluate the effects of clutching mechanism on remote object manipulation tasks, which compares two clutching mechanisms under various tasks settings. The results suggested that an efficient clutching mechanism can effectively improve the usability in remote object manipulation tasks.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="C1015">A Testbed for Exploring Multi-Level Precueing in Augmented Reality</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: B12 </strong></small> <br /> </p>

<!---->
    
    <p><i>Jen-Shuo Liu</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/iC-sHV8GEJY" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1015" class="wrap-collabsible"> <input id="collapsibleabstractC1015" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1015" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Precueing information about upcoming subtasks prior to performing them has the potential to make an entire task faster and easier to accomplish than cueing only the current subtask. Most AR and VR research on precueing has addressed path-following tasks requiring simple actions at a series of locations, such as pushing a button or just visiting that location. We present a testbed for exploring multi-level precueing in a richer task that requires the user to move their hand between specified locations, transporting an object between some of them, and rotating it to a designated orientation.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="C1021">Resolution Tradeoff in Gameplay Experience, Performance, and Simulator Sickness in Virtual Reality Games</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: B13 </strong></small> <br /> </p>

<!---->
    
    <p><i>Jialin Wang</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/lLPIOWe4mDw" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1021" class="wrap-collabsible"> <input id="collapsibleabstractC1021" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1021" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Higher resolution is one of the main directions and drivers in the development of Virtual Reality (VR) Head-Mounted Displays (HMDs). However, given their associated higher cost, it is unclear the benefits of having higher resolution on user experience, especially in VR games. This research aims to investigate the effects of resolution in gameplay experience and simulator sickness for VR games. To this end, we designed an experiment to collect gameplay experience, simulator sickness (SS), and player performance data with a VR First-Person Shooter game. Our results indicate that 2K resolution is an important threshold for an enhanced gameplay experience without affecting performance and increasing SS levels.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1028">VCoach: Enabling Personalized Boxing Training in Virtual Reality</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: B14 </strong></small> <br /> </p>

<!---->
    
    <p><i>Hao Chen</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/bVH0yf5FKUw" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1028" class="wrap-collabsible"> <input id="collapsibleabstractC1028" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1028" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We propose a training system in virtual reality, VCoach, automatically generating interactive and personalized boxing training drills for individual trainees. The training drill is generated in real-time based on the trainee's updated performance, including the evaluation of punch speed, reaction time, and punch pose through wearable VR devices. The training drill is visualized as a sequence of target points on a virtual heavy bag and the corresponding punch motion, as well as the performance feedback. Our experiments show that VCoach can generate personalized training drills to help trainees improve skills efficiently.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="C1031">Control with Vergence Eye Movement in Augmented Reality See-Through Vision</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: B21 </strong></small> <br /> </p>

<!---->
    
    <p><i>Zhimin Wang</i></p>
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=Eycv9iuvfwg" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1031" class="wrap-collabsible"> <input id="collapsibleabstractC1031" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1031" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Augmented Reality (AR) see-through vision enables the user to see through a wall and view the occluded objects. Most existing works only used common modalities to control the display for see-through vision, e.g., button clicking and speech. However, we use visual system to watch see-through vision. Using an addition interaction channel will distract the user and degrade the user experience. In this paper, we propose a novel interaction method using vergence eye movement for controlling see-through vision in AR. Specifically, we first customize eye cameras and design gaze depth estimation method for Microsoft HoloLens 2. With our algorithm, fixation depth can be computed from the vergence, and used to manage the see-through vision.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="C1033">Semi-Analytical Surface Tension Model for Free Surface Flows</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: B22 </strong></small> <br /> </p>

<!---->
    
    <p><i>Nurshat Menglik</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/AeY0b5mlLtk" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1033" class="wrap-collabsible"> <input id="collapsibleabstractC1033" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1033" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In this paper, a semi-analytical surface tension model is proposed for smoothed particle hydrodynamics (SPH). Different from previous approaches, cohesive and adhesive forces in our model are unified within a surface energy framework for nonuniform systems. To calculate the adhesive force, we use a semi-analytical solution to convert the volume integral into a surface integral, triangular meshes which represent solid boundaries can be directly introduced into liquid-solid interactions. A gradient descent algorithm is employed to optimize the objective function, which represents the total energy of the fluid. Experiments show that our model can efficiently handle complex solid boundaries with surface-tension-driven phenomena.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="C1034">Deformable torso anatomy education with three-dimensional autostereoscopic visualization and free-hand interaction</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: B28 </strong></small> <br /> </p>

<!---->
    
    <p><i>Nan Zhang</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/zgVVK7eBjh4" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1034" class="wrap-collabsible"> <input id="collapsibleabstractC1034" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1034" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Virtual reality/augmented reality has advantages in immersive learning and conveying meta-information in anatomy education. In this study, we present an interactive naked-eye 3D torso anatomy education environment based on population anatomical information. In particular, we utilize the deformable anatomy models, constructed from big data of healthy adults, to convey the anatomy knowledge of truthfulness organ shapes and population anatomical variances. In addition, the proposed anatomy education system is conveyed by free-hand 3D autostereoscopic visualization, which supports multi-user and glass-free. A user study with fourteen students was implemented and demonstrates that the system is appropriate and useful for torso anatomy education.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1106">FusedAR: Adaptive Environment Lighting Reconstruction for Visually Coherent Mobile AR Rendering</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: B27 </strong></small> <br /> </p>

<!---->
    
    <p><i>Yiqin Zhao</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/WJNtbfDRaZE" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1106" class="wrap-collabsible"> <input id="collapsibleabstractC1106" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1106" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Obtaining accurate omnidirectional environment lighting for high quality rendering in mobile augmented reality is challenging due to the practical limitation of mobile devices and the inherent spatial variance of lighting. In this paper, we present a novel adaptive environment lighting reconstruction method called FusedAR, which is designed from the outset to consider mobile characteristics, e.g., by exploiting mobile user natural behaviors of pointing the camera sensors perpendicular to the observation-rendering direction. Our initial evaluation shows that FusedAR achieves better rendering effects compared to using a recent deep learning-based AR lighting estimation system and environment lighting captured by 360&deg; cameras.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1127">Designing a Physiological Loop for the Adaptation of Virtual Human Characters in a Social VR Scenario</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: B26 </strong></small> <br /> </p>

<!---->
    
    <p><i>Francesco Chiossi</i></p>
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=OdQeaU5NvTM" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1127" class="wrap-collabsible"> <input id="collapsibleabstractC1127" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1127" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Social virtual reality is getting mainstream not only for entertainment purposes but also for productivity and education. This makes the design of social VR scenarios functional to support the operator's performance. We present a physiologically-adaptive system that optimizes for visual complexity in a dual-task scenario based on electrodermal activity. Specifically, we propose a system that adapts the amount of non-player characters while jointly performing an N-Back task (primary) and visual detection task (secondary). Our preliminary results show that when optimizing the complexity of the secondary task, users report an improved user experience.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="C1144">GestureExplorer: Immersive Visualisation and Exploration of Gesture Data</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: B25 </strong></small> <br /> </p>

<!---->
    
    <p><i>Ang Li</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/_MynTaap_g8" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1144" class="wrap-collabsible"> <input id="collapsibleabstractC1144" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1144" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This paper presents GestureExplorer, which features versatile immersive visualisations to grant the user free control over their perspective, allowing them to gain a better understanding of gestures. It provides multiple data visualisation views, and interactive features to support analysis and exploration of gesture datasets. A pair of iterative user studies provides initial feedback from several participants, including experts on immersive visualisation, and demonstrates the potential of GestureExplorer for providing a useful and engaging experience for exploring gesture data.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="C1158">Multi Touch Smartphone Based Progressive Refinement VR Selection</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: B34 </strong></small> <br /> </p>

<!---->
    
    <p><i>Elaheh Samimi</i></p>
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=-NW0NGOMF28" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1158" class="wrap-collabsible"> <input id="collapsibleabstractC1158" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1158" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We developed a progressive refinement technique for VR object selection using a smartphone as a controller. Our technique combines progressive refinement with the marking menu-based CountMarks, which uses multi-finger touch gestures to &quot;short-circuit&quot; multi-item marking menus. Users can indicate a specific item in a sub-menu by pressing a specific number of fingers on the screen while swiping in the desired menu's direction. This reduces the number of steps in progressive refinement selection.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1182">Predictive Power of Pupil Dynamics in a Team Based Virtual Reality Task</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: B32 </strong></small> <br /> </p>

<!---->
    
    <p><i>Yinuo Qin</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/56Jhy_oH-_o" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1182" class="wrap-collabsible"> <input id="collapsibleabstractC1182" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1182" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In this work, we describe a team-based VR task termed the Apollo Distributed Control Task (ADCT), where individuals, via the single independent degree-of-freedom control and limited environmental views, must work together to guide a virtual spacecraft back to Earth. Focusing on the analysis of pupil dynamics, which have been linked to a number of cognitive and physiological processes such as arousal, cognitive control, and working memory, we find that pupil diameter changes are predictive of multiple task-related dimensions, including the difficulty of the task, the role of the team member, and the type of communication.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1231">Ebublio: Edge Assisted Multi-user 360-Degree Video Streaming</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: B33 </strong></small> <br /> </p>

<!---->
    
    <p><i>Yili Jin</i></p>
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=sctsTxVfSCM" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1231" class="wrap-collabsible"> <input id="collapsibleabstractC1231" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1231" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Compared to traditional videos, streaming 360&deg; videos is more difficult. We propose Ebublio, a novel intelligent edge caching framework consisting of a collaborative FoV prediction (CFP) module and a long-term tile caching optimization (LTO) module. The former integrates the features of video content, user trajectory, and other users' records for combined prediction. The latter employs the Lyapunov framework and a subgradient optimization toward the optimal caching replacement policy. Our trace-driven evaluation further demonstrates the superiority of our framework, with about 42&#37; improvement in FoV prediction, and 36&#37; improvement in QoE under similar traffic consumption.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1270">3Dify: Extruding Common 2D Charts with Timeseries Data</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: B34 </strong></small> <br /> </p>

<!---->
    
    <p><i>Richard Brath</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/JKz_XgJCs2E" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1270" class="wrap-collabsible"> <input id="collapsibleabstractC1270" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1270" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>3D charts are not common in financial services. We review chart use in practice. We create 3D financial visualizations starting with 2D charts used extensively in financial services, then extend into the third dimension with timeseries data. We embed the 2D view into the the 3D scene&#59; constrain interaction and add depth cues to facilitate comprehension. Usage and extensions indicate success.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="C1275">Virtual Reality Point Cloud Annotation</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: B23 </strong></small> <br /> </p>

<!---->
    
    <p><i>Anton Franzluebbers</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/3blPgeAk4p0" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1275" class="wrap-collabsible"> <input id="collapsibleabstractC1275" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1275" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This work presents an immersive headset-based virtual reality visualization and annotation system for point clouds, oriented towards application on laser scans of plants. The system can be used to paint regions or individual points with fine detail, even with large, dense point clouds. A non-immersive desktop interface was designed for comparison within the same application. A within-subjects user study (N=16) was conducted to compare these interfaces for annotation and counting tasks. Results showed a strong preference for the immersive virtual reality interface, likely as a result of perceived and actual significant differences in task performance.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="C1283">Design and Evaluation of an Augmented Reality Application for Learning Spatial Transformations and Their Mathematical Representations</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: B24 </strong></small> <br /> </p>

<!---->
    
    <p><i>Zohreh Shaghaghian</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/2_QNfjQKW7k" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1283" class="wrap-collabsible"> <input id="collapsibleabstractC1283" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1283" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>There is a close relation between spatial thinking and mathematical problem-solving. This paper presents a newly developed educational Augmented Reality (AR) mobile application, BRICKxAR/T, to help students intuitively learn spatial transformations and the related mathematics through play. A pilot study with 7 undergraduate students evaluates students learning gain through a mental rotation and a math test on transformation matrices. The results show most students performed better with a higher score after learning with the app. Students found the app interesting to play and useful for learning geometric transformations and matrices.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1288">VR Edufication on Historic Lunar Roving Missions</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D32 </strong></small> <br /> </p>

<!---->
    
    <p><i>Huadong Zhang</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/nWQYRksCMUw" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1288" class="wrap-collabsible"> <input id="collapsibleabstractC1288" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1288" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This work presents the design and evaluation of an educational VR game to teach historic Apollo lunar roving missions. The game consists of three gameplay scenes, and each scene aims to convey one type of knowledge. The game adheres with the learning objectives, presents the contents, and conveys knowledge in both active and passive interaction modes. We conducted a user study focusing on the understanding of the influence of different interaction modes on learning outcomes and learning engagement.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="C1299">The Immediate and Retained Effectiveness of One-time Virtual Reality Exposure in Enhancing Intercultural Sensitivity</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D31 </strong></small> <br /> </p>

<!---->
    
    <p><i>Dr Richard Chen Li</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/iGc2KpLlVqg" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1299" class="wrap-collabsible"> <input id="collapsibleabstractC1299" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1299" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This study aims to investigate the immediate and retained effects of one-time virtual reality (VR) exposure on intercultural sensitivity (IS) and identify the contributing factors. Three virtual scenarios about the ethnic minorities in Hong Kong were created for the empirical study. The longitudinal results involving 30 participants (15M 15F) showed that both the immediate and retained effects of the one-time VR exposure on IS were significant. Moreover, linear growth curve models suggested that among the female participants, presence and emotional empathy were closely associated with the change of IS over time, but this relation was not significant among the males.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1312">Redirected Placement: Retargeting Destinations of Passive Props for Enhancing Bimanual Haptic Feedback in Virtual Reality</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: C11 </strong></small> <br /> </p>

<!---->
    
    <p><i>Xuanhui Yang</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/Hn7_ubi6B38" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1312" class="wrap-collabsible"> <input id="collapsibleabstractC1312" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1312" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Haptic retargeting is a commonly used technique to match passive props to virtual objects and add tactile feedback in Virtual Reality (VR). However, researchers have mainly focused on single-hand retargeting and applied these techniques primarily for tasks of touching objects. In this work, we propose a novel retargeting solution for tasks of grabbing and placing objects in VR, called redirected placement (RP), which is applied when placing virtual objects. The key idea of this method is that when the user places the virtual object, it can enable the user to place the physical prop in the user's hand in a position that is easier to match with multiple other virtual objects, without being detected by the user.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1340">Moving Visual-Inertial Ordometry into Cross-platform Web for Markerless Augmented Reality</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: C12 </strong></small> <br /> </p>

<!---->
    
    <p><i>Yakun Huang</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/cCrxg97h3ZQ" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1340" class="wrap-collabsible"> <input id="collapsibleabstractC1340" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1340" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Widespread mobile AR implementation still suffers from the unique adaption for different mobile platforms. Enabling AR experience on the cross-platform web is a potential alternative and provides unified implementation. We reveal a lightweight VIO implementation on the web without any visual marker or cumbersome frameworks for AR services. The contributions include 1) designing a novel VIO architecture to balance the user experience and the efficiency&#59; 2) optimizing a single-thread VIO algorithm to avoid the sophisticated multi-threading management and enhancing compatibility&#59; 3) improving the dirty noise of IMU and developing efficient numerical libraries like SuiteSparse for the massive computation afford.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1357">Automatic Virtual Portals Placement for Efficient VR Navigation</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: C13 </strong></small> <br /> </p>

<!---->
    
    <p><i>Yi Liu</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/muuu9BtwzEA" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1357" class="wrap-collabsible"> <input id="collapsibleabstractC1357" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1357" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Portals placement in a large virtual scene can help users improve navigation efficiency, but determining the number and the positions of the portals has some challenges. In this paper, we proposed two automatic virtual portals placement methods for efficient VR navigation. To reduce the number of reverse redirections, we also proposed a real-time portal orientation determination algorithm. For any given single-floor outdoor virtual scene, our methods can automatically place the portals.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1372">Material Reflectance Property Estimation of Complex Objects Using an Attention Network</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: C14 </strong></small> <br /> </p>

<!---->
    
    <p><i>Bin Cheng</i></p>
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=NEonIkcMYkI" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1372" class="wrap-collabsible"> <input id="collapsibleabstractC1372" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1372" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Material reflectance property modeling can be used in realistic rendering to generate realistic appearances for virtual objects. However, current works mainly focus on near plane objects. In this paper, we propose an end-to-end network framework with attention mechanism to estimate the reflectance properties of any 3D object surface from a single image, where a single attention module is used for each reflectance property respectively to learn the property specific features. We also generate a material dataset by rendering a set of 3D complex shape models. The dataset is suitable for reflectance property estimation of arbitrary complex shape objects. Experiments validate the proposed method.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1398">3D Scene Reconstruction from RGB Images Using Dynamic Graph Convolution for Augmented Reality</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: C21 </strong></small> <br /> </p>

<!---->
    
    <p><i>Robin Fischer</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/ZNLnmNjCmVg" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1398" class="wrap-collabsible"> <input id="collapsibleabstractC1398" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1398" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>The 3D scene reconstruction task aims to reconstruct the object
shape, object pose, and the 3D layout of the scene. In the field of
augmented reality, this information is required for interactions with
the surroundings. In this paper, we develop a holistic end-to-end
scene reconstruction system using only RGB images. We further
designed an architecture that can adapt to different types of objects
through our graph convolution network during object surface generation. Moreover, a scene-merging strategy is proposed to alleviate the
occlusion problem by merging different views continuously. This
also allows our system to reconstruct the complete surroundings in a
room.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1411">Designing a Mixed Reality System for Exploring Genetic Mutation Data of Cancer Patients</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: C22 </strong></small> <br /> </p>

<!---->
    
    <p><i>Aniqa Imtiaz</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/5OQhYcxcuE4" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1411" class="wrap-collabsible"> <input id="collapsibleabstractC1411" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1411" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Increased availability of cancerous genomics mutation data provides researchers the opportunity to discover associations among genetic mutation patterns within the same organ as well as similar mutation patterns among different organs. However, the complexity, variety, and scale of multi-dimensional data involved in analyzing mutations across organs poses challenges for clinicians and researchers to draw such relationships. We present a prototype application that leverages multiple-coordinated views in mixed reality (MR) to enable investigations of genetic mutation patterns and the organs affected by cancer. We believe our prototype has the potential to enhance data and association discovery within and across different organs.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="C1421">A Pinch-based Text Entry Method for Head-mounted Displays</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: C23 </strong></small> <br /> </p>

<!---->
    
    <p><i>Haiyan Jiang</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/RO1-IWaqLrw" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1421" class="wrap-collabsible"> <input id="collapsibleabstractC1421" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1421" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Pinch gestures have been used for text entry in Head-mounted displays (HMDs), enabling a comfortable and eyes-free text entry. However, the number of pinch gestures is limited, making it difficult to input all characters. In addition, the common pinch-based methods with a QWERTY keyboard require accurate control of the hand position and angle, which could be affected by natural tremors and the Heisenberg effect. So, we propose a new text entry method for HMDs, which combines hand positions and pinch gestures with a condensed key-based keyboard, enabling one-handed text entry for HMDs. The results of a primary study show that the mean input speed of the proposed method is 7.60 words-per-minute (WPM).</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="C1431">Analysis of Emotional Tendency and Syntactic Properties for VR Game Reviews</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: C24 </strong></small> <br /> </p>

<!---->
    
    <p><i>Anqi CHEN</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/ga4S24gIu7Y" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1431" class="wrap-collabsible"> <input id="collapsibleabstractC1431" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1431" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>The studies of player reviews can help game developers design and optimize VR games. To this end, we investigated 288,685 reviews from 506 VR games on the Steam platform to analyze their sentiment tendencies using the machine learning-based model SKEP, which finds that although some of the reviews are &quot;recommend&quot;, they actually have opposite emotional tendencies. We also study the syntactic properties based on the natural language processing (NLP) kits Stanza and NLTK library, and we find that cybersickness is a significant concern for players.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1440">Role of Dynamic Affordance and Cognitive Load in the Design of Extended Reality based Simulation Environments for Surgical Contexts</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: C28 </strong></small> <br /> </p>

<!---->
    
    <p><i>Avinash Gupta</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/cyf-Gb-D2Sk" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1440" class="wrap-collabsible"> <input id="collapsibleabstractC1440" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1440" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In this paper, HCI-based design criteria for the Extended Reality (XR) based training environments. The design criteria explored in the paper help lay a foundation for the creation of Human Centric XR environments to train users in an orthopedic surgical procedure. The HCI-based perspective presented in the paper investigates the criteria such as affordance and cognitive load during the design. The paper focuses on the design of XR based environments based on the participatory design approach and information-centric modeling. Testing and assessment strategy presented provide insights into the impact of such HCI-based criteria on participants' acquisition of skills and knowledge during interactions with the XR environments.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="C1445">MienCap: Performance-Based Facial Animation with Live Mood Dynamics</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: C31 </strong></small> <br /> </p>

<!---->
    
    <p><i>Ye Pan</i></p>
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=cUkbgL-4mwo" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1445" class="wrap-collabsible"> <input id="collapsibleabstractC1445" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1445" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Our purpose is to improve performance-based animation which can drive believable 3D stylized characters that are truly perceptual. By combining traditional blendshape animation techniques with machine learning models, we present a real time motion capture system, called MienCap, which drive character expressions in a geometrically consistent and perceptually valid way. We demonstrate the effectiveness of our system by comparing to a commercial product Faceware. Results reveal that ratings of the recognition of expressions depicted for animated characters via our systems are statistically higher than Faceware. Our results provide animators with a system for creating the expressions they wish to use more quickly and accurately.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="C1448">Preliminary analysis of effective assistance timing for iterative visual search tasks using gaze-based visual cognition estimation</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: C38 </strong></small> <br /> </p>

<!---->
    
    <p><i>Hirotake Yamazoe</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/dDR4rG3Ns8c" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1448" class="wrap-collabsible"> <input id="collapsibleabstractC1448" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1448" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In this paper, focusing on whether a person has visually recognized a target (visual cognition, VC) in iterative visual-search tasks, we propose an efficient assistance method based on the VC. In the proposed method, we first estimate the participant's VC of the target in the previous task. We then determine the target for the next task based on the VC and start to guide the participant's attention to the target for the next task at the VC timing. By initiating the guidance from the timing of the previous target's VC, we can guide attention at an earlier time and achieve efficient attention guidance. The preliminary experimental results showed that VC-based assistance improves task performance.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1459">Towards Conducting Effective Locomotion Through Hardware Transformation in Head-Mounted-Device - A Review Study</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: C37 </strong></small> <br /> </p>

<!---->
    
    <p><i>Pawankumar Gururaj Yendigeri</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/qFwJ9k9E5OA" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1459" class="wrap-collabsible"> <input id="collapsibleabstractC1459" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1459" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Locomotion in Virtual Reality (VR) acts as a motion tracking unit for simulating user movements based on the Degree-of-Freedom (DOF) of the application. For effective locomotion, VR practitioners may have to transform their hardware from 3-DOF to 6-DOF. In this context, we conducted a literature review on different motion tracking methods employed in the Head-Mounted-Devices (HMD) to understand such hardware transformation to conduct locomotion in VR. Our observations led us to formulate a taxonomy of the tracking methods for locomotion in VR based on system design. Our study also captures different metrics that VR practitioners use to evaluate the hardware based on the context, performance, and significance for conducting locomotion.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1509">Head in the Clouds - Floating Locomotion in Virtual Reality</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: C36 </strong></small> <br /> </p>

<!---->
    
    <p><i>Priya Ganapathi</i></p>
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=aquU2pb_F1w" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1509" class="wrap-collabsible"> <input id="collapsibleabstractC1509" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1509" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Navigating large virtual spaces within the confines of a small tracked volume in a seated position becomes a serious accessibility issue when users' lower seating position reduces their visibility and makes it uncomfortable to reach for items with ease. Hence, we propose a &quot;floating&quot; accessibility technique, in which a seated VR user experiences the virtual environment from the perspective of a standing eye height. We conducted a user study comparing sitting, standing and floating conditions and observed that the floating technique had no detrimental effect in comparison to the standing technique and had a slight benefit over the sitting technique.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="C1516">OmniSyn: Synthesizing 360 Videos with Wide-baseline Panoramas</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: C35 </strong></small> <br /> </p>

<!---->
    
    <p><i>David Li</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/n4dhMVKwnkE" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1516" class="wrap-collabsible"> <input id="collapsibleabstractC1516" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1516" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Immersive maps such as Google Street View and Bing Streetside provide true-to-life views with a massive collection of panoramas. However, these panoramas are only available at sparse intervals along the path they are taken, resulting in visual discontinuities during navigation. In this paper, we leverage the unique characteristics of wide-baseline panoramas and present OmniSyn, a novel pipeline for 360 view synthesis between wide-baseline panoramas. OmniSyn predicts omnidirectional depth maps, renders meshes, and synthesizes intermediate views. We envision our work may inspire future research for this real-world task and lead to smoother experiences navigating immersive maps.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1542">AiRType: An Air-Tapping Keyboard for Augmented Reality Environments</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: C41 </strong></small> <br /> </p>

<!---->
    
    <p><i>&Uuml;lk&uuml; Meteriz-Y?ld?ran</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/iUII8ckpudU" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1542" class="wrap-collabsible"> <input id="collapsibleabstractC1542" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1542" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We present AiRType for AR/VR HMDs that enables text entry through bare hands for more natural perception. The hand models in the virtual environment mirror hand movements of the user and user targets and selects the keys via hand models. AiRType fully leverages the additional dimension without restraining the interaction space by users' arm lengths. It can be attached to anywhere and can be scaled freely. We evaluated and compared AiRType with the baseline-the built-in keyboard of Magic Leap 1. AiRType shows 27&#37; decrease in the error rate, 3.3&#37; increase in character-per-second, and 9.4&#37; increase in user satisfaction.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1566">Head-Worn Markerless Augmented Reality Inside A Moving Vehicle</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: C42 </strong></small> <br /> </p>

<!---->
    
    <p><i>Zhiwei Zhu</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/yNXXpmcYuYQ" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1566" class="wrap-collabsible"> <input id="collapsibleabstractC1566" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1566" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This paper describes a system that provides general head-worn outdoor augmented reality (AR) capability for the user inside a moving vehicle. Our system follows the concept of combining pose estimation from both vehicle navigation system and wearable sensors to address the failure of commercial AR devices inside a moving vehicle. We continuously match natural visual features from the camera against a prebuilt database of interior vehicle scenes. To improve the robustness in a moving vehicle with other passengers, a human detection module is adapted to filter out people from the camera scene. Experiments demonstrate the effectiveness of the proposed solution.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1573">Using External Video to Attack Behavior-Based Security Mechanisms in Virtual Reality (VR)</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: C43 </strong></small> <br /> </p>

<!---->
    
    <p><i>Robert Miller</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/Y6aqFL8C-e0" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1573" class="wrap-collabsible"> <input id="collapsibleabstractC1573" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1573" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>As VR systems become prevalent in domains such as healthcare and education, sensitive data must be protected from attacks. Password-based techniques are circumvented once an attacker gains access to the user's credentials. Behavior-based approaches are susceptible to attacks from malicious users who mimic the actions of a genuine user or gain access to the 3D trajectories. We investigate a novel attack where a malicious user obtains a 2D video of genuine user interacting in VR. We demonstrate that an attacker can extract 2D motion trajectories from the video and match them to 3D enrollment trajectories to defeat behavior-based VR security.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1580">VR-based Context Priming to Increase Student Engagement and Academic Performance</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: C44 </strong></small> <br /> </p>

<!---->
    
    <p><i>Daniel Hawes</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/WrEeicPC0QA" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1580" class="wrap-collabsible"> <input id="collapsibleabstractC1580" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1580" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Research suggests that virtual environments can be designed to increase engagement and performance with many cognitive tasks. This paper compares the efficacy of specifically designed 3D environments intended to prime these effects within Virtual Reality (VR). A 27-minute seminar &quot;The Creative Process of Making an Animated Movie&quot; was presented to 51 participants within three VR learning spaces: two prime and one no-prime. The prime conditions included two situated learning environments&#59; an animation studio and a theatre with animation artifacts vs. the no-prime: theatre without artifacts. Increased academic performance was observed in both prime conditions. A UX survey was also completed.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1603">From 2D to 3D: Facilitating Single-Finger Mid-Air Typing on Virtual Keyboards with Probabilistic Touch Modeling</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D11 </strong></small> <br /> </p>

<!---->
    
    <p><i>Chen Liang</i></p>
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=6BEOYAMo5S4" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1603" class="wrap-collabsible"> <input id="collapsibleabstractC1603" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1603" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Mid-air text entry on virtual keyboards suffers from the lack of tactile feedback, bringing challenges to both tap detection and input prediction. In this poster, we demonstrated the feasibility of efficient single-finger typing in mid-air through probabilistic touch modeling. We first collected users' typing data on different sizes of virtual keyboards. Based on analyzing the data, we derived an input prediction algorithm that incorporated probabilistic touch detection and elastic probabilistic decoding. In the evaluation study where the participants performed real text entry tasks with this technique, they reached a pick-up single-finger typing speed of 24.0 WPM with 2.8&#37; word-level error rate.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="C1625">Splitting Large Convolutional Neural Network Layers to Run Real-Time Applications on Mixed-Reality Hardware: Extended Abstract</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D12 </strong></small> <br /> </p>

<!---->
    
    <p><i>Anthony Beug</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/-E4fvkwCKyA" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractC1625" class="wrap-collabsible"> <input id="collapsibleabstractC1625" class="toggle" type="checkbox" /> <label for="collapsibleabstractC1625" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>When executing a computationally expensive Convolutional Neural Network (CNN) in a real-time mixed-reality application, some convolutional layers may take longer than the target frame time to execute. In this work, dropped frames produced by large convolutional layers are avoided by dividing the work performed in a convolution so that it can be executed over multiple frames. Existing convolution splitting techniques are applied to pretrained CNNs with expensive convolutions, and static schedules are designed to execute the resulting layers over multiple frames. Overhead is introduced, but the average frame rate is increased since delays produced by computing large layers are avoided.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1092">Digital Twins of Wave Energy Generation Based on Artificial Intelligence</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D13 </strong></small> <br /> </p>

<!---->
    
    <p><i>Yuqi Liu: College of Computer Science and Technology&#59; Xiaocheng Liu: College of Computer Science and Technology&#59; Jinkang Guo: Qingdao University&#59; Ranran Lou: Qingdao University&#59; Zhihan Lv: Uppsala University</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/iiRWyM8fM7M" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1092" class="wrap-collabsible"> <input id="collapsibleabstractPO1092" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1092" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Ocean waves provide a large amount of renewable energy, and Wave energy converter (WEC) can convert wave energy into electric energy. This paper proposes a visualization platform for wave power generation. The platform can monitor various indicators of wave power generation in real time, combined with Long Short-Term Memory (LSTM) neural network to predict wave power and electricity consumption. We make digital twins of a wave power plant in a computer, allowing users to remotely view the factory through VR glasses.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1131">Who do you look like? - Gaze-based authentication for workers in VR</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D14 </strong></small> <br /> </p>

<!---->
    
    <p><i>Karina LaRubbio: University of Florida&#59; Jeremiah Wright: University of Florida&#59; Brendan David-John: University of Florida&#59; Andreas Enqvist: University of Florida&#59; Eakta JAIN: University of Florida</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/5Q-Yhv7c0dw" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1131" class="wrap-collabsible"> <input id="collapsibleabstractPO1131" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1131" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Behavior-based authentication methods are actively being developed for XR. In particular, gaze-based methods promise continuous authentication of remote users. However, gaze behavior depends on the task being performed. Identification rate is typically highest when comparing data from the same task. In this study, we compared authentication performance using VR gaze data during random dot viewing, 360-degree image viewing, and a nuclear training simulation. We found that within-task authentication performed best for image viewing (72&#37;). The implication for practitioners is to integrate image viewing into a VR workflow to collect gaze data that is viable for authentication.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1155">High-Quality Surface-Based 3D Reconstruction Using 2.5D Maps</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D24 </strong></small> <br /> </p>

<!---->
    
    <p><i>Lingxiao Song: Beijing Institute of Technology&#59; Xiao Yu: Beijing Institute of Technology&#59; Huijun Di: Beijing Institute of Technology&#59; Weiran Wang: Beijing Institute of Technology</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/s-mgZu_Jk7M" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1155" class="wrap-collabsible"> <input id="collapsibleabstractPO1155" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1155" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Previous works on RGB-D reconstruction are based on voxels, points, or meshes, which are either too computationally expensive to represent high-resolution 3D models, or not convenient for model regularization to deal with noise and errors. In this paper, we propose a new method to reconstruct 3D models with more accurate geometric information and better texture. Our method uses abstract surfaces consisting of different points with similar information as units of the model. To reduce the complexity, we use 2.5D heightmaps to represent each surface in the reconstructed model, making it convenient to do regularization. Experiments demonstrate the effectiveness of our method.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1173">Using Direct Volume Rendering for Augmented Reality in Resource-constrained Platforms</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D23 </strong></small> <br /> </p>

<!---->
    
    <p><i>Berk Cetinsaya: University of Central Florida&#59; Carsten Neumann: University of Central Florida&#59; Dirk Reiners: University of Central Florida</i></p>
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=4zpvQR9BtV0" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1173" class="wrap-collabsible"> <input id="collapsibleabstractPO1173" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1173" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Rendering a large volume is a challenging task on mobile and Augmented Reality (AR) devices due to lack of memory space and device limitations. Therefore, we implemented an Empty Space Skipping (ESS) optimization algorithm to render the high-quality large models on HoloLens. We designed and developed a system to visualize the computerized tomography (CT) scan data and Digital Imaging and Communications in Medicine (DICOM) files on Microsoft HoloLens 2. We used the Unity3D game engine to develop the system. As a result, we achieved about 10 times more frames per second (fps) on a high-quality model than the non-optimized version.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="PO1174">Emotional Empathy and Facial Mimicry of Avatar Faces</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D22 </strong></small> <br /> </p>

<!---->
    
    <p><i>Angela Saquinaula: Western Connecticut State University&#59; Adriel Juarez: Monmouth University&#59; Joe Geigel: Rochester Institute of Technology&#59; Reynold Bailey: Rochester Institute of Technology&#59; Cecilia Ovesdotter Alm: Rochester Institute of Technology</i></p>
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=ar6LgzBntFE" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1174" class="wrap-collabsible"> <input id="collapsibleabstractPO1174" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1174" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We explore the extent to which empathetic reactions are elicited when subjects view 3D motion-capture driven avatar faces compared to viewing human faces. Through a remote study, we captured subjects' facial reactions when viewing avatar and humans faces, and elicited self reported feedback regarding empathy. Avatar faces varied by gender and realism. Results show no sign of facial mimicry&#59; only mimicking of slight facial movements with no solid consistency. Participants tended to empathize with avatars when they could adequately identify the stimulus' emotion. As avatar realism increased, it negatively impacted the subjects' feelings towards the stimuli.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="PO1178">A Time Reversal Symmetry Based Real-time Optical Motion Capture Missing Marker Recovery Method</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D21 </strong></small> <br /> </p>

<!---->
    
    <p><i>Dongdong Weng: Beijing Institute of Technology&#59; Yihan Wang: Beijing Institute of Technology&#59; Dong Li: Beijing Institute of Technology</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/1QLTzN3k4h8" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1178" class="wrap-collabsible"> <input id="collapsibleabstractPO1178" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1178" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This paper proposes a deep learning model based on time reversal symmetry for real-time recovery of continuous missing marker sequences in optical motion capture. This paper firstly uses time reversal symmetry of human motion as a constraint of the model. BiLSTM is used to describe the constraint and extract the bidirectional spatiotemporal features. This paper proposes a weight position loss function for model training, which describes the effect of different joints on the pose. Compared with the existing methods, the experimental results show that the proposed method has higher accuracy and good real-time performance.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1181">Let Every Seat Be Perfect! A Case Study on Combining BIM and VR for Room Planning</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D38 </strong></small> <br /> </p>

<!---->
    
    <p><i>Wai Tong: The Hong Kong University of Science and Technology&#59; Haotian Li: The Hong Kong University of Science and Technology&#59; Huan Wei: The Hong Kong University of Science and Technology&#59; Liwenhan Xie: Hong Kong University of Science and Technology&#59; Yanna Lin: The Hong Kong University of Science and Technology&#59; Huamin Qu: The Hong Kong University of Science and Technology</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/-nz2LG7Gzio" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1181" class="wrap-collabsible"> <input id="collapsibleabstractPO1181" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1181" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>When communicating indoor room design, professional designers normally rely on software like Revit to export walk-through videos for their clients. However, a lack of in-situ experience restricts the ultimate users from evaluating the design and hence provides limited feedback, which may lead to a rework after actual construction. In this case study, we explore empowering end-users by exposing rich design details through a Virtual Reality (VR) application based on building an information model. Qualitative feedback in our user study shows promising results. We further discuss the benefits of the approach and opportunities for future research.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1185">Virtual reality-based distraction on pain and performance during and after moderate-vigorous intensity cycling</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D37 </strong></small> <br /> </p>

<!---->
    
    <p><i>Carly Wender: Kessler Foundation&#59; Phillip Tomporowski: University of Georgia&#59; Sun Joo (Grace) Ahn: University of Georgia&#59; Patrick O'Connor: University of Georgia</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/KlCCzJWAsc0" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1185" class="wrap-collabsible"> <input id="collapsibleabstractPO1185" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1185" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This experiment measured effects of visual perceptual load (PL) within immersive virtual reality (VR) on exercise-induced pain during cycling. Using a within-subjects design (n=43), participants cycled at a perceptually &quot;hard&quot; intensity for 10 minutes without VR (i.e., no PL - NPL) or with VR of low or high PL (i.e., LPL or HPL). Mean quadriceps pain was significantly lower in the NPL condition than either the LPL (d=0.472) or HPL conditions (d=0.391). Mean cycling performance was significantly greater during the LPL condition. Compared to traditional cycling (NPL), cycling in the LPL condition resulted in greater exercise performance despite greater pain.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1188">Evaluating 3D Visual Fatigue Induced by VR Headset Using EEG and Self-attention CNN</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D36 </strong></small> <br /> </p>

<!---->
    
    <p><i>Haochen Hu: Beijing Institution of Technology&#59; Yue Liu: Beijing Institute of Technology&#59; Kang Yue: Beijing INstitute of Technology</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/0snuEkI5VfQ" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1188" class="wrap-collabsible"> <input id="collapsibleabstractPO1188" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1188" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>3D visual fatigue is one of the major factors that hinder the development of virtual reality contents towards larger population. We proposed an EEG-based self-attention CNN model to evaluate user's 3D visual fatigue in an end-to-end fashion. We adopted a wavelet-based convolution to extract spatiotemporal information and prevent overfitting. Besides, a self-attention layer was added to the feature extractor backbone to cope with the subject-variation problem in EEG-decoding. The proposed method is compared with four state-of-the-art methods, and the results demonstrate that our model has the best performance among all methods in subject-dependent and cross-subject scenarios.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1196">Perception of Symmetry of Actual and Modulated Self-Avatar Gait Movements During Treadmill Walking</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D35 </strong></small> <br /> </p>

<!---->
    
    <p><i>Iris Willaert: Ecole de technologie superieure&#59; Rachid Aissaoui: CHUM research center&#59; Sylvie Nadeau: University of Montreal&#59; Cyril Duclos: Universit&eacute; de Montreal&#59; David Labbe PhD: Ecole de technologie superieure</i></p>
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=EDTC8OcuHPA&amp;ab_channel=IrisWillaert" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1196" class="wrap-collabsible"> <input id="collapsibleabstractPO1196" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1196" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In virtual reality, it is possible to simulate one's visual self-representation by mapping one's body movements to those of an avatar. Accepting the virtual body as part of one's own body creates an ownership illusion. This study aimed to assess the perception threshold between a subject's actual gait movements and those of their modulated self-avatar during treadmill walking.
Preliminary results on two subjects suggest that healthy subjects
can detect the mismatch, but differences may exist between
subjects</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1204">Moving Soon? Rearranging Furniture using Mixed Reality</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: B31 </strong></small> <br /> </p>

<!---->
    
    <p><i>Shihao Song: Beijing Institute of Technology&#59; Yujia Wang: Beijing Institute of Technology&#59; Wei Liang: Beijing Institute of Technology&#59; Xiangyuan Li: Beijing Forestry University</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/6m-JRvDGkvA" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1204" class="wrap-collabsible"> <input id="collapsibleabstractPO1204" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1204" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We present a mixed reality (MR) system to help users with a houseful of furniture moving from an existing home into a new space, inheriting the preferences of furniture layout from the previous scene.<br />With the RGB-D cameras mounted on a mixed reality device, Microsoft HoloLens 2, our system first reconstructs the 3D model of the existing scene and leverages a deep learning-based approach to detect and to group objects. Then, our system generates a personalized furniture layout by optimizing a cost function, incorporating the analyzed relevance of between and within groups, and the spatial constraints of the new layout.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="PO1205">Add-on Occlusion: An External Module for Optical See-through Augmented Reality Displays to Support Mutual Occlusion</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D42 </strong></small> <br /> </p>

<!---->
    
    <p><i>Yan Zhang: Nara Institute of Science and Technology&#59; Kiyoshi Kiyokawa: Nara Institute of Science and Technology&#59; Naoya Isoyama: Nara Institute of Science and Technology&#59; Hideaki Uchiyama: Nara Institute of Science and Technology&#59; Xubo Yang: SHANGHAI JIAO TONG UNIVERSITY</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/_yom8z7jKvE" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1205" class="wrap-collabsible"> <input id="collapsibleabstractPO1205" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1205" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>The occlusion function benefits augmented reality (AR) in many aspects. However, existing occlusion-capable optical see-through augmented reality (OC-OST-AR) displays are designed by integrating virtual displays into a dedicated occlusion-capable architecture, hereby, we miss merits from emerging OST-AR displays. In this article, we propose an external occlusion module that can be added to common OST-AR displays. Per-pixel occlusion is supported with a small form-factor by using polarization-based optical path compression. The occlusion function can be switched on/off by controlling the incident light polarization. A prototype within a volume of 6x6x3cm is built. A preliminary experiment proves that occlusion is realized successfully.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1213">Assist Home Training Table Tennis Skill Acquisition via Immersive Learning and Web Technologies</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: D41 </strong></small> <br /> </p>

<!---->
    
    <p><i>Jian-Jia Weng: National Tsing Hua University&#59; Yu-Hsin Wang: National Tsing Hua University&#59; Calvin Ku: National Tsing Hua University&#59; Dong-Xian Wu: National Tsing Hua University&#59; Yi-Min Lau: National Tsing Hua University&#59; Wan-Lun Tsai: National Cheng Kung University&#59; Tse-Yu Pan: National Tsing Hua University&#59; Min-Chun Hu: National Tsing Hua University&#59; Hung-Kuo Chu: National Tsing Hua University&#59; Te-Cheng Wu: National Tsing Hua University</i></p>
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=oQF9e4T2rhM" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1213" class="wrap-collabsible"> <input id="collapsibleabstractPO1213" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1213" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Sports applications in Virtual Reality (VR) have become immensely popular for training skill-based sports like table tennis. However, the existing researches do not focus on designing an intuitive system for efficient communication between the trainee and the coach. We developed a VR table tennis training system for table tennis skill acquisition that focuses on helping coaches to convey a player's mistake clearly. Our system consists of a VR training system where trainees can learn a skill gradually and a web-based feedback annotative tool for coaches. Trainees can examine their mistakes through a tablet or an immersive VR world.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1236">Touch the History in Virtuality: Combine Passive Haptic with 360&deg; videos in history learning</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: E23 </strong></small> <br /> </p>

<!---->
    
    <p><i>YanXiang Zhang: University of Science and Technology of China&#59; YingNa Wang: University of Science and Technology of China&#59; QingQin Liu: University of Science and Technology of China</i></p>
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=55YnY0zQi0U" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1236" class="wrap-collabsible"> <input id="collapsibleabstractPO1236" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1236" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Based on ethical principles and Asimov's three laws of robotics, this article discusses three ethical issues generated by the use of virtual reality to &quot;revive&quot; or come into contact with the deceased, including &quot;not harm humans,&quot; &quot;rights-related issues,&quot; and &quot;fairness and meaning.&quot; And religious factors are also taken into consideration. It is necessary to predict the ethical risk of virtual reality &quot; revive &quot; of the deceased, which will make ethics play a better role in its future development. In addition, it will help stakeholders to pay more attention to the ethical issues involved in virtual avatars.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="PO1239">The Sloped Shoes: Influence Human Perception of the Virtual Slope</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: E22 </strong></small> <br /> </p>

<!---->
    
    <p><i>YanXiang Zhang: University of Science and Technology of China&#59; JiaLing Wu: University of Science and Technology of China&#59; QingQin Liu: University of Science and Technology of China</i></p>
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=NQQlxA2YAJI" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1239" class="wrap-collabsible"> <input id="collapsibleabstractPO1239" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1239" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In this study, people were allowed to walk uphill or downhill in virtual environments by changing the slope of shoes, while they walked on a flat wooden board in physical environments. We can explore the impact of the shoe slope on users' perception of walking uphill or downhill in the virtual world. We find that the slope of the shoes affects participants' perception, increasing their sense of realism when walking uphill and downhill in the virtual world. With changing the shoe slope, participants' perception of the possibility of walking uphill or downhill will change and establish corresponding detection thresholds.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1243">Redirected Walking in 360&deg; Video: Effect of Environment Size on Detection Thresholds for Translation and Rotation Gains</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: E21 </strong></small> <br /> </p>

<!---->
    
    <p><i>YanXiang Zhang: University of Science and Technology of China&#59; QingQin Liu: University of Science and Technology of China&#59; YingNa Wang: University of Science and Technology of China</i></p>
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=RHHEWZ6gE54" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1243" class="wrap-collabsible"> <input id="collapsibleabstractPO1243" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1243" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Using real walking to control the playback of the 360&deg; videos is a natural and immersive way to match visual and self-motion perception. Redirected walking can enable users to walk in limited physical tracking space but experience larger scenes. Environment size may affect user perception in 360&deg; videos. We conducted a user study about the detection thresholds (DTs) for translation and rotation gains in 360&deg; video-based virtual environments in three scenes with different widths. Results show that environment size of the scene increases the DTs for both lower and upper translation gains but doesn't affect the DTs for rotation gains.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="PO1246">Movement Augmentation in Virtual Reality: Impact on Sense of Agency Measured by Subjective Responses and Electroencephalography</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: E26 </strong></small> <br /> </p>

<!---->
    
    <p><i>Liu Wang: Xi'an Jiaotong-Liverpool University&#59; Mengjie Huang: Xi'an Jiaotong-Liverpool University&#59; Chengxuan Qin: Xi'an Jiaotong-Liverpool University&#59; Yiqi Wang: University College London&#59; Rui Yang: Xi'an Jiaotong-Liverpool University</i></p>
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=xuUb2Fn-QCw" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1246" class="wrap-collabsible"> <input id="collapsibleabstractPO1246" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1246" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Virtual movement augmentation, which refers to the visual amplification of remapped movement, shows potential to be applied in motion-related virtual reality programs. Sense of agency (SoA), which measures the user's feeling of control in their action, has not been fully investigated for augmented movement. This study investigated the effect of augmented movement at three different levels (baseline, medium, and high) on users' SoA using both subjective responses and electroencephalography (EEG). Results show that SoA can be boosted slightly at medium augmentation level but drops at high level. The augmented virtual movement only helps to enhance SoA to a certain extent.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1251">A Location-Triggered Augmented Reality Walking Tour Using Snap Spectacles 2021</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: E27 </strong></small> <br /> </p>

<!---->
    
    <p><i>AadilMehdi Sanchawala: Columbia University&#59; Mara Dimofte: Columbia University&#59; Steven Feiner: Columbia University</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/1-baRouqURk" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1251" class="wrap-collabsible"> <input id="collapsibleabstractPO1251" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1251" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We present an on-site 3D-animated audiovisual tour guide augmented reality application developed for Snap Spectacles 2021. The primary goal of this project is to explore how to use this experimental product to create an augmented reality tour guide. In addition, we present the design considerations for the user interface and the underlying system architecture. We illustrate the workflow of the tour application and discuss our experience working with Spectacles 2021 and its experimental API. We also present our design choices and directions for future work.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1266">Video2Force: Experiencing Object Motion in Video with Dynamic Force Feedback based on Bio-Inspired Sensing and Processing</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: E28 </strong></small> <br /> </p>

<!---->
    
    <p><i>Guangxin Zhao: School of Computer Science and Engineering, Beijing Technology and Business University&#59; Zhaobo Wang: The University of Sydney&#59; Xiaoming Chen: Beijing Technology and Business University&#59; Zhicheng Lu: The University of Sydney&#59; Yuk Ying Chung: The University of Sydney&#59; Haisheng LI: Beijing Technology and Business University</i></p>
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=fYy3WRRLCHA" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1266" class="wrap-collabsible"> <input id="collapsibleabstractPO1266" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1266" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In this work, we propose Video2Force, a framework that can estimate the video object motion and its resulting &quot;force&quot; by leveraging the emerging bio-inspired vision sensors for low-complexity and low-latency visual information processing. While the user watches video, the estimated force is &quot;delivered&quot; to the user's sensation with dynamic force feedback via emerging haptic gloves, synchronized with the video content. Consequently, the user is enabled to experience the &quot;virtual existence&quot; of the video object and its motion. Preliminary experimental results demonstrate that Video2Force is feasible and can enhance the users' presence in video experience.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1272">Immersive Visualization of Sneeze Simulation Data on Mobile Devices</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: E33 </strong></small> <br /> </p>

<!---->
    
    <p><i>Liangding Li: University of Central Florida&#59; Douglas Fontes: University of Central Florida&#59; Carsten Neumann: University of Central Florida&#59; Dirk Reiners: University of Central Florida&#59; Carolina Cruz-Neira: University of Central Florida&#59; Michael Kinzel: University of Central Florida</i></p>
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=X18fi9krrBg&amp;list=PLZtzOwu_ts4gnc-wixx1a_FRC8x8YcehP" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1272" class="wrap-collabsible"> <input id="collapsibleabstractPO1272" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1272" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>One key factor in stopping the spread of COVID-19 is practicing social distancing. Visualizing possible sneeze droplets' transmission routes in front of an infected person might be an effective way to help people understand the importance of social distancing. This paper presents a mobile virtual reality (VR) interface that helps people visualize droplet dispersion from the target person's view. We implemented a VR application to visualize and interact with the sneeze simulation data immersively. Our application provides an easy way to communicate the correlation between social distance and infected droplets exposure, which is difficult to achieve in the real world.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1282">Towards Retargetable Animations for Industrial Augmented Reality</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: E32 </strong></small> <br /> </p>

<!---->
    
    <p><i>Reza Manuel Mirzaiee: University of Virginia&#59; Teodor I Vernica: Aarhus University&#59; Kurt Scheuringer: Lockheed Martin Corporation&#59; William Bernstein: Air Force Research Lab</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/l4BYlPQqTWs" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1282" class="wrap-collabsible"> <input id="collapsibleabstractPO1282" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1282" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>The adoption of Augmented Reality (AR) within manufacturing has surged. However, updating animations due to upstream changes or design variations is expensive. Currently, platform-agnostic standards for computer-aided design files do not adequately specify kinematic animations. Consequently, animations require manual, individual updates. We showcase a method for implementing geometry-independent AR assembly animations using skeletal armatures, a technique widely used in the entertainment industry. This technique allows upstream engineering changes to propagate through to the AR assembly visualization, leading to a more automated pipeline for handling animations in Industrial AR systems.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="PO1285">Synesthesia AR: Creating Sound-to-Color Synesthesia in Augmented Reality</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: E31 </strong></small> <br /> </p>

<!---->
    
    <p><i>Shashaank N: Columbia University&#59; Steven Feiner: Columbia University</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/QbmLwAd2UOc " target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1285" class="wrap-collabsible"> <input id="collapsibleabstractPO1285" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1285" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Sound-to-color synesthesia is a neurological condition where people experience different colors and shapes when listening to music. We present an augmented reality application that aims to create an interactive synesthesia experience for non-synesthetes. In this application, users can visualize colors corresponding to each unique note in the 12-tone equal-temperament tuning system, and the auditory input can be selected from audio files or real-time microphone. A gestural hand-tracking interface allows users to paint the world space in visualized synesthetic colors.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1297">Studying the Effect of Physical Realism on Time Perception in a HAZMAT VR Simulation</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: C32 </strong></small> <br /> </p>

<!---->
    
    <p><i>Kadir Baturalp Lofca: University of North Carolina at Greensboro&#59; Jason Haskins: Nextgen Interactions&#59; Jason Jerald: Nextgen Interactions&#59; Regis Kopper: University of North Carolina at Greensboro</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/_q_88EgLuBk" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1297" class="wrap-collabsible"> <input id="collapsibleabstractPO1297" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1297" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Our research focuses on how physical props in virtual reality (VR) can affect users' time perception. We designed an experiment with the goal of comparing users' perception of time when using physical props in VR as compared to standard controllers and only virtual elements. In order to quantify this effect, time estimates for both conditions are compared to time estimates for a matching real-world task. In this experiment, participants assume the role of a firefighter trainee, going through a HAZMAT scenario, where they touch and interact with different physical props that match the virtual elements of the scene.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h3 id="PO1309">Learning Environments in AR Comparing Tablet and Head-mounted Augmented Reality Devices at Room and Table Scale</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: C33 </strong></small> <br /> </p>

<!---->
    
    <p><i>Paul Craig: University of Minnesota&#59; Peter Willemsen: University of Minnesota Duluth&#59; Edward Downs: University of Minnesota Duluth&#59; Alex Lover: University of Minnesota Duluth&#59; William Barber: University of Minnesota Duluth</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/i1wYTf48WW4" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1309" class="wrap-collabsible"> <input id="collapsibleabstractPO1309" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1309" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This paper presents work to examine how presentation scale and device form factor may affect learning in augmented reality (AR) environments. We conducted a 2 (form factor) x 2 (scale) experiment in which 131 participants explored an AR learning environment using either tablet AR or a Hololens2 crossed with either full room scale or at table scale. Dependent variables measured participants declarative knowledge about information acquired in the environment as well as their understanding of spatial layout. Initial analysis suggests comparable outcomes across all manipulations with respect to acquiring declarative and spatial knowledge.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    <h3 id="PO1310">The Digital Twins of Thor's Hammer Based on Motion Sensing</h3>
    <p><strong>Poster</strong></p>
    
<p> <small><strong style="color: black;"> Booth: C34 </strong></small> <br /> </p>

<!---->
    
    <p><i>Zengxu Bian: College of Computer Science and Technology&#59; Yuqi Liu: College of Computer Science and Technology&#59; Jinkang Guo: Qingdao University&#59; Zhihan Lv: Uppsala University</i></p>
    
        <p>Teaser Video: <a href="https://youtu.be/JpHYGdFWp2w" target="_blank">Watch Now</a></p>
    
    
    
    <div id="abstractPO1310" class="wrap-collabsible"> <input id="collapsibleabstractPO1310" class="toggle" type="checkbox" /> <label for="collapsibleabstractPO1310" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Ancient humans attribute the phenomenon of thunder and lightning to divine power. The power of Thor that can lift Thor's Hammer, the body not be hurt by thunder and lightning. It's not impossible for us to control thunder and lightning like Thor. The Digital Twins system of the robotic arm designed in this paper integrates the physical device of the robotic arm, the digital model of robotic arm, the body sense interaction, and the virtual-reality mapping module. It can digitally control the robotic arm.With this system, we can all lift Thor's hammer in the future.</p>
            </div>
        </div>
    </div>   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
</div>

<!--

<div>
    

    <h3 id="PO1002">Distant Hand Interaction Framework in Augmented Reality</h3>
    
<p> <small><strong style="color: black;"> Booth: C27 </strong></small> <br> </p>
    
    <p><i>Jesus Ugarte, Nahal Norouzi, Austin Erickson, Gerd Bruder, Greg Welch</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=OD97erd3lxI" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1002" class="wrap-collabsible"> <input id="collapsibleabstractPO1002" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1002" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Recent augmented reality (AR) head-mounted displays support shared multi-user experiences, with past research studying the enhancement of interpersonal communication cues. However, less is known about distant interaction in AR and, in particular, distant communication. In this demonstration, we present a research framework for distant hand interaction in AR, including different techniques for hand communication, such as distant communication through symbolic hand gestures and distant drawing.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1003">Mid-air Haptic Texture Exploration in VR</h3>
    
<p> <small><strong style="color: black;"> Booth: C32 </strong></small> <br> </p>
    
    <p><i>Orestis Georgiou, Jonatan Martinez, Abdenaceur Abdouni, Adam Harwood</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/ABAt3CF6XxU" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1003" class="wrap-collabsible"> <input id="collapsibleabstractPO1003" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1003" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Mid-air haptic feedback has traditionally been used to enhance gesture input interactions. Here we present a VR research demo that expands such interactions to include for active haptic exploration. In the demo, the user can explore a virtual object using both hands and feel its intrinsic properties such as consistency and texture. The paper describes the physical apparatus used, the haptic rendering techniques leveraged, and the demo's relevance to applications such as VR shopping.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1008">We Are Oulu: Exploring Situated Empathy through a Communal Virtual Reality Experience</h3>
    
<p> <small><strong style="color: black;"> Booth: C26 </strong></small> <br> </p>
    
    <p><i>Mohammad Sina Kiarostami, Aku Visuri, Simo Hosio</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=nWQQBg16_xo" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1008" class="wrap-collabsible"> <input id="collapsibleabstractPO1008" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1008" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In this research, we explore and measure situated empathy. We focus on the hardships of an international community in a foreign country using a virtual reality experiment. Our aim is to facilitate a better understanding of an international community's quality of life and unique difficulties in a society. To this end, we designed a VR experiment with three main stages: data collection, a pre-experiment questionnaire and a post-experiment questionnaire with a concluding interview.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1011">Asymmetric interfaces with stylus and gesture for VR sketching</h3>
    
<p> <small><strong style="color: black;"> Booth: C33 </strong></small> <br> </p>
    
    <p><i>Qianyuan Zou, Huidong Bai, Lei Gao, Allan Fowler, Mark Billinghurst</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/fLlSrU3GpkA" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1011" class="wrap-collabsible"> <input id="collapsibleabstractPO1011" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1011" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Virtual Reality (VR) can be used for design and artistic applications. However, traditional symmetrical input devices are not specifically designed as creative tools and may not fully meet artist needs. In this demonstration, we present a variety of tool-based asymmetric VR interfaces to assist artists to create artwork with better performance and easier effort. We conducted a pilot study showing that most users prefer to create art with different tools in both hands.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1012">Pixel Processor Arrays For Low Latency Gaze Estimation</h3>
    
<p> <small><strong style="color: black;"> Booth: C25 </strong></small> <br> </p>
    
    <p><i>Laurie Bose, Piotr Dudek, Stephen Carey, Jianing Chen</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/x2Ac1Y8hhW4" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1012" class="wrap-collabsible"> <input id="collapsibleabstractPO1012" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1012" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We demonstrate gaze tracking at over 10,000 Hz, with processing latency below 0.1 ms via use of a Pixel Processor Array (PPA) vision sensor. The PPA allows visual data to be processed efficiently at the point of light capture. By extracting features used for gaze tracking upon the PPA, we reduce data transfer from sensor to processing from entire images to a hand-full of contextual bytes, saving significant power, time and allowing for frame-rates far exceeding traditional camera sensors.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1013">Aroaro - A Tool for Distributed Immersive Mixed Reality Visualization</h3>
    
<p> <small><strong style="color: black;"> Booth: C34 </strong></small> <br> </p>
    
    <p><i>Fernando Beltr&aacute;n, David C White, Jing Geng</i></p>
    
    
    
    
    <div id="abstractPO1013" class="wrap-collabsible"> <input id="collapsibleabstractPO1013" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1013" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In this research demo we present three immersive scenarios on three XR modalities - VR, immersive AR on HoloLens and 2D AR on Android. 
These scenarios are a network of Harry Potter characters in VR, a map-based visualization of a soldier's history with rich attributes including images and sound on HoloLens, and a visualization of car racing on Android. 
These visualizations have been created with Aroaro our distributed mixed reality data visualization tool.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1014">3DColAR: Exploring 3D Colour Selection and Surface Painting for Head Worn AR using Hand Gestures</h3>
    
<p> <small><strong style="color: black;"> Booth:  </strong></small> <br> </p>
    
    <p><i>Louise M Lawrence. Gun Lee, Mark Billinghurst, Damien Rompapas</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/PGOm-8Vf3Po" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1014" class="wrap-collabsible"> <input id="collapsibleabstractPO1014" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1014" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Color selection and surface painting has been largely unexplored in head-worn AR using hand gestures. We present a system with two key approaches for painting a virtual 3D model using mid- air hand gestures. This includes a virtual pen which the user can grasp using their hand, and the use of the user's fingertip directly. We hope to explore how the various techniques effect users when performing surface painting of virtual objects using mid-air hand gestures via. several user 
studies.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1015">B-Handy: An Augmented Reality System for Biomechanical Measurement</h3>
    
<p> <small><strong style="color: black;"> Booth:  </strong></small> <br> </p>
    
    <p><i>James O Campbell, Alvaro Cassinelli, Daniel Saakes, Damien Rompapas</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=AZOsmsZHv3U" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1015" class="wrap-collabsible"> <input id="collapsibleabstractPO1015" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1015" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>The study of Bio-mechanics allows us to infer measurements without measuring tools. A limitation comes from the complex mental transformations of space involved. The efficiency of this task degrades the larger these measurements become. We present a system that offloads this mental workload by providing visual transformations of space in the form of tracking and duplicating the user's hand in AR.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1016">ORUN - A Virtual reality serious-game for kinematics learning</h3>
    
<p> <small><strong style="color: black;"> Booth:  </strong></small> <br> </p>
    
    <p><i>Jhasmani Tito, Regina Moraes, Tania Basso</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/B2Ay-LsVHkA" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1016" class="wrap-collabsible"> <input id="collapsibleabstractPO1016" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1016" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Virtual Reality is one of the new educational technologies that has an imperative importance due to the possibilities that it offers, such as bringing hands-on experiences for learning physics phenomena. This demo is a serious game, based on VR, and focused on learning of specific concepts of kinematics. The game is intended to deliver an immersive experience in which the student has an active role and whose game design includes theoretical concepts to maintain engagement throughout the tasks.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1017">Demonstrating Immersive Gesture Exploration with GestureExplorer</h3>
    
<p> <small><strong style="color: black;"> Booth:  </strong></small> <br> </p>
    
    <p><i>Ang Li, Jiazhou Liu, Maxime Cordeil, Barrett Ens</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/Kq9APfo7DFc" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1017" class="wrap-collabsible"> <input id="collapsibleabstractPO1017" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1017" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We demonstrate GestureExplorer, which features versatile immersive visualisations to grant the user free control over their perspective, allowing them to gain a better understanding of gestures. It provides multiple data visualisation views, and interactive features to support analysis and exploration of gesture data sets. This demonstration shows the potential of GestureExplorer for providing a useful and engaging experience for exploring gesture data.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1020">NUX IVE - a research tool for comparing voice user interface and graphical user interface in VR</h3>
    
<p> <small><strong style="color: black;"> Booth:  </strong></small> <br> </p>
    
    <p><i>Karolina Buchta, Piotr W&oacute;jcik, Mateusz Pelc, Agnieszka G&oacute;rowska, Duarte Mota, Kostiantyn Boichenko, Konrad Nakonieczny, Krzysztof Wrona, Marta Szymczyk, Tymoteusz Czuchnowski, Justyna Janicka, Damian Ga?uszka, Rados?aw Sterna, Magdalena Igras-Cybulska</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=vgwrknpn5tY" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1020" class="wrap-collabsible"> <input id="collapsibleabstractPO1020" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1020" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We introduce a new IVE designed to compare user interaction between the mode with traditional graphical user interface (GUI) with the mode in which every element of interface is replaced by voice user interface (VUI). In each version, 4 scenarios of interaction with a virtual assistant in a sci-fi location are implemented, each of them lasting several minutes. The IVE is supplemented with tools for automatic generating reports on user behavior (clicktracking, audiotracking and eyetracking).</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1021">Feeding the fish: Interaction design to support listening to accounts of marginalization</h3>
    
<p> <small><strong style="color: black;"> Booth:  </strong></small> <br> </p>
    
    <p><i>Dylan Par&eacute;, John Craig, Scout Windsor</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/orsG09QrLbs" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1021" class="wrap-collabsible"> <input id="collapsibleabstractPO1021" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1021" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We present a study using a virtual reality experience designed to deepen understanding of gender and sexuality-based marginalization in STEM fields as complex experiences with individual and systemic dimensions. Our design supports learners to remain engaged in listening to and learning about the marginalized other. Our analysis describes how participants reflect upon and use the interactions to move through the difficulties of listening to another's stories of marginalization.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1022">Intelligence Visualization for Wave Energy Power Generation</h3>
    
<p> <small><strong style="color: black;"> Booth:  </strong></small> <br> </p>
    
    <p><i>Xiaocheng Liu, Yuqi Liu, Jinkang Guo, Ranran Lou, Zhihan Lv</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/5Zs8tyV4MyM" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1022" class="wrap-collabsible"> <input id="collapsibleabstractPO1022" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1022" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Ocean waves provide a large amount of renewable energy, and Wave energy converter (WEC) can convert wave energy into electric energy with the linear motion of waves. This paper proposes a visualization platform for wave power generation. The platform can intelligently allocate power generation equipment based on the power generation forecast data to achieve precise matching of power generation and power consumption, thereby improving overall power generation efficiency.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1024">Liquid digital twins based on magnetic fluid toy</h3>
    
<p> <small><strong style="color: black;"> Booth:  </strong></small> <br> </p>
    
    <p><i>Yuqi Liu, Zengxu Bian, Xiaocheng Liu, Zhihan Lv</i></p>
    
    
    
    
    <div id="abstractPO1024" class="wrap-collabsible"> <input id="collapsibleabstractPO1024" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1024" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>As a new type of functional material,magnetic fluid has both the fluidity of liquid and the magnetic properties of solid magnetic material. By controlling the magnets,one can simulate the effect of manipulating liquids like a sea emperor.This will provide new ideas for the multiverse of the metaverse. Therefore,this paper hopes to provide a control idea for the future application of magnetic fluid by performing Digital Twins simulation of magnetic fluid.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1012">Effects of Clutching Mechanism on Remote Object Manipulation Tasks</h3>
    
<p> <small><strong style="color: black;"> Booth: B11 </strong></small> <br> </p>
    
    <p><i>Zihan Gao</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/coWStqGOQG0" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1012" class="wrap-collabsible"> <input id="collapsibleabstractC1012" class="toggle" type="checkbox"> <label for="collapsibleabstractC1012" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Remote object manipulation in practice is often an iterative process that needs clutching. However, while many interaction techniques have been designed for manipulating remote objects, clutching mechanism is often an important but overlooked aspect in manipulation tasks. In this paper, we evaluate the effects of clutching mechanism on remote object manipulation tasks, which compares two clutching mechanisms under various tasks settings. The results suggested that an efficient clutching mechanism can effectively improve the usability in remote object manipulation tasks.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1015">A Testbed for Exploring Multi-Level Precueing in Augmented Reality</h3>
    
<p> <small><strong style="color: black;"> Booth: B12 </strong></small> <br> </p>
    
    <p><i>Jen-Shuo Liu</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/iC-sHV8GEJY" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1015" class="wrap-collabsible"> <input id="collapsibleabstractC1015" class="toggle" type="checkbox"> <label for="collapsibleabstractC1015" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Precueing information about upcoming subtasks prior to performing them has the potential to make an entire task faster and easier to accomplish than cueing only the current subtask. Most AR and VR research on precueing has addressed path-following tasks requiring simple actions at a series of locations, such as pushing a button or just visiting that location. We present a testbed for exploring multi-level precueing in a richer task that requires the user to move their hand between specified locations, transporting an object between some of them, and rotating it to a designated orientation.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1021">Resolution Tradeoff in Gameplay Experience, Performance, and Simulator Sickness in Virtual Reality Games</h3>
    
<p> <small><strong style="color: black;"> Booth: B13 </strong></small> <br> </p>
    
    <p><i>Jialin Wang</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/lLPIOWe4mDw" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1021" class="wrap-collabsible"> <input id="collapsibleabstractC1021" class="toggle" type="checkbox"> <label for="collapsibleabstractC1021" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Higher resolution is one of the main directions and drivers in the development of Virtual Reality (VR) Head-Mounted Displays (HMDs). However, given their associated higher cost, it is unclear the benefits of having higher resolution on user experience, especially in VR games. This research aims to investigate the effects of resolution in gameplay experience and simulator sickness for VR games. To this end, we designed an experiment to collect gameplay experience, simulator sickness (SS), and player performance data with a VR First-Person Shooter game. Our results indicate that 2K resolution is an important threshold for an enhanced gameplay experience without affecting performance and increasing SS levels.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1023">Taming Cyclops: Mixed Reality Head-Mounted Displays as Laser Safety Goggles for Advanced Optics Laboratories</h3>
    
<p> <small><strong style="color: black;"> Booth: B14 </strong></small> <br> </p>
    
    <p><i>Ke Li</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/wv7lltZVv30" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1023" class="wrap-collabsible"> <input id="collapsibleabstractC1023" class="toggle" type="checkbox"> <label for="collapsibleabstractC1023" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In this poster paper, we present a mixed reality application for laser eye protection based on a video see-through head-mounted display. With our setup, laser lab users perceive the real environment through the head-mounted display, using it as a substitute for laser safety goggles required by health and safety regulations. We designed and evaluated our prototype with a human-centered computing approach at the Deutsches Elektronen-Synchrotron where there exists some of the most advanced and extreme optics laboratory working conditions. We demonstrated that virtual reality headsets can be an attractive future alternative to conventional laser safety goggles.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1028">VCoach: Enabling Personalized Boxing Training in Virtual Reality</h3>
    
<p> <small><strong style="color: black;"> Booth: B14 </strong></small> <br> </p>
    
    <p><i>Hao Chen</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/bVH0yf5FKUw" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1028" class="wrap-collabsible"> <input id="collapsibleabstractC1028" class="toggle" type="checkbox"> <label for="collapsibleabstractC1028" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We propose a training system in virtual reality, VCoach, automatically generating interactive and personalized boxing training drills for individual trainees. The training drill is generated in real-time based on the trainee's updated performance, including the evaluation of punch speed, reaction time, and punch pose through wearable VR devices. The training drill is visualized as a sequence of target points on a virtual heavy bag and the corresponding punch motion, as well as the performance feedback. Our experiments show that VCoach can generate personalized training drills to help trainees improve skills efficiently.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1031">Control with Vergence Eye Movement in Augmented Reality See-Through Vision</h3>
    
<p> <small><strong style="color: black;"> Booth: B21 </strong></small> <br> </p>
    
    <p><i>Zhimin Wang</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=Eycv9iuvfwg" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1031" class="wrap-collabsible"> <input id="collapsibleabstractC1031" class="toggle" type="checkbox"> <label for="collapsibleabstractC1031" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Augmented Reality (AR) see-through vision enables the user to see through a wall and view the occluded objects. Most existing works only used common modalities to control the display for see-through vision, e.g., button clicking and speech. However, we use visual system to watch see-through vision. Using an addition interaction channel will distract the user and degrade the user experience. In this paper, we propose a novel interaction method using vergence eye movement for controlling see-through vision in AR. Specifically, we first customize eye cameras and design gaze depth estimation method for Microsoft HoloLens 2. With our algorithm, fixation depth can be computed from the vergence, and used to manage the see-through vision.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1033">Semi-Analytical Surface Tension Model for Free Surface Flows</h3>
    
<p> <small><strong style="color: black;"> Booth: B22 </strong></small> <br> </p>
    
    <p><i>Nurshat Menglik</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/AeY0b5mlLtk" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1033" class="wrap-collabsible"> <input id="collapsibleabstractC1033" class="toggle" type="checkbox"> <label for="collapsibleabstractC1033" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In this paper, a semi-analytical surface tension model is proposed for smoothed particle hydrodynamics (SPH). Different from previous approaches, cohesive and adhesive forces in our model are unified within a surface energy framework for nonuniform systems. To calculate the adhesive force, we use a semi-analytical solution to convert the volume integral into a surface integral, triangular meshes which represent solid boundaries can be directly introduced into liquid-solid interactions. A gradient descent algorithm is employed to optimize the objective function, which represents the total energy of the fluid. Experiments show that our model can efficiently handle complex solid boundaries with surface-tension-driven phenomena.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1034">Deformable torso anatomy education with three-dimensional autostereoscopic visualization and free-hand interaction</h3>
    
<p> <small><strong style="color: black;"> Booth: B28 </strong></small> <br> </p>
    
    <p><i>Nan Zhang</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/zgVVK7eBjh4" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1034" class="wrap-collabsible"> <input id="collapsibleabstractC1034" class="toggle" type="checkbox"> <label for="collapsibleabstractC1034" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Virtual reality/augmented reality has advantages in immersive learning and conveying meta-information in anatomy education. In this study, we present an interactive naked-eye 3D torso anatomy education environment based on population anatomical information. In particular, we utilize the deformable anatomy models, constructed from big data of healthy adults, to convey the anatomy knowledge of truthfulness organ shapes and population anatomical variances. In addition, the proposed anatomy education system is conveyed by free-hand 3D autostereoscopic visualization, which supports multi-user and glass-free. A user study with fourteen students was implemented and demonstrates that the system is appropriate and useful for torso anatomy education.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1036">Absence Agents: Mitigating Interruptions in Extended Reality Remote Collaboration</h3>
    
<p> <small><strong style="color: black;"> Booth: B13 </strong></small> <br> </p>
    
    <p><i>Huyen Nguyen</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/m80kWoJKXdg" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1036" class="wrap-collabsible"> <input id="collapsibleabstractC1036" class="toggle" type="checkbox"> <label for="collapsibleabstractC1036" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Although dealing with interruptions in remote collaboration has been studied in general, few works have done this for Extended Reality (XR) collaboration. With the current explosion of interest in XR collaboration, we explore the negative impacts of interruptions in synchronous distributed XR environments and propose a novel concept for dealing with them: Absence Agents. We present their requirements analysis, design, and a prototype implementation. We believe that our concept and design of Absence Agents are important for practitioners and researchers alike, as they highlight avenues for future research.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1043">Group WiM: A Group Navigation Technique for Collaborative Virtual Reality Environments</h3>
    
<p> <small><strong style="color: black;"> Booth: B12 </strong></small> <br> </p>
    
    <p><i>Vuthea Chheang</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/ZLbcZU4sQ7Y" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1043" class="wrap-collabsible"> <input id="collapsibleabstractC1043" class="toggle" type="checkbox"> <label for="collapsibleabstractC1043" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In this work, we present a group World-in-Miniature (WiM) navigation technique that allows a guide to navigate a team in collaborative virtual reality (VR) environments. We evaluated the usability, discomfort, and user performance of the technique compared to state-of-the-art group teleportation in a user study. The results show that the proposed technique induces less discomfort for the guide and has slight usability advantages. The group WiM technique seems superior regarding task completion time for obstructed target destination. The group WiM technique provides potential benefits for effective group navigation in complex virtual environments and harder-to-reach target locations.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1066">CV-Mora Based Lip Sync Facial Animations for Japanese Speech</h3>
    
<p> <small><strong style="color: black;"> Booth: B11 </strong></small> <br> </p>
    
    <p><i>Ryoto Kato</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/HB52DVsXrkg" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1066" class="wrap-collabsible"> <input id="collapsibleabstractC1066" class="toggle" type="checkbox"> <label for="collapsibleabstractC1066" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>To generate authentic real-time facial animations using face mesh data, which corresponds to fifty-six consonant and vowel (CV) types of morae that form the basis of Japanese speech, we propose a new method. Our method produces facial expressions by the weighted addition of fifty-three face meshes based on the real-time mapping of voice streaming to registered morae. In the user study, results showed that facial expressions produced during Japanese speech were more natural using our method than those using popular methods to generate real-time English-based Oculus lip sync and volume intensity-based facial animations.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1072">Impact of Parameter Disentanglement on Collaborative Alignment</h3>
    
<p> <small><strong style="color: black;"> Booth: B11 </strong></small> <br> </p>
    
    <p><i>Tianyu Song</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/eulUFXdOrD0" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1072" class="wrap-collabsible"> <input id="collapsibleabstractC1072" class="toggle" type="checkbox"> <label for="collapsibleabstractC1072" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>The interactive alignment of real and virtual content in AR is often non-trivial. Positional errors along the user's view direction frequently lead to the misjudgment of the object's depth. This work takes advantage of alternative users' viewpoints in collaborative settings to mitigate these errors. Furthermore, we systematically restrict the parameters used to control the virtual content's pose and investigate the impact of sharing and disentangling such parameters. Results from this work show that alignment schemes that disentangle the control parameters improve overall alignment accuracy with a similar workload for the users and no significant increase in execution time.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1088">Gaze Capture based Considerate Behaviour Control of Virtual Guiding Agent</h3>
    
<p> <small><strong style="color: black;"> Booth: B12 </strong></small> <br> </p>
    
    <p><i>Hironori Mitake</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=h4-93eQURWM" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1088" class="wrap-collabsible"> <input id="collapsibleabstractC1088" class="toggle" type="checkbox"> <label for="collapsibleabstractC1088" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Agents in VR have wide application like guidance. Most current agents are passive, so that people should suspend their current tasks and request agents with explicit demand. It is necessary to make agent more actively open the interaction naturally but without being bothering.
We propose a virtual guidance agent which provide voice explanation in appropriate timing, using gaze tracking, attention amount estimation and attention driven state machine. We used time-decayed moving average of angle between gaze direction and face front direction.
We implemented the method in VR and evaluated effectiveness in virtual guiding tour experimentally.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1091">Perceptions of Colour Pickers and Companions in Virtual Reality Art-Making</h3>
    
<p> <small><strong style="color: black;"> Booth: B13 </strong></small> <br> </p>
    
    <p><i>Marylyn Alex</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/RPcqwawFn60" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1091" class="wrap-collabsible"> <input id="collapsibleabstractC1091" class="toggle" type="checkbox"> <label for="collapsibleabstractC1091" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Virtual reality art is reshaping digital art experiences but may elicit different first impressions across disparate age groups. We investigate first impressions of VR colour pickers and the impact of a virtual companion via an online survey with 63 adults and 24 older adults. The colour pickers differed significantly in perceived hedonic qualities. We found no statistical differences between perceptions of adults and older adults. The virtual companion had no significant effect on participants' overall experiences. However, we found statistical trends where older adults rated the virtual companion higher in terms of companionship and making VR art more engaging.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1094">Light VR Client for Point Cloud Navigation with 360&deg; Images</h3>
    
<p> <small><strong style="color: black;"> Booth: B21 </strong></small> <br> </p>
    
    <p><i>Cl&eacute;ment Dluzniewski</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=DCQTostPJV0" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1094" class="wrap-collabsible"> <input id="collapsibleabstractC1094" class="toggle" type="checkbox"> <label for="collapsibleabstractC1094" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Since point clouds require a large amount of data to be visually pleasing, they tend to be voluminous. Hence, hardware with limited computational and memory capabilities may not be able to handle such large data structures. Here, we propose a light VR client to explore a static point cloud, stored in a remote server, through 360&deg; images. The client visualizes in an HMD the omnidirectional rendering of the point cloud and moves to another position with a teleportation metaphor. The main advantage of our proposition is the ability to work on modest hardware without a continuous high bandwidth.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1099">Vibrating tilt platform enhancing immersive experience in VR</h3>
    
<p> <small><strong style="color: black;"> Booth: B22 </strong></small> <br> </p>
    
    <p><i>Dorota Kami&nacute;ska</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=JMPiNTJlYBY" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1099" class="wrap-collabsible"> <input id="collapsibleabstractC1099" class="toggle" type="checkbox"> <label for="collapsibleabstractC1099" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Virtual reality systems, since their initial development, suffered some disadvantages. One of them was the fact that they had only visual interfaces. This limitation, however, has been successfully overcome with the development of haptic technology. Peripheral solutions reinforcing and enriching VR experience are now commonplace, and many haptic systems are being developed for deepening VR immersion. This paper discusses a new peripheral solution - a vibrating tilt platform with three angles of inclination to be used for enhancing the VR experience.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1103">Augmenting VR Ski Training using Time Distortion</h3>
    
<p> <small><strong style="color: black;"> Booth: B14 </strong></small> <br> </p>
    
    <p><i>Takashi Matsumoto</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/rUNxRoenp7o" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1103" class="wrap-collabsible"> <input id="collapsibleabstractC1103" class="toggle" type="checkbox"> <label for="collapsibleabstractC1103" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Virtual reality-based sports simulators are widely developed, which makes training in a virtual environment possible. On the other hand, methods using temporal features are also introduced to realize an adaptive training. In this paper, we study the effect of time distortion on alpine ski training to find out how modifying the temporal space can affect sports training. Experiments are conducted to investigate how a fast/slow and a static/dynamic time distortion-based training, respectively, can impact the performance of users.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1106">FusedAR: Adaptive Environment Lighting Reconstruction for Visually Coherent Mobile AR Rendering</h3>
    
<p> <small><strong style="color: black;"> Booth: B27 </strong></small> <br> </p>
    
    <p><i>Yiqin Zhao</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/WJNtbfDRaZE" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1106" class="wrap-collabsible"> <input id="collapsibleabstractC1106" class="toggle" type="checkbox"> <label for="collapsibleabstractC1106" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Obtaining accurate omnidirectional environment lighting for high quality rendering in mobile augmented reality is challenging due to the practical limitation of mobile devices and the inherent spatial variance of lighting. In this paper, we present a novel adaptive environment lighting reconstruction method called FusedAR, which is designed from the outset to consider mobile characteristics, e.g., by exploiting mobile user natural behaviors of pointing the camera sensors perpendicular to the observation-rendering direction. Our initial evaluation shows that FusedAR achieves better rendering effects compared to using a recent deep learning-based AR lighting estimation system and environment lighting captured by 360&deg; cameras.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1113">Enabling Virtual Reality Interactions in Confined Spaces by Re-Associating Finger Motions</h3>
    
<p> <small><strong style="color: black;"> Booth: B28 </strong></small> <br> </p>
    
    <p><i>Wen-Jie Tseng</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/l2ugJVlk45w" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1113" class="wrap-collabsible"> <input id="collapsibleabstractC1113" class="toggle" type="checkbox"> <label for="collapsibleabstractC1113" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>As Virtual Reality (VR) headsets become mobile, people can interact in public places with applications often requiring large arm movements. However, using these open gestures is often uncomfortable and sometimes impossible in confined and public spaces (e.g., commuting in a vehicle). We introduce the concept of finger mapping, re-associating small-scale finger motions onto virtual arms in a larger VR space. Finger mapping supports various interactions (e.g., arms swinging movement, selection, manipulation, and locomotion) when the environment is constrained and does not allow large gestures. Finally, we discuss the opportunities and challenges of using finger mapping for VR interactions.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1125">Understanding Shoulder Surfer Behavior Using Virtual Reality</h3>
    
<p> <small><strong style="color: black;"> Booth: B27 </strong></small> <br> </p>
    
    <p><i>Yasmeen Abdrabou</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/HHgy8bDRPXc" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1125" class="wrap-collabsible"> <input id="collapsibleabstractC1125" class="toggle" type="checkbox"> <label for="collapsibleabstractC1125" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We explore how attackers behave during shoulder surfing. Such behavior is difficult to study as it is often opportunistic and can occur wherever potential attackers can observe other people's private screens. Therefore, we investigate shoulder surfing using virtual reality (VR). We recruited 24 participants and observed their behavior in two virtual waiting scenarios: at a bus stop and in an open office space. In both scenarios, avatars interacted with private screens displaying different content, thus providing opportunities for shoulder surfing. From the results, we derive an understanding of factors influencing shoulder surfing behavior.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1127">Designing a Physiological Loop for the Adaptation of Virtual Human Characters in a Social VR Scenario</h3>
    
<p> <small><strong style="color: black;"> Booth: B26 </strong></small> <br> </p>
    
    <p><i>Francesco Chiossi</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=OdQeaU5NvTM" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1127" class="wrap-collabsible"> <input id="collapsibleabstractC1127" class="toggle" type="checkbox"> <label for="collapsibleabstractC1127" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Social virtual reality is getting mainstream not only for entertainment purposes but also for productivity and education. This makes the design of social VR scenarios functional to support the operator's performance. We present a physiologically-adaptive system that optimizes for visual complexity in a dual-task scenario based on electrodermal activity. Specifically, we propose a system that adapts the amount of non-player characters while jointly performing an N-Back task (primary) and visual detection task (secondary). Our preliminary results show that when optimizing the complexity of the secondary task, users report an improved user experience.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1144">GestureExplorer: Immersive Visualisation and Exploration of Gesture Data</h3>
    
<p> <small><strong style="color: black;"> Booth: B25 </strong></small> <br> </p>
    
    <p><i>Ang Li</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/_MynTaap_g8" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1144" class="wrap-collabsible"> <input id="collapsibleabstractC1144" class="toggle" type="checkbox"> <label for="collapsibleabstractC1144" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This paper presents GestureExplorer, which features versatile immersive visualisations to grant the user free control over their perspective, allowing them to gain a better understanding of gestures. It provides multiple data visualisation views, and interactive features to support analysis and exploration of gesture datasets. A pair of iterative user studies provides initial feedback from several participants, including experts on immersive visualisation, and demonstrates the potential of GestureExplorer for providing a useful and engaging experience for exploring gesture data.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1158">Multi Touch Smartphone Based Progressive Refinement VR Selection</h3>
    
<p> <small><strong style="color: black;"> Booth: B34 </strong></small> <br> </p>
    
    <p><i>Elaheh Samimi</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=-NW0NGOMF28" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1158" class="wrap-collabsible"> <input id="collapsibleabstractC1158" class="toggle" type="checkbox"> <label for="collapsibleabstractC1158" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We developed a progressive refinement technique for VR object selection using a smartphone as a controller. Our technique combines progressive refinement with the marking menu-based CountMarks, which uses multi-finger touch gestures to &quot;short-circuit&quot; multi-item marking menus. Users can indicate a specific item in a sub-menu by pressing a specific number of fingers on the screen while swiping in the desired menu's direction. This reduces the number of steps in progressive refinement selection.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1164">Investigating Display Position of a Head-Fixed Augmented Reality Notification for Dual-task</h3>
    
<p> <small><strong style="color: black;"> Booth: B35 </strong></small> <br> </p>
    
    <p><i>Hyunjin Lee</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/ALXzQpHXWt8" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1164" class="wrap-collabsible"> <input id="collapsibleabstractC1164" class="toggle" type="checkbox"> <label for="collapsibleabstractC1164" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Providing additional information in the proper position of augmented reality (AR) head-mounted display (HMD) can help increase AR performance and usability for dual-task. Therefore, our study investigated how to place notifications for the dual-task to address this. We compared eight display positions and two tasks (single and dual tasks) to identify the appropriate area for displaying notifications. We confirmed that the middle-right reduces response time and task load. In contrast, the top-left is the location, which should avoid providing any notification in AR dual-task. Our study contributes to designing AR notifications on HMDs to enhance everyday AR experiences.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1175">Priority-Dependent Display of Notifications in the Peripheral Field of View of Smart Glasses</h3>
    
<p> <small><strong style="color: black;"> Booth: B26 </strong></small> <br> </p>
    
    <p><i>Anja Faulhaber</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/yqvPWwrtNm0" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1175" class="wrap-collabsible"> <input id="collapsibleabstractC1175" class="toggle" type="checkbox"> <label for="collapsibleabstractC1175" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We propose a concept for displaying notifications in the peripheral field of view of smart glasses aiming to achieve a balance between perception and distraction depending on the priority of the notification. We designed three different visualizations for notifications of low, medium, and high priority. To evaluate this concept, we conducted a study with 24 participants who reacted to the notifications while performing a primary task. Reaction times for the low-priority notification were significantly higher. The medium- and high-priority notifications did not show a clear difference.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1179">Studying the User Adaptability to Hyperbolic Spaces and Delay Time Scenarios</h3>
    
<p> <small><strong style="color: black;"> Booth: B25 </strong></small> <br> </p>
    
    <p><i>Ana Rita Rebelo</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/tPqrSp3VBCU" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1179" class="wrap-collabsible"> <input id="collapsibleabstractC1179" class="toggle" type="checkbox"> <label for="collapsibleabstractC1179" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>To create immersive virtual experiences, it is crucial to understand how users perceive Virtual Environments (VEs) and which interaction techniques are most appropriate for their tasks.
We created a tangible VE - the VR Lab - where it is possible to study space and time conditions to analyse the user's adaptability to different forms of interaction.
As a case study, we restricted the scope of the investigation to two morphing scenarios. The space morphing scenario compares the adaptability of users to Euclidean versus hyperbolic spaces. The time morphing scenario aims to establish from which values the visual delay affects performance.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1181">Augmented Reality Fitts' Law Input Comparison Between Touchpad, Pointing Gesture, and Raycast</h3>
    
<p> <small><strong style="color: black;"> Booth: B21 </strong></small> <br> </p>
    
    <p><i>Domenick Mifsud</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=c3rabmgy_Hc" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1181" class="wrap-collabsible"> <input id="collapsibleabstractC1181" class="toggle" type="checkbox"> <label for="collapsibleabstractC1181" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>With the goal of exploring the impact of transparency on selection in augmented reality (AR), we present a Fitts' law experiment with 18 participants, comparing three different input methods (finger based Pointing Gesture, controller using the Touchpad, and controller using Raycast), across 4 different target transparency levels (0&#37;, 30&#37;, 60&#37;, and 90&#37;) in an optical see-through AR head-mounted display. The results indicate that transparency has little effect on selection throughput and error rates. Overall, the Raycast input method performed significantly better than the pointing gesture and Touchpad inputs in terms of error rate and throughput in all opacity conditions.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1182">Predictive Power of Pupil Dynamics in a Team Based Virtual Reality Task</h3>
    
<p> <small><strong style="color: black;"> Booth: B32 </strong></small> <br> </p>
    
    <p><i>Yinuo Qin</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/56Jhy_oH-_o" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1182" class="wrap-collabsible"> <input id="collapsibleabstractC1182" class="toggle" type="checkbox"> <label for="collapsibleabstractC1182" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In this work, we describe a team-based VR task termed the Apollo Distributed Control Task (ADCT), where individuals, via the single independent degree-of-freedom control and limited environmental views, must work together to guide a virtual spacecraft back to Earth. Focusing on the analysis of pupil dynamics, which have been linked to a number of cognitive and physiological processes such as arousal, cognitive control, and working memory, we find that pupil diameter changes are predictive of multiple task-related dimensions, including the difficulty of the task, the role of the team member, and the type of communication.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1190">High-speed Gaze-oriented Projection by Cross-ratio-based Eye Tracking with Dual Infrared Imaging</h3>
    
<p> <small><strong style="color: black;"> Booth: C11 </strong></small> <br> </p>
    
    <p><i>Ayumi Matsumoto</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=0wTEgvkwA1I" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1190" class="wrap-collabsible"> <input id="collapsibleabstractC1190" class="toggle" type="checkbox"> <label for="collapsibleabstractC1190" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>While gaze-oriented projection can be high-resolution and wide-area display, conventional methods have difficulties in handling quick human eye movements. In this paper, we propose a high-speed gaze-oriented projection system using a synchronized high-speed tracking projector and cross-ratio-based eye tracking. The tracking projector with a high-speed projector and rotational mirrors enables temporal geometric consistency of the projection. The eye tracking uses high-speed cameras and infrared lightings of different wavelengths, and can achieve fast and almost calibration-free due to the cross-ratio algorithm. We have experimentally validated the eye tracking speed and accuracy, system latency, and demonstrated gaze-oriented projection.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1210">VR Wayfinding Training for People with Visual Impairment using VR Treadmill and VR Tracker</h3>
    
<p> <small><strong style="color: black;"> Booth: C12 </strong></small> <br> </p>
    
    <p><i>Sangsun Han</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/6ri75t3PAFA" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1210" class="wrap-collabsible"> <input id="collapsibleabstractC1210" class="toggle" type="checkbox"> <label for="collapsibleabstractC1210" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>There are virtual reality (VR) wayfinding training systems for people with visual impairment, but there is a lack of studies about how training environments can affect spatial information acquisition of people with visual impairment. Using a VR treadmill and a VR tracker, we studied how walk-in-place and actual walking can affect the acquisition of spatial information with regard to paths and obstacles. Our results show that people with visual impairment remember routes better when trained with VR treadmill, but they remember obstacles better when trained with VR tracker. We evaluate the respective efficacies of these approaches on spatial information memorization.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1218">&quot;What a Mess!&quot;': Traces of Use to Increase Asynchronous Social Presence in Shared Virtual Environments</h3>
    
<p> <small><strong style="color: black;"> Booth: B31 </strong></small> <br> </p>
    
    <p><i>Linda Hirsch</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/YTsGGwH8Fdo" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1218" class="wrap-collabsible"> <input id="collapsibleabstractC1218" class="toggle" type="checkbox"> <label for="collapsibleabstractC1218" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Shared virtual environments (VEs) are challenged conveying and triggering users' feelings of social presence. 
Traces of use are implicit evidence of prior interactions that support social awareness in the real environment (RE). However, they have not been explored in VEs so far.
We investigate the traces' effect on users' perception of asynchronous social presences in a within-subject study (N=26) by comparing the users' experience with and without traces. 
The traces significantly increased the feeling of social presence. We contribute an initial exploration of the \emph{traces of use} concept in VE to design shared social spaces for long-term use.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1231">Ebublio: Edge Assisted Multi-user 360-Degree Video Streaming</h3>
    
<p> <small><strong style="color: black;"> Booth: B33 </strong></small> <br> </p>
    
    <p><i>Yili Jin</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=sctsTxVfSCM" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1231" class="wrap-collabsible"> <input id="collapsibleabstractC1231" class="toggle" type="checkbox"> <label for="collapsibleabstractC1231" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Compared to traditional videos, streaming 360&deg; videos is more difficult. We propose Ebublio, a novel intelligent edge caching framework consisting of a collaborative FoV prediction (CFP) module and a long-term tile caching optimization (LTO) module. The former integrates the features of video content, user trajectory, and other users' records for combined prediction. The latter employs the Lyapunov framework and a subgradient optimization toward the optimal caching replacement policy. Our trace-driven evaluation further demonstrates the superiority of our framework, with about 42&#37; improvement in FoV prediction, and 36&#37; improvement in QoE under similar traffic consumption.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1250">Beyond Flicker, Beyond Blur: View-coherent Metameric Light Fields for Foveated Display</h3>
    
<p> <small><strong style="color: black;"> Booth: B32 </strong></small> <br> </p>
    
    <p><i>Prithvi Kohli</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=uCWhjDCPIWY" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1250" class="wrap-collabsible"> <input id="collapsibleabstractC1250" class="toggle" type="checkbox"> <label for="collapsibleabstractC1250" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Ventral metamers, pairs of images which may differ substantially in the periphery, but are perceptually identical, offer exciting new possibilities in foveated rendering and image compression, as well as offering insights into the human visual system. However, existing literature has mainly focused on creating metamers of static images. In this work, we develop a method for creating sequences of metameric frames, specifically light fields, with enforced consistency along the temporal, or angular, dimension. This greatly expands the potential applications for these metamers, and expanding metamers along the third dimension offers further new potential for compression.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1265">HoloInset: 3D Biomedical Image Data Exploration through Augmented Hologram Insets</h3>
    
<p> <small><strong style="color: black;"> Booth: C13 </strong></small> <br> </p>
    
    <p><i>JunYoung Choi</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=g1FrGxDQrAQ" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1265" class="wrap-collabsible"> <input id="collapsibleabstractC1265" class="toggle" type="checkbox"> <label for="collapsibleabstractC1265" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>The extended reality (XR) provides realistic depth perception and huge visualization spaces, which can serve as a powerful workspace for 3D data exploration and analysis. However, a direct adaptation of XR to conventional 3D data exploration tasks is less feasible due to several hardware limitations, such as low screen resolution, dizziness, narrow field of view, etc. In this paper, we propose a novel mixed reality visualization scheme, HoloInset, which combines a conventional visual analytics system and a virtual environment to effectively explore 3D biomedical image data. We also demonstrate the usability of the proposed visualization through a real-world analysis case.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1270">3Dify: Extruding Common 2D Charts with Timeseries Data</h3>
    
<p> <small><strong style="color: black;"> Booth: B34 </strong></small> <br> </p>
    
    <p><i>Richard Brath</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/JKz_XgJCs2E" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1270" class="wrap-collabsible"> <input id="collapsibleabstractC1270" class="toggle" type="checkbox"> <label for="collapsibleabstractC1270" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>3D charts are not common in financial services. We review chart use in practice. We create 3D financial visualizations starting with 2D charts used extensively in financial services, then extend into the third dimension with timeseries data. We embed the 2D view into the the 3D scene&#59; constrain interaction and add depth cues to facilitate comprehension. Usage and extensions indicate success.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1275">Virtual Reality Point Cloud Annotation</h3>
    
<p> <small><strong style="color: black;"> Booth: B23 </strong></small> <br> </p>
    
    <p><i>Anton Franzluebbers</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/3blPgeAk4p0" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1275" class="wrap-collabsible"> <input id="collapsibleabstractC1275" class="toggle" type="checkbox"> <label for="collapsibleabstractC1275" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This work presents an immersive headset-based virtual reality visualization and annotation system for point clouds, oriented towards application on laser scans of plants. The system can be used to paint regions or individual points with fine detail, even with large, dense point clouds. A non-immersive desktop interface was designed for comparison within the same application. A within-subjects user study (N=16) was conducted to compare these interfaces for annotation and counting tasks. Results showed a strong preference for the immersive virtual reality interface, likely as a result of perceived and actual significant differences in task performance.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1283">Design and Evaluation of an Augmented Reality Application for Learning Spatial Transformations and Their Mathematical Representations</h3>
    
<p> <small><strong style="color: black;"> Booth: B24 </strong></small> <br> </p>
    
    <p><i>Zohreh Shaghaghian</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/2_QNfjQKW7k" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1283" class="wrap-collabsible"> <input id="collapsibleabstractC1283" class="toggle" type="checkbox"> <label for="collapsibleabstractC1283" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>There is a close relation between spatial thinking and mathematical problem-solving. This paper presents a newly developed educational Augmented Reality (AR) mobile application, BRICKxAR/T, to help students intuitively learn spatial transformations and the related mathematics through play. A pilot study with 7 undergraduate students evaluates students learning gain through a mental rotation and a math test on transformation matrices. The results show most students performed better with a higher score after learning with the app. Students found the app interesting to play and useful for learning geometric transformations and matrices.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1284">KARLI: Kid-friendly Augmented Reality for Primary School Health Education</h3>
    
<p> <small><strong style="color: black;"> Booth: B33 </strong></small> <br> </p>
    
    <p><i>Mariella Seel</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/6JG7izsWXh8" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1284" class="wrap-collabsible"> <input id="collapsibleabstractC1284" class="toggle" type="checkbox"> <label for="collapsibleabstractC1284" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Acquiring health knowledge is essential and starts already in primary school. Augmented Reality (AR) helps to convey complex topics in a more understandable way. In this work, we present the prototype of KARLI, the &quot;Kid-friendly Augmented Reality Learning Interface&quot;. This AR smartphone app for in-school use is designed for age level 8 to 10, enabling pupils to explore a 3D model of the human body based on the primary school curriculum. Underlining the importance of kid-friendly app development and testing, our evaluation results of 38 pupils and 3 teachers indicate that KARLI is suitable and helpful for health education in primary schools.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1288">VR Edufication on Historic Lunar Roving Missions</h3>
    
<p> <small><strong style="color: black;"> Booth: D32 </strong></small> <br> </p>
    
    <p><i>Huadong Zhang</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/nWQYRksCMUw" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1288" class="wrap-collabsible"> <input id="collapsibleabstractC1288" class="toggle" type="checkbox"> <label for="collapsibleabstractC1288" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This work presents the design and evaluation of an educational VR game to teach historic Apollo lunar roving missions. The game consists of three gameplay scenes, and each scene aims to convey one type of knowledge. The game adheres with the learning objectives, presents the contents, and conveys knowledge in both active and passive interaction modes. We conducted a user study focusing on the understanding of the influence of different interaction modes on learning outcomes and learning engagement.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1299">The Immediate and Retained Effectiveness of One-time Virtual Reality Exposure in Enhancing Intercultural Sensitivity</h3>
    
<p> <small><strong style="color: black;"> Booth: D31 </strong></small> <br> </p>
    
    <p><i>Dr Richard Chen Li</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/iGc2KpLlVqg" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1299" class="wrap-collabsible"> <input id="collapsibleabstractC1299" class="toggle" type="checkbox"> <label for="collapsibleabstractC1299" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This study aims to investigate the immediate and retained effects of one-time virtual reality (VR) exposure on intercultural sensitivity (IS) and identify the contributing factors. Three virtual scenarios about the ethnic minorities in Hong Kong were created for the empirical study. The longitudinal results involving 30 participants (15M 15F) showed that both the immediate and retained effects of the one-time VR exposure on IS were significant. Moreover, linear growth curve models suggested that among the female participants, presence and emotional empathy were closely associated with the change of IS over time, but this relation was not significant among the males.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1306">Comparing principally imagination and interaction versions of a play anywhere mobile AR location-based story</h3>
    
<p> <small><strong style="color: black;"> Booth: B34 </strong></small> <br> </p>
    
    <p><i>Gideon Raeburn</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/TPuO4tyOOv8" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1306" class="wrap-collabsible"> <input id="collapsibleabstractC1306" class="toggle" type="checkbox"> <label for="collapsibleabstractC1306" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Augmented Reality (AR) allows virtual elements to be overlaid on the real world, providing new opportunities for location-based storytelling, by offering blended environments to more closely resemble those in a story. However, there is limited research considering how different interaction opportunities with such augmented surroundings, affect user engagement and immersion in such a story. A mobile AR app, Map Story 2, was developed to investigate this, offering a guided story-walk around a user's chosen location, either interacting with overlaid virtual objects to progress the story, or being asked to imagine the same events playing out at each augmented location.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1312">Redirected Placement: Retargeting Destinations of Passive Props for Enhancing Bimanual Haptic Feedback in Virtual Reality</h3>
    
<p> <small><strong style="color: black;"> Booth: C11 </strong></small> <br> </p>
    
    <p><i>Xuanhui Yang</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/Hn7_ubi6B38" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1312" class="wrap-collabsible"> <input id="collapsibleabstractC1312" class="toggle" type="checkbox"> <label for="collapsibleabstractC1312" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Haptic retargeting is a commonly used technique to match passive props to virtual objects and add tactile feedback in Virtual Reality (VR). However, researchers have mainly focused on single-hand retargeting and applied these techniques primarily for tasks of touching objects. In this work, we propose a novel retargeting solution for tasks of grabbing and placing objects in VR, called redirected placement (RP), which is applied when placing virtual objects. The key idea of this method is that when the user places the virtual object, it can enable the user to place the physical prop in the user's hand in a position that is easier to match with multiple other virtual objects, without being detected by the user.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1323">Design of Mentally and Physically Demanding Tasks as Distractors of Rotation Gains</h3>
    
<p> <small><strong style="color: black;"> Booth: B23 </strong></small> <br> </p>
    
    <p><i>Eike Langbehn</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/DZ--H0NIzlQ" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1323" class="wrap-collabsible"> <input id="collapsibleabstractC1323" class="toggle" type="checkbox"> <label for="collapsibleabstractC1323" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Rotation gains decouple real and virtual head turning. When reaching a boundary of the tracking space, the user's orientation can be reset by applying a rotation gain and forcing the user to do a certain task that requires head rotations. To identify what kind of tasks are best suited to mask the redirection, four tasks were designed that differentiate in their amount of mentally and physically demand: a spatial memory task, a verbal memory task, a physically exhausting task and a task which requires physical skill. A first pilot study was conducted to evaluate the influence on the redirection awareness.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1325">Minimaps for Impossible Spaces: Improving Spatial Cognition in Self-Overlapping Virtual Rooms</h3>
    
<p> <small><strong style="color: black;"> Booth: B24 </strong></small> <br> </p>
    
    <p><i>Eike Langbehn</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/ulTp3mtP230" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1325" class="wrap-collabsible"> <input id="collapsibleabstractC1325" class="toggle" type="checkbox"> <label for="collapsibleabstractC1325" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Natural walking in virtual reality is constrained by the physical boundaries of the tracking space.
Impossible spaces enlarge the virtual environment by creating overlapping architecture and letting multiple locations occupy the same physical space.
Minimaps, which are small representations of the environment, are a common method to assist with wayfinding and navigation.
Unfortunately, in a naive implementation of such minimaps for an environment with impossible spaces, the overlap would be obvious.
We investigated approaches to displaying impossible spaces on minimaps without attracting users' attention to the overlapping parts.
We conducted a study that investigated effects of minimaps on spatial cognition.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1340">Moving Visual-Inertial Ordometry into Cross-platform Web for Markerless Augmented Reality</h3>
    
<p> <small><strong style="color: black;"> Booth: C12 </strong></small> <br> </p>
    
    <p><i>Yakun Huang</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/cCrxg97h3ZQ" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1340" class="wrap-collabsible"> <input id="collapsibleabstractC1340" class="toggle" type="checkbox"> <label for="collapsibleabstractC1340" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Widespread mobile AR implementation still suffers from the unique adaption for different mobile platforms. Enabling AR experience on the cross-platform web is a potential alternative and provides unified implementation. We reveal a lightweight VIO implementation on the web without any visual marker or cumbersome frameworks for AR services. The contributions include 1) designing a novel VIO architecture to balance the user experience and the efficiency&#59; 2) optimizing a single-thread VIO algorithm to avoid the sophisticated multi-threading management and enhancing compatibility&#59; 3) improving the dirty noise of IMU and developing efficient numerical libraries like SuiteSparse for the massive computation afford.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1356">Augmenting Sculpture with Immersive Sonification</h3>
    
<p> <small><strong style="color: black;"> Booth: C14 </strong></small> <br> </p>
    
    <p><i>Yichen Wang</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/MlBKWVlkEDc" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1356" class="wrap-collabsible"> <input id="collapsibleabstractC1356" class="toggle" type="checkbox"> <label for="collapsibleabstractC1356" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We present an artistic Mixed Reality (MR) system that remixes a sculptural element of a building and its aesthetic context to provide an on-site augmented art experience. Mainstream MR systems, particularly art-related, focus on the use of visuals in presenting additional information, whereas the use of audio as the main information channel has rarely been considered. In this work, we explore two different versions of a sonic experience for walking through an artistic staircase to enhance its public's engagement. Our user evaluation reveals the effectiveness of sonic design for a rewarding MR experience. With this, we emphasise the importance of sonic design in MR applications.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1357">Automatic Virtual Portals Placement for Efficient VR Navigation</h3>
    
<p> <small><strong style="color: black;"> Booth: C13 </strong></small> <br> </p>
    
    <p><i>Yi Liu</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/muuu9BtwzEA" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1357" class="wrap-collabsible"> <input id="collapsibleabstractC1357" class="toggle" type="checkbox"> <label for="collapsibleabstractC1357" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Portals placement in a large virtual scene can help users improve navigation efficiency, but determining the number and the positions of the portals has some challenges. In this paper, we proposed two automatic virtual portals placement methods for efficient VR navigation. To reduce the number of reverse redirections, we also proposed a real-time portal orientation determination algorithm. For any given single-floor outdoor virtual scene, our methods can automatically place the portals.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1370">STARE: Semantic Augmented Reality Decision Support in Smart Environments</h3>
    
<p> <small><strong style="color: black;"> Booth: E24 </strong></small> <br> </p>
    
    <p><i>Mengya Zheng</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/siY94Q4jFVw" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1370" class="wrap-collabsible"> <input id="collapsibleabstractC1370" class="toggle" type="checkbox"> <label for="collapsibleabstractC1370" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>The Internet of Things (IoT) facilitates real-time decision support within smart environments. Augmented Reality (AR) allows for the ubiquitous visualization of IoT-derived data, and AR visualization will simultaneously permit the cognitive and visual binding of information to the physical object(s) to which they pertain. Essential questions exist about efficiently filtering, prioritizing, determining relevance, and adjudicating individual information needs in real-time decision-making. To this end, this paper proposes a novel AR decision support framework (STARE) to support immediate decisions within a smart environment by augmenting the user's focal objects with assemblies of semantically relevant IoT data and corresponding suggestions.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1372">Material Reflectance Property Estimation of Complex Objects Using an Attention Network</h3>
    
<p> <small><strong style="color: black;"> Booth: C14 </strong></small> <br> </p>
    
    <p><i>Bin Cheng</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=NEonIkcMYkI" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1372" class="wrap-collabsible"> <input id="collapsibleabstractC1372" class="toggle" type="checkbox"> <label for="collapsibleabstractC1372" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Material reflectance property modeling can be used in realistic rendering to generate realistic appearances for virtual objects. However, current works mainly focus on near plane objects. In this paper, we propose an end-to-end network framework with attention mechanism to estimate the reflectance properties of any 3D object surface from a single image, where a single attention module is used for each reflectance property respectively to learn the property specific features. We also generate a material dataset by rendering a set of 3D complex shape models. The dataset is suitable for reflectance property estimation of arbitrary complex shape objects. Experiments validate the proposed method.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1377">Emotional Support Companions in Virtual Reality</h3>
    
<p> <small><strong style="color: black;"> Booth: E26 </strong></small> <br> </p>
    
    <p><i>Linda Graf</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/VDEfJL2oqxc" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1377" class="wrap-collabsible"> <input id="collapsibleabstractC1377" class="toggle" type="checkbox"> <label for="collapsibleabstractC1377" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>According to social psychological models, the presence of another person or even a virtual character in stressful situations can have stress-reducing effects. Thereby, the outcome can depend on the congruency between one's mood and the perceived mood of the other person. This dependence guides the design of VR applications for stress reduction, which use different virtual companion designs depending on the emotional state of individual users. This paper describes an ongoing design and development process towards such an emotionally supportive companion and shares initial results concerning the perception and stress-reducing effects of positively- and negatively-minded companions.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1385">Heuristic Short-term Path Prediction for Spontaneous Human Locomotionin Virtual Open Spaces</h3>
    
<p> <small><strong style="color: black;"> Booth: C11 </strong></small> <br> </p>
    
    <p><i>Christian Hirt</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/2cF50ABMYlI" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1385" class="wrap-collabsible"> <input id="collapsibleabstractC1385" class="toggle" type="checkbox"> <label for="collapsibleabstractC1385" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Redirected Walking (RDW) shrinks large virtual environments to fit small physical tracking spaces while supporting natural locomotion. In predictive RDW, algorithms rely on predicting users' paths to adjust the induced redirection. Current predictors assume drastic simplifications or build on complex locomotion models. Further, also adapting existing predictive RDW algorithms to unconstrained open spaces exponentially increases their computational complexity. In this paper, we propose simple yet flexible path prediction models supporting dynamic virtual open spaces. Our proposed prediction models consist of a drop and a sector shape, defining an area, in which linear and clothoidic walking trajectories will be investigated.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1398">3D Scene Reconstruction from RGB Images Using Dynamic Graph Convolution for Augmented Reality</h3>
    
<p> <small><strong style="color: black;"> Booth: C21 </strong></small> <br> </p>
    
    <p><i>Robin Fischer</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/ZNLnmNjCmVg" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1398" class="wrap-collabsible"> <input id="collapsibleabstractC1398" class="toggle" type="checkbox"> <label for="collapsibleabstractC1398" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>The 3D scene reconstruction task aims to reconstruct the object
shape, object pose, and the 3D layout of the scene. In the field of
augmented reality, this information is required for interactions with
the surroundings. In this paper, we develop a holistic end-to-end
scene reconstruction system using only RGB images. We further
designed an architecture that can adapt to different types of objects
through our graph convolution network during object surface generation. Moreover, a scene-merging strategy is proposed to alleviate the
occlusion problem by merging different views continuously. This
also allows our system to reconstruct the complete surroundings in a
room.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1402">Towards Eye-Perspective Rendering for Optical See-Through Head-Mounted Displays</h3>
    
<p> <small><strong style="color: black;"> Booth: C12 </strong></small> <br> </p>
    
    <p><i>Gerlinde Emsenhuber</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/dADsQIE64n4" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1402" class="wrap-collabsible"> <input id="collapsibleabstractC1402" class="toggle" type="checkbox"> <label for="collapsibleabstractC1402" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Optical see-through (OST) HMDs are a typical platform for Augmented Reality and allows users to experience augmentations. Utilizing information of the real-world background, visualization algorithms adapt the layout and representation of content to improve legibility. Typically, background information is captured via built-in HMD cameras. However, HMD camera views of the real-world scene are distinctively different to the user's view through the OST HMD. We propose eye-perspective rendering as a solution to synthesize high fidelity renderings of the user's view for OST HMDs to enable adaptation algorithms to utilize visual information as seen from the user's perspective to improve placement, rendering and, thus, legibility of content.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1405">Feasibility of Training Elite Athletes for Improving their Mental Imagery Ability Using Virtual Reality</h3>
    
<p> <small><strong style="color: black;"> Booth: C21 </strong></small> <br> </p>
    
    <p><i>Yuanjie Wu</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=dfpbJw3mVFY" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1405" class="wrap-collabsible"> <input id="collapsibleabstractC1405" class="toggle" type="checkbox"> <label for="collapsibleabstractC1405" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>The goal of imagery training for athletes is to create realistic images in their minds and to familiarize them with certain procedures, environments, and other aspects related to competition. Traditional imagery training methods use still images or videos, and athletes study the pictures or watch the videos in order to mentally rehearse. However, factors such as distractions and low realism can affect the training quality. In this paper, we present a VR solution and a study that explores our hypotheses that 1) high-fidelity VR systems improve mental imagery skills, and that 2) the presence of elements such as an audience or photographers in the VR environment result in better mental imagery skill improvement.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1411">Designing a Mixed Reality System for Exploring Genetic Mutation Data of Cancer Patients</h3>
    
<p> <small><strong style="color: black;"> Booth: C22 </strong></small> <br> </p>
    
    <p><i>Aniqa Imtiaz</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/5OQhYcxcuE4" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1411" class="wrap-collabsible"> <input id="collapsibleabstractC1411" class="toggle" type="checkbox"> <label for="collapsibleabstractC1411" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Increased availability of cancerous genomics mutation data provides researchers the opportunity to discover associations among genetic mutation patterns within the same organ as well as similar mutation patterns among different organs. However, the complexity, variety, and scale of multi-dimensional data involved in analyzing mutations across organs poses challenges for clinicians and researchers to draw such relationships. We present a prototype application that leverages multiple-coordinated views in mixed reality (MR) to enable investigations of genetic mutation patterns and the organs affected by cancer. We believe our prototype has the potential to enhance data and association discovery within and across different organs.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1421">A Pinch-based Text Entry Method for Head-mounted Displays</h3>
    
<p> <small><strong style="color: black;"> Booth: C23 </strong></small> <br> </p>
    
    <p><i>Haiyan Jiang</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/RO1-IWaqLrw" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1421" class="wrap-collabsible"> <input id="collapsibleabstractC1421" class="toggle" type="checkbox"> <label for="collapsibleabstractC1421" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Pinch gestures have been used for text entry in Head-mounted displays (HMDs), enabling a comfortable and eyes-free text entry. However, the number of pinch gestures is limited, making it difficult to input all characters. In addition, the common pinch-based methods with a QWERTY keyboard require accurate control of the hand position and angle, which could be affected by natural tremors and the Heisenberg effect. So, we propose a new text entry method for HMDs, which combines hand positions and pinch gestures with a condensed key-based keyboard, enabling one-handed text entry for HMDs. The results of a primary study show that the mean input speed of the proposed method is 7.60 words-per-minute (WPM).</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1431">Analysis of Emotional Tendency and Syntactic Properties for VR Game Reviews</h3>
    
<p> <small><strong style="color: black;"> Booth: C24 </strong></small> <br> </p>
    
    <p><i>Anqi CHEN</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/ga4S24gIu7Y" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1431" class="wrap-collabsible"> <input id="collapsibleabstractC1431" class="toggle" type="checkbox"> <label for="collapsibleabstractC1431" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>The studies of player reviews can help game developers design and optimize VR games. To this end, we investigated 288,685 reviews from 506 VR games on the Steam platform to analyze their sentiment tendencies using the machine learning-based model SKEP, which finds that although some of the reviews are &quot;recommend&quot;, they actually have opposite emotional tendencies. We also study the syntactic properties based on the natural language processing (NLP) kits Stanza and NLTK library, and we find that cybersickness is a significant concern for players.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1438">Emotional Avatars: Facial Emotion Identification Methodology for Avatar Based Systems</h3>
    
<p> <small><strong style="color: black;"> Booth: C22 </strong></small> <br> </p>
    
    <p><i>Dilshani Kumarapeli</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/sEhP59USpWk" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1438" class="wrap-collabsible"> <input id="collapsibleabstractC1438" class="toggle" type="checkbox"> <label for="collapsibleabstractC1438" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This work analyses the effect of uncanniness behaviour in identifying emotions from different humanoid avatar representations. Expressive avatars play a vital role in immersive environments. However, technical limitations in replicating subtle emotional cues using real-time expression conversion techniques create an uncanniness to the viewers. Hence, achieving the desired emotional awareness is arguable. Therefore, using an avatar representation resistant to uncanniness in systems sensitive to emotional changes is vital. Therefore, here we analyse the level of uncanniness noticed by people for different avatars exhibiting various emotions using expressive faces and the behavioural trends of people in catching emotion-related uncanniness.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1440">Role of Dynamic Affordance and Cognitive Load in the Design of Extended Reality based Simulation Environments for Surgical Contexts</h3>
    
<p> <small><strong style="color: black;"> Booth: C28 </strong></small> <br> </p>
    
    <p><i>Avinash Gupta</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/cyf-Gb-D2Sk" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1440" class="wrap-collabsible"> <input id="collapsibleabstractC1440" class="toggle" type="checkbox"> <label for="collapsibleabstractC1440" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In this paper, HCI-based design criteria for the Extended Reality (XR) based training environments. The design criteria explored in the paper help lay a foundation for the creation of Human Centric XR environments to train users in an orthopedic surgical procedure. The HCI-based perspective presented in the paper investigates the criteria such as affordance and cognitive load during the design. The paper focuses on the design of XR based environments based on the participatory design approach and information-centric modeling. Testing and assessment strategy presented provide insights into the impact of such HCI-based criteria on participants' acquisition of skills and knowledge during interactions with the XR environments.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1445">MienCap: Performance-Based Facial Animation with Live Mood Dynamics</h3>
    
<p> <small><strong style="color: black;"> Booth: C31 </strong></small> <br> </p>
    
    <p><i>Ye Pan</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=cUkbgL-4mwo" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1445" class="wrap-collabsible"> <input id="collapsibleabstractC1445" class="toggle" type="checkbox"> <label for="collapsibleabstractC1445" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Our purpose is to improve performance-based animation which can drive believable 3D stylized characters that are truly perceptual. By combining traditional blendshape animation techniques with machine learning models, we present a real time motion capture system, called MienCap, which drive character expressions in a geometrically consistent and perceptually valid way. We demonstrate the effectiveness of our system by comparing to a commercial product Faceware. Results reveal that ratings of the recognition of expressions depicted for animated characters via our systems are statistically higher than Faceware. Our results provide animators with a system for creating the expressions they wish to use more quickly and accurately.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1448">Preliminary analysis of effective assistance timing for iterative visual search tasks using gaze-based visual cognition estimation</h3>
    
<p> <small><strong style="color: black;"> Booth: C38 </strong></small> <br> </p>
    
    <p><i>Hirotake Yamazoe</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/dDR4rG3Ns8c" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1448" class="wrap-collabsible"> <input id="collapsibleabstractC1448" class="toggle" type="checkbox"> <label for="collapsibleabstractC1448" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In this paper, focusing on whether a person has visually recognized a target (visual cognition, VC) in iterative visual-search tasks, we propose an efficient assistance method based on the VC. In the proposed method, we first estimate the participant's VC of the target in the previous task. We then determine the target for the next task based on the VC and start to guide the participant's attention to the target for the next task at the VC timing. By initiating the guidance from the timing of the previous target's VC, we can guide attention at an earlier time and achieve efficient attention guidance. The preliminary experimental results showed that VC-based assistance improves task performance.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1449">Prototyping a Virtual Agent for Pre-school English Teaching</h3>
    
<p> <small><strong style="color: black;"> Booth: C23 </strong></small> <br> </p>
    
    <p><i>Eduardo Benitez Sandoval</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/iPKHRcGrQOs" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1449" class="wrap-collabsible"> <input id="collapsibleabstractC1449" class="toggle" type="checkbox"> <label for="collapsibleabstractC1449" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This paper describes a case study and the insights gained from prototyping an Intelligent Virtual Agent (IVA) for English vocabulary building for Spanish-speaking preschool children. After an initial exploration to evaluate the feasibility of developing an IVA, we followed a Human-Centered Design (HCD) approach to create a prototype. We report on the multidisciplinary process used that incorporated two well-known educative concepts: gamification and story-telling as the main components for engagement. Our results suggest that a multidisciplinary approach to developing an educational IVA is effective. We report on the relevant aspects of the ideation and design processes that informed the vision and mission of the project.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1459">Towards Conducting Effective Locomotion Through Hardware Transformation in Head-Mounted-Device - A Review Study</h3>
    
<p> <small><strong style="color: black;"> Booth: C37 </strong></small> <br> </p>
    
    <p><i>Pawankumar Gururaj Yendigeri</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/qFwJ9k9E5OA" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1459" class="wrap-collabsible"> <input id="collapsibleabstractC1459" class="toggle" type="checkbox"> <label for="collapsibleabstractC1459" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Locomotion in Virtual Reality (VR) acts as a motion tracking unit for simulating user movements based on the Degree-of-Freedom (DOF) of the application. For effective locomotion, VR practitioners may have to transform their hardware from 3-DOF to 6-DOF. In this context, we conducted a literature review on different motion tracking methods employed in the Head-Mounted-Devices (HMD) to understand such hardware transformation to conduct locomotion in VR. Our observations led us to formulate a taxonomy of the tracking methods for locomotion in VR based on system design. Our study also captures different metrics that VR practitioners use to evaluate the hardware based on the context, performance, and significance for conducting locomotion.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1489">A Tangible Augmented Reality Programming Learning Environment for textual languages</h3>
    
<p> <small><strong style="color: black;"> Booth: C24 </strong></small> <br> </p>
    
    <p><i>Dmitry Resnyansky</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/Ta0ufvfVPf4" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1489" class="wrap-collabsible"> <input id="collapsibleabstractC1489" class="toggle" type="checkbox"> <label for="collapsibleabstractC1489" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We present a novel Tangible Augmented Reality Programming Learning Environment system using head-mounted display (HMD) and physical manipulatives for teaching programming. The system supports student understanding/recollection of terms, and statement construction through access to terminology, explanations, and programming hints. It is designed to provide a virtual workspace for natural interaction with learning material using affordances of Augmented Reality (AR) and Tangible User Interfaces (TUIs). An AR code template provides a building and testing environment for learners to practice statement construction and computational skills. The system bolsters active learning with localised AR program visualisations and HMD-anchored AR glossary.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1497">Improved Offset Handling in Hand-centered Object Manipulation Extending Ray-casting</h3>
    
<p> <small><strong style="color: black;"> Booth: C13 </strong></small> <br> </p>
    
    <p><i>Karljohan Lundin Palmerius</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/hWx0UxcCtS4" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1497" class="wrap-collabsible"> <input id="collapsibleabstractC1497" class="toggle" type="checkbox"> <label for="collapsibleabstractC1497" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>One of the most common types of interaction in Virtual Reality (VR) is pose manipulation. The simultaneous manipulation of both object position and orientation, 6 DoF interaction, that can be complex in desktop environments, can become simple in VR, by mapping the natural motion with a 3D interaction controller (wand) to the motion of the object. In this paper we acknowledge the importance of an offset variable in HOMER, present a way of handling the offset consistently when the user faces different directions during interaction, compare this to using naive static offset with respect to task completion time and number of re-grabs, and measure their respective System Usability Scale (SUS) score and Raw NASA Task Load Index (RTLX).</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1498">If I Share with you my Perspective, Would you Share your Data with me?</h3>
    
<p> <small><strong style="color: black;"> Booth: C14 </strong></small> <br> </p>
    
    <p><i>Tianyu Song</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/Trd9Y1AOYvU" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1498" class="wrap-collabsible"> <input id="collapsibleabstractC1498" class="toggle" type="checkbox"> <label for="collapsibleabstractC1498" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Real-time 3D reconstruction using multiple RGBD cameras and their online transmission facilitates the adoption of mixed reality telepresence. However, such a system can only cover a limited volume, and increasing the number of RGBD cameras is unfeasible due to setup complexity and space constraints. To address this issue, we present the concept of Dynamic 3D View Sharing, which complements the views of a 3D reconstruction system by the dynamic view of the user's HMD. Here, we present a markerless calibration method integrating these two seamlessly into the mixed reality telepresence systems without disrupting the current workflow.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1509">Head in the Clouds - Floating Locomotion in Virtual Reality</h3>
    
<p> <small><strong style="color: black;"> Booth: C36 </strong></small> <br> </p>
    
    <p><i>Priya Ganapathi</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=aquU2pb_F1w" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1509" class="wrap-collabsible"> <input id="collapsibleabstractC1509" class="toggle" type="checkbox"> <label for="collapsibleabstractC1509" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Navigating large virtual spaces within the confines of a small tracked volume in a seated position becomes a serious accessibility issue when users' lower seating position reduces their visibility and makes it uncomfortable to reach for items with ease. Hence, we propose a &quot;floating&quot; accessibility technique, in which a seated VR user experiences the virtual environment from the perspective of a standing eye height. We conducted a user study comparing sitting, standing and floating conditions and observed that the floating technique had no detrimental effect in comparison to the standing technique and had a slight benefit over the sitting technique.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1516">OmniSyn: Synthesizing 360 Videos with Wide-baseline Panoramas</h3>
    
<p> <small><strong style="color: black;"> Booth: C35 </strong></small> <br> </p>
    
    <p><i>David Li</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/n4dhMVKwnkE" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1516" class="wrap-collabsible"> <input id="collapsibleabstractC1516" class="toggle" type="checkbox"> <label for="collapsibleabstractC1516" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Immersive maps such as Google Street View and Bing Streetside provide true-to-life views with a massive collection of panoramas. However, these panoramas are only available at sparse intervals along the path they are taken, resulting in visual discontinuities during navigation. In this paper, we leverage the unique characteristics of wide-baseline panoramas and present OmniSyn, a novel pipeline for 360 view synthesis between wide-baseline panoramas. OmniSyn predicts omnidirectional depth maps, renders meshes, and synthesizes intermediate views. We envision our work may inspire future research for this real-world task and lead to smoother experiences navigating immersive maps.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1518">Proximity in VR: The Importance of Character Attractiveness and Participant Gender</h3>
    
<p> <small><strong style="color: black;"> Booth: C21 </strong></small> <br> </p>
    
    <p><i>Katja Zibrek</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/MorQ88_xyWk" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1518" class="wrap-collabsible"> <input id="collapsibleabstractC1518" class="toggle" type="checkbox"> <label for="collapsibleabstractC1518" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In this study, we expand upon recent evidence that the motion of virtual characters affects the proximity of users who are immersed with them in virtual reality (VR). Characters with attractive motions decrease proximity, while females prefer further distances from characters than males. No effect of character gender on proximity was found. We designed a similar experiment where users observed walking motions in VR which were displayed on male and female virtual characters. Our results show similar patterns found in previous studies, while some differences due to the specifics of the characters emerged.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1530">A Comparison of Input Devices for Precise Interaction Tasks in VR-based Surgical Planning and Training</h3>
    
<p> <small><strong style="color: black;"> Booth: C22 </strong></small> <br> </p>
    
    <p><i>Mareen Allgaier</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=ixza7NYrZ_8" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1530" class="wrap-collabsible"> <input id="collapsibleabstractC1530" class="toggle" type="checkbox"> <label for="collapsibleabstractC1530" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We present a comparison of input devices for common interaction tasks in medical VR training and planning based on two relevant applications. The chosen devices, VR controllers, VR Ink, data gloves, and a real medical instrument, differ in their degree of specialization and their grip.
The conducted user study shows that the controllers and VR Ink performed significantly better than the other devices regarding precision. Concerning questionnaire results, no device stands out but most participants preferred the VR Ink for both applications.
These results can serve as a guide to identify an appropriate device for future medical VR applications.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1542">AiRType: An Air-Tapping Keyboard for Augmented Reality Environments</h3>
    
<p> <small><strong style="color: black;"> Booth: C41 </strong></small> <br> </p>
    
    <p><i>&Uuml;lk&uuml; Meteriz-Y?ld?ran</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/iUII8ckpudU" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1542" class="wrap-collabsible"> <input id="collapsibleabstractC1542" class="toggle" type="checkbox"> <label for="collapsibleabstractC1542" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We present AiRType for AR/VR HMDs that enables text entry through bare hands for more natural perception. The hand models in the virtual environment mirror hand movements of the user and user targets and selects the keys via hand models. AiRType fully leverages the additional dimension without restraining the interaction space by users' arm lengths. It can be attached to anywhere and can be scaled freely. We evaluated and compared AiRType with the baseline-the built-in keyboard of Magic Leap 1. AiRType shows 27&#37; decrease in the error rate, 3.3&#37; increase in character-per-second, and 9.4&#37; increase in user satisfaction.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1557">Interacting with a Torque-Controlled Virtual Human in Virtual Reality for Ergonomics Studies</h3>
    
<p> <small><strong style="color: black;"> Booth: C23 </strong></small> <br> </p>
    
    <p><i>Jacques Zhong</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/6gfz9rzWXE8" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1557" class="wrap-collabsible"> <input id="collapsibleabstractC1557" class="toggle" type="checkbox"> <label for="collapsibleabstractC1557" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This paper presents a new tool to help ergonomists conduct studies on digital human models (DHM) in a more intuitive and physically consistent way. To do so, a virtual reality setup was combined with a DHM in a real-time physics simulation. Therefore, the user is able to directly manipulate the DHM within the virtual workplace and quickly experiment with a variety of scenarios.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1566">Head-Worn Markerless Augmented Reality Inside A Moving Vehicle</h3>
    
<p> <small><strong style="color: black;"> Booth: C42 </strong></small> <br> </p>
    
    <p><i>Zhiwei Zhu</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/yNXXpmcYuYQ" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1566" class="wrap-collabsible"> <input id="collapsibleabstractC1566" class="toggle" type="checkbox"> <label for="collapsibleabstractC1566" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This paper describes a system that provides general head-worn outdoor augmented reality (AR) capability for the user inside a moving vehicle. Our system follows the concept of combining pose estimation from both vehicle navigation system and wearable sensors to address the failure of commercial AR devices inside a moving vehicle. We continuously match natural visual features from the camera against a prebuilt database of interior vehicle scenes. To improve the robustness in a moving vehicle with other passengers, a human detection module is adapted to filter out people from the camera scene. Experiments demonstrate the effectiveness of the proposed solution.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1572">Cloud-Based Cross-Platform Collaborative AR in Flutter</h3>
    
<p> <small><strong style="color: black;"> Booth: C24 </strong></small> <br> </p>
    
    <p><i>Christian Eichhorn</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/Ia6EJzKj3Jw" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1572" class="wrap-collabsible"> <input id="collapsibleabstractC1572" class="toggle" type="checkbox"> <label for="collapsibleabstractC1572" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We present a novel collaborative AR framework aimed at lowering the entry barriers and operating expenses of AR applications. It includes a cross-platform and cloud-based Flutter plugin combined with a web-based content management system allowing non-technical staff to take over operational tasks such as providing 3D models. To provide a SOA feature set, the AR Flutter plugin builds upon ARCore and ARKit and unifies the two frameworks using an abstraction layer written in Dart. Our contribution closes a gap by providing an AR framework seamlessly integrating with the familiar development process of cross-platform apps. With the accompanying content management system, AR can be used as a tool to achieve business objectives.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1573">Using External Video to Attack Behavior-Based Security Mechanisms in Virtual Reality (VR)</h3>
    
<p> <small><strong style="color: black;"> Booth: C43 </strong></small> <br> </p>
    
    <p><i>Robert Miller</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/Y6aqFL8C-e0" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1573" class="wrap-collabsible"> <input id="collapsibleabstractC1573" class="toggle" type="checkbox"> <label for="collapsibleabstractC1573" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>As VR systems become prevalent in domains such as healthcare and education, sensitive data must be protected from attacks. Password-based techniques are circumvented once an attacker gains access to the user's credentials. Behavior-based approaches are susceptible to attacks from malicious users who mimic the actions of a genuine user or gain access to the 3D trajectories. We investigate a novel attack where a malicious user obtains a 2D video of genuine user interacting in VR. We demonstrate that an attacker can extract 2D motion trajectories from the video and match them to 3D enrollment trajectories to defeat behavior-based VR security.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1577">How Late is Too Late? Effects of Network Latency on Audio-Visual Perception During AR Remote Musical Collaboration</h3>
    
<p> <small><strong style="color: black;"> Booth: C28 </strong></small> <br> </p>
    
    <p><i>Torin Hopkins</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=69ewcIKQWdY" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1577" class="wrap-collabsible"> <input id="collapsibleabstractC1577" class="toggle" type="checkbox"> <label for="collapsibleabstractC1577" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Networked Musical Collaboration requires near-instantaneous network transmission for successful real-time collaboration. We studied the way changes in network latency affect participants' auditory and visual perception in latency detection, as well as latency tolerance in AR. Twenty-four participants were asked to play a hand drum with a prerecorded remote musician rendered as an avatar in AR at different levels of audio-visual latency. We analyzed the subjective responses of the participants from each session. Results suggest a minimum noticeable delay value between 160 milliseconds (ms) and 320 ms, as well as no upper limit to audio-visual delay tolerance.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1578">Toward Using Multi-Modal Machine Learning for User Behavior Prediction in Simulated Smart Home for Extended Reality</h3>
    
<p> <small><strong style="color: black;"> Booth: C31 </strong></small> <br> </p>
    
    <p><i>Powen Yao</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/nKgjwGd8ZOs" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1578" class="wrap-collabsible"> <input id="collapsibleabstractC1578" class="toggle" type="checkbox"> <label for="collapsibleabstractC1578" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In this work, we propose a multi-modal approach to manipulate smart home devices in a smart home environment simulated in virtual reality (VR). We determine the user's target device and the desired action by their utterance, spatial information (gestures, positions, etc.), or a combination of the two. Since the information contained in the user's utterance and the spatial information can be disjoint or complementary to each other, we process the two sources of information in parallel using our array of machine learning models. We use ensemble modeling to aggregate the results of these models and enhance the quality of our final prediction results. We present our preliminary architecture, models, and findings.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1580">VR-based Context Priming to Increase Student Engagement and Academic Performance</h3>
    
<p> <small><strong style="color: black;"> Booth: C44 </strong></small> <br> </p>
    
    <p><i>Daniel Hawes</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/WrEeicPC0QA" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1580" class="wrap-collabsible"> <input id="collapsibleabstractC1580" class="toggle" type="checkbox"> <label for="collapsibleabstractC1580" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Research suggests that virtual environments can be designed to increase engagement and performance with many cognitive tasks. This paper compares the efficacy of specifically designed 3D environments intended to prime these effects within Virtual Reality (VR). A 27-minute seminar &quot;The Creative Process of Making an Animated Movie&quot; was presented to 51 participants within three VR learning spaces: two prime and one no-prime. The prime conditions included two situated learning environments&#59; an animation studio and a theatre with animation artifacts vs. the no-prime: theatre without artifacts. Increased academic performance was observed in both prime conditions. A UX survey was also completed.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1590">A Live-Coded Add-On System for Video Conferencing in Virtual Reality</h3>
    
<p> <small><strong style="color: black;"> Booth: C38 </strong></small> <br> </p>
    
    <p><i>Septian Razi</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/HdTGUFWILqk" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1590" class="wrap-collabsible"> <input id="collapsibleabstractC1590" class="toggle" type="checkbox"> <label for="collapsibleabstractC1590" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Despite our increasing reliance on video conferencing, it has inherent limitations in engagement and effective communication. We explore a VR add-on system that supplements traditional video conferencing where a host live-codes data visualisations for stakeholders. It combines VR visualisation techniques with libraries present in popular data analytics tools such as Python, allowing real-time changes by utilising a RESTful real-time database. An application has been built to explore pedigree node-link graphs, and its feasibility has been analysed with a few domain experts. Trials have demonstrated that our system appears to enhance engagement and communication above traditional video conferencing for data exploration.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1603">From 2D to 3D: Facilitating Single-Finger Mid-Air Typing on Virtual Keyboards with Probabilistic Touch Modeling</h3>
    
<p> <small><strong style="color: black;"> Booth: D11 </strong></small> <br> </p>
    
    <p><i>Chen Liang</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=6BEOYAMo5S4" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1603" class="wrap-collabsible"> <input id="collapsibleabstractC1603" class="toggle" type="checkbox"> <label for="collapsibleabstractC1603" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Mid-air text entry on virtual keyboards suffers from the lack of tactile feedback, bringing challenges to both tap detection and input prediction. In this poster, we demonstrated the feasibility of efficient single-finger typing in mid-air through probabilistic touch modeling. We first collected users' typing data on different sizes of virtual keyboards. Based on analyzing the data, we derived an input prediction algorithm that incorporated probabilistic touch detection and elastic probabilistic decoding. In the evaluation study where the participants performed real text entry tasks with this technique, they reached a pick-up single-finger typing speed of 24.0 WPM with 2.8&#37; word-level error rate.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1607">Seamless Walk: Novel Natural Virtual Reality Locomotion Method with a High-Resolution Tactile Sensor</h3>
    
<p> <small><strong style="color: black;"> Booth: C37 </strong></small> <br> </p>
    
    <p><i>Yunho Choi</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/h2pAaVl5PUQ" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1607" class="wrap-collabsible"> <input id="collapsibleabstractC1607" class="toggle" type="checkbox"> <label for="collapsibleabstractC1607" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Natural movement is a challenging problem in virtual reality locomotion. However, existing foot-based locomotion methods lack naturalness due to physical limitations caused by wearing equipment. Therefore, in this study, we propose Seamless-walk, a novel virtual reality (VR) locomotion technique to enable locomotion in the virtual environment by walking on a high-resolution tactile carpet. The proposed Seamless-walk moves the user's virtual character by extracting the users' walking speed and orientation from raw tactile signals using machine learning techniques. We demonstrate that the proposed Seamless-walk is more natural and effective than existing VR locomotion methods by comparing them in VR game-playing tasks.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1614">Understanding the Capabilities of the HoloLens 1 and 2 in a Mixed Reality Environment for Direct Volume Rendering with a Ray-casting Algorithm</h3>
    
<p> <small><strong style="color: black;"> Booth: C36 </strong></small> <br> </p>
    
    <p><i>Hoijoon Jung</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/mqHb7gZn-fs" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1614" class="wrap-collabsible"> <input id="collapsibleabstractC1614" class="toggle" type="checkbox"> <label for="collapsibleabstractC1614" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Direct volume rendering (DVR) is a standard technique for visualizing scientific volumetric data in three-dimension (3D). Utilizing current mixed reality head-mounted displays (MR-HMDs), the DVR can be displayed as a 3D hologram that can be superimposed on the original 'physical' object, offering supplementary x-ray visions showing its interior features. These MR-MHDs are stimulating innovations in a range of scientific application fields, yet their capabilities on DVR have yet to be thoroughly investigated. In this study, we explore a key requirement of rendering latency capability for MR-HMDs by proposing a benchmark application with 5 volumes and 30 rendering parameter variations.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1625">Splitting Large Convolutional Neural Network Layers to Run Real-Time Applications on Mixed-Reality Hardware: Extended Abstract</h3>
    
<p> <small><strong style="color: black;"> Booth: D12 </strong></small> <br> </p>
    
    <p><i>Anthony Beug</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/-E4fvkwCKyA" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1625" class="wrap-collabsible"> <input id="collapsibleabstractC1625" class="toggle" type="checkbox"> <label for="collapsibleabstractC1625" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>When executing a computationally expensive Convolutional Neural Network (CNN) in a real-time mixed-reality application, some convolutional layers may take longer than the target frame time to execute. In this work, dropped frames produced by large convolutional layers are avoided by dividing the work performed in a convolution so that it can be executed over multiple frames. Existing convolution splitting techniques are applied to pretrained CNNs with expensive convolutions, and static schedules are designed to execute the resulting layers over multiple frames. Overhead is introduced, but the average frame rate is increased since delays produced by computing large layers are avoided.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1630">The Virtual-Augmented Reality Simulator: Evaluating OST-HMD AR calibration algorithms in VR</h3>
    
<p> <small><strong style="color: black;"> Booth: C35 </strong></small> <br> </p>
    
    <p><i>Danilo Gasques</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/uhn3jzH2iP0" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1630" class="wrap-collabsible"> <input id="collapsibleabstractC1630" class="toggle" type="checkbox"> <label for="collapsibleabstractC1630" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>When developing AR applications for high-precision domains such as surgery, we face a common problem: how can the system guarantee that the end-user will see a virtual object aligned with its real-world counterpart? Alignment, or registration, is a crucial feature of AR displays, but achieving accurate alignment between real and virtual objects is not trivial. With hundreds of calibration approaches available, we need better tools to understand how and when calibration algorithms fail as well as understand what can be done to improve alignment. This poster introduces a novel AR simulator in VR that facilitates experimentation with different calibration algorithms.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1634">Mixed Reality Support for Bridge Inspectors</h3>
    
<p> <small><strong style="color: black;"> Booth: C28 </strong></small> <br> </p>
    
    <p><i>Urs Riedlinger</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/3vw9nlg3_4M" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1634" class="wrap-collabsible"> <input id="collapsibleabstractC1634" class="toggle" type="checkbox"> <label for="collapsibleabstractC1634" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Bridge inspectors work for the safety of our infrastructure and mobility. In regular intervals, they conduct structural inspections - a manual task with a long-lasting and firmly normed analogue tradition. We propose a mixed analogue and digital workflow that includes Mixed Reality views that can be ready-to-hand for bridge inspectors during their work at and in a bridge. Our presented demonstrator was iteratively designed in a collaborative research project and turned into a tablet-based application to digitally support that work. It employs BIM data that contains 3D geometry-data and additional data about the structure, such as previous damage reports.</p>
            </div>
        </div>
    </div>
    

    <h3 id="C1645">Study of communication modalities for teaching distance information</h3>
    
<p> <small><strong style="color: black;"> Booth: C31 </strong></small> <br> </p>
    
    <p><i>Cassandre Simon</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/tpwFHLdRCbI" target="_blank">Watch Now</a></p>
    
    <div id="abstractC1645" class="wrap-collabsible"> <input id="collapsibleabstractC1645" class="toggle" type="checkbox"> <label for="collapsibleabstractC1645" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We present an exploratory study to compare the haptic, visual, and verbal modalities for communicating distance information in a shared virtual environment. The results show that the visual modality decreased the distance estimation error while the haptic modality decreased the completion time. The verbal modality increased the sense of copresence but was the least preferred modality. These results suggest that a combination of modalities could improve communication of distance information to a partner. These findings can contribute to improve the design of collaborative VR systems and open new research perspectives on studying the effectiveness of multimodal interaction.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1039">Relationship Between the Sensory Processing Patterns and the Detection Threshold of Curvature Gain</h3>
    
<p> <small><strong style="color: black;"> Booth: C41 </strong></small> <br> </p>
    
    <p><i>Keigo Matsumoto: The University of Tokyo&#59; Takuji Narumi: the University of Tokyo</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/_IWc7ftJ8x8" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1039" class="wrap-collabsible"> <input id="collapsibleabstractPO1039" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1039" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This study examines the relationship between sensory processing patterns (SPPs) and the effects of redirected walking (RDW). Research efforts have been devoted to identifying the detection threshold (DT) of the RDW techniques, and various DTs have been reported in different studies. Recently, age, sex, and spatial ability have been found to be associated with the DTs of RDW techniques. A preliminary examination was conducted on the relationship between SSPs, as measured by the Adolescents/Adult Sensory Profile, and the DT of curvature gains, one of the fundamental RDW techniques, and it was suggested that the higher sensory sensitivity tendencies were associated with lower DT, i.e., participants were more likely to notice the RDW technique.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1061">Predicting Blendshapes of Virtual Humans for Low-Delay Remote Rendering using LSTM</h3>
    
<p> <small><strong style="color: black;"> Booth: C42 </strong></small> <br> </p>
    
    <p><i>Haruhisa Kato: KDDI Research Inc.&#59; Tatsuya Kobayashi: KDDI Research Inc.&#59; Sei Naito: KDDI Research Inc.</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=_6Z9bqDXLvQ" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1061" class="wrap-collabsible"> <input id="collapsibleabstractPO1061" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1061" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This paper proposes a novel framework to reduce the perceptual delay in rendering the facial expressions of virtual humans. The proposed method reduces the delay by predicting the future blend coefficients that represent facial expressions. The prediction accuracy of the proposed method is improved by 27&#37; on average over a conventional
LSTM. We also subjectively confirmed that the proposed method achieves natural facial expressions.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1081">Using 3D Reconstruction to create Pervasive Augmented Reality Experiences: A comparison</h3>
    
<p> <small><strong style="color: black;"> Booth: C38 </strong></small> <br> </p>
    
    <p><i>Miguel Neves: University of Aveiro&#59; Bernardo Marques: Universidade de Aveiro&#59; Tiago Madeira: Universidade de Aveiro&#59; Paulo Dias: University of Aveiro&#59; Beatriz Sousa Santos: University of Aveiro</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/YUP0r-UvQ74" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1081" class="wrap-collabsible"> <input id="collapsibleabstractPO1081" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1081" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This paper presents a prototype for configuration and visualization of Pervasive Augmented Reality (AR) experiences using two versions: desktop and mobile. It makes use of 3D scans from physical environments to provide a reconstructed digital representation of such spaces to the desktop version and enable positional tracking for the mobile. While the desktop presents a non-immersive setting, the mobile provides continuous AR in the physical environment. Both versions can be used to place virtual content and ultimately configure an AR experience. The authoring capabilities of the proposed solution were compared by conducting a user study focused on evaluating their usability.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1082">Does Remote Expert Representation really matters: A comparison of Video and AR-based Guidance</h3>
    
<p> <small><strong style="color: black;"> Booth: C37 </strong></small> <br> </p>
    
    <p><i>Bernardo Marques: Universidade de Aveiro&#59; Samuel Silva: Universidade de Aveiro&#59; Paulo Dias: University of Aveiro&#59; Beatriz Sousa Santos: University of Aveiro</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/SPju4spZht4" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1082" class="wrap-collabsible"> <input id="collapsibleabstractPO1082" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1082" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This work describes a user study aimed at understanding how the remote expert representation affects the sense of social presence in scenarios of remote guidance. We compared a traditional video chat solution with an Augmented Reality (AR) annotation tool. These were selected due to ongoing research with partners from the industry sector, following the insights of a participatory design process. A well defined-problem was used, i.e., a synchronous maintenance task with 4 completion stages that required a remote expert using a computer to guide 26 on-site participants wielding a handheld device. The results of the study are described and discussed.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1083">Whac-A-Mole: Exploring Virtual Reality (VR) for Upper-Limb Post-Stroke Physical Rehabilitation based on Participatory Design and Serious Games</h3>
    
<p> <small><strong style="color: black;"> Booth: C36 </strong></small> <br> </p>
    
    <p><i>Helder Paraense Serra: University of Aveiro&#59; Bernardo Marques: Universidade de Aveiro&#59; Paula Amorim: University of Beira Interior&#59; Paulo Dias: University of Aveiro&#59; Beatriz Sousa Santos: University of Aveiro</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/2hmMowFxLiU" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1083" class="wrap-collabsible"> <input id="collapsibleabstractPO1083" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1083" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This paper describes a Human-Centered Design methodology aimed at understanding how Virtual Reality (VR) can assist in post-stroke physical rehabilitation. Based on insights from stroke survivors and healthcare members, a serious game prototype is proposed. We focused on upper-limb rehabilitation, which inspired the game narrative and the movements users must perform. The game supports two modes: 1- normal version - users can use any arm to pick a virtual hammer and hit objects; 2- mirror version - converts a traditional approach to VR, providing the illusion that the arm affected by the stroke is moving. These were evaluated through a user study.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1092">Digital Twins of Wave Energy Generation Based on Artificial Intelligence</h3>
    
<p> <small><strong style="color: black;"> Booth: D13 </strong></small> <br> </p>
    
    <p><i>Yuqi Liu: College of Computer Science and Technology&#59; Xiaocheng Liu: College of Computer Science and Technology&#59; Jinkang Guo: Qingdao University&#59; Ranran Lou: Qingdao University&#59; Zhihan Lv: Uppsala University</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/iiRWyM8fM7M" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1092" class="wrap-collabsible"> <input id="collapsibleabstractPO1092" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1092" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Ocean waves provide a large amount of renewable energy, and Wave energy converter (WEC) can convert wave energy into electric energy. This paper proposes a visualization platform for wave power generation. The platform can monitor various indicators of wave power generation in real time, combined with Long Short-Term Memory (LSTM) neural network to predict wave power and electricity consumption. We make digital twins of a wave power plant in a computer, allowing users to remotely view the factory through VR glasses.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1095">Distinguishing Visual Fatigue, Mental Workload and Acute Stress in Immersive Virtual Reality with Physiological Data: pre-test results</h3>
    
<p> <small><strong style="color: black;"> Booth: C35 </strong></small> <br> </p>
    
    <p><i>Alexis Souchet: CNRS&#59; Weifei Xie: CNRS&#59; Domitile Lourdeaux: Sorbonne University Association, University of Technology of Compi&egrave;gne, CNRS</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=SZOrf_7zrsc" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1095" class="wrap-collabsible"> <input id="collapsibleabstractPO1095" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1095" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Virtual Reality Induced Symptoms and Effects (VRISE) can arise. Experimental paradigms are heterogeneous to assess them, and various factors can induce physiological variations. Therefore, we developed a Stroop task to study and distinguish VRISE. We use eye tracking, ECG, EDA, and VRSQ, NASA-TLX, and STAI-6 questionnaires. Pre-tests have been conducted with 6 subjects exposed to 4 experimental conditions: control, dual task, stressful, and stereoscopy. Subjects report different subjective visual fatigue and mental workload but not stress between conditions. Several physiological features are different between conditions. A VRISE detector can be envisioned based on physiological data and questionnaires as an index.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1103">AIR-range: Arranging optical systems to present mid-AIR images with continuous luminance on and above a tabletop</h3>
    
<p> <small><strong style="color: black;"> Booth: C43 </strong></small> <br> </p>
    
    <p><i>Tomoyo Kikuchi: The University of Tokyo&#59; Yuchi Yahagi: The University of Tokyo&#59; Shogo Fukushima: The University of Tokyo&#59; Saki Sakaguchi: Tokyo Metropolitan University&#59; Takeshi Naemura: The University of Tokyo</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/OOtvkFNTKT0" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1103" class="wrap-collabsible"> <input id="collapsibleabstractPO1103" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1103" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We propose &quot;AIR-range&quot;- a system that seamlessly connects mid-air images from the surface of a table to mid-air space. This system can display tall mid-air images in the three-dimensional (3D) space beyond the screen. AIR-range is implemented using a symmetrical mirror structure that displays a large image by integrating multiple imaging paths. The mirror arrangement in previous research had a problem in that the luminance was discontinuous. In this study, we theorize the relationship between the parameters of optical elements and the appearance of mid-air images and optimize an optical system to minimize the difference in luminance between image paths.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1104">Towards Scalable and Real-time Markerless Motion Capture</h3>
    
<p> <small><strong style="color: black;"> Booth: C41 </strong></small> <br> </p>
    
    <p><i>Georgios Albanis: University of Thessaly&#59; Anargyros Chatzitofis: AC CODEWHEEL LTD&#59; Spyridon Thermos: AC Codewheel Ltd&#59; Nikolaos Zioulis: Independent Researcher&#59; Kostas Kolomvatsos: University of Thessaly</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=f8lWahAPwUA" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1104" class="wrap-collabsible"> <input id="collapsibleabstractPO1104" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1104" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Human motion capture and perception without the need for complex systems with specialized cameras or wearable equipment is the holy grail for many human-centric applications. Here, we present a scalable markerless motion capture method that estimates 3D human poses in real-time using low-cost hardware. We do so by replacing the inefficient 3D joint reconstruction techniques, such as learnable triangulation and feature splatting, with a novel uncertainty-driven approach that exploits the available depth information and the edge sensors' spatial alignment to fuse the per viewpoint estimates into final 3D joint positions.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1105">A Mixed Reality Guidance System for Blind and Visually Impaired People</h3>
    
<p> <small><strong style="color: black;"> Booth: C42 </strong></small> <br> </p>
    
    <p><i>Hannah Schieber: Friedrich Alexander University Erlangen-N&uuml;rnberg&#59; Constantin Kleinbeck: Friedrich-Alexander Universit&auml;t Erlangen-N&uuml;rnberg&#59; Charlotte Pradel: Friedrich Alexander University Erlangen-N&uuml;rnberg&#59; Luisa Theelke: Friedrich Alexander University Erlangen-N&uuml;rnberg&#59; Daniel Roth: Friedrich Alexander University Erlangen-N&uuml;rnberg</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=gj1QBMwGdQE" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1105" class="wrap-collabsible"> <input id="collapsibleabstractPO1105" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1105" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Persons affected by blindness or visual impairments are challenged by spatially understanding unfamiliar environments. To obtain such understanding, they have to sense their environment closely and carefully. Especially objects outside the sensing area of analog assistive devices, such as a white cane, are simply not perceived and can be the cause of collisions. This project proposes a mixed reality guidance system that aims at preventing such problems. We use object detection and the 3D sensing capabilities of a mixed reality head mounted device to inform users about their spatial surroundings.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1107">Holding Hands for Short-Term Group Navigation in Social Virtual Reality</h3>
    
<p> <small><strong style="color: black;"> Booth: C43 </strong></small> <br> </p>
    
    <p><i>Tim Weissker: Bauhaus-Universitaet Weimar&#59; Pauline Bimberg: Bauhaus-Universitaet Weimar&#59; Ankith Kodanda: Bauhaus-Universitaet Weimar&#59; Bernd Froehlich: Bauhaus-Universit&auml;t Weimar</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=UPtn18b-jIE" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1107" class="wrap-collabsible"> <input id="collapsibleabstractPO1107" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1107" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Prior research has shown that social interactions in VR benefit from techniques for group navigation that bring multiple users to a common destination together. In this work, we propose the metaphor of holding onto another user's virtual hand for the ad-hoc formation of a navigational group and report on the positive results of an initial usability study in an exploratory two-user scenario.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1111">Third-Person Perspective Avatar Embodiment in Augmented Reality: Examining the Proteus Effect on Physical Performance</h3>
    
<p> <small><strong style="color: black;"> Booth: C44 </strong></small> <br> </p>
    
    <p><i>Riku Otono: Nara Institute of Science and Technology&#59; Naoya Isoyama: Nara Institute of Science and Technology&#59; Hideaki Uchiyama: Nara Institute of Science and Technology&#59; Kiyoshi Kiyokawa: Nara Institute of Science and Technology</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/RBcTPqbDxhs" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1111" class="wrap-collabsible"> <input id="collapsibleabstractPO1111" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1111" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Embodiment in augmented reality (AR) is applicable to various fields such as exercise and education. However, full-body embodiment in AR is still challenging to implement due to technical problems such as low body tracking accuracy. Therefore, the study on the impact of an avatar in AR on user performance is limited. We implemented an AR embodiment system and investigated its impact on user physical performance. The system allows users to see their avatar instead of their real body from a third-person perspective. The results show that a muscular avatar improves user physical performance during and after controlling the avatar.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1113">Stay Safe! Safety Precautions for Walking on a Conventional Treadmill in VR</h3>
    
<p> <small><strong style="color: black;"> Booth: C44 </strong></small> <br> </p>
    
    <p><i>Sandra Birnstiel: University of W&uuml;rzburg&#59; Sebastian Oberd&ouml;rfer: University of W&uuml;rzburg&#59; Marc Erich Latoschik: Department of Computer Science, HCI Group</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/P2Dvf64aeps" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1113" class="wrap-collabsible"> <input id="collapsibleabstractPO1113" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1113" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Conventional treadmills are used in virtual reality (VR) applications, such as for rehabilitation training or gait studies. However, using the devices in VR poses risks of injury. Therefore, this study investigates safety precautions when using a conventional treadmill for a walking task. We designed a safety belt and displayed parts of the treadmill in VR. The safety belt was much appreciated by the participants and did not affect the walking behavior. However, the participants requested more visual cues in the user's field of view.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1117">Exploring How, for Whom and in Which Contexts Extended Reality Training 'Works' in Upskilling Healthcare Workers: A Realist Review</h3>
    
<p> <small><strong style="color: black;"> Booth: D11 </strong></small> <br> </p>
    
    <p><i>Norina Gasteiger: University of Manchester&#59; Sabine N van der Veer: University of Manchester&#59; Paul Wilson: University of Manchester&#59; Dawn Dowding: University of Manchester</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/dgssW35R8LM" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1117" class="wrap-collabsible"> <input id="collapsibleabstractPO1117" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1117" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Extended reality (XR), including virtual reality (VR) and augmented reality (AR) may overcome barriers to training healthcare workers, such as resource constraints. However, the effectiveness of XR training is disputed and not well understood. Our realist review explores how, for whom and in what contexts AR and VR training 'works' in upskilling healthcare workers. Eighty papers informed our program theory, while 46 empirical studies tested/refined it. We conclude that XR triggers perceptions of realism and deep immersion, and enables visualization, interactive learning, enhancement of skills and repeated practice within a safe learning environment, consequently bettering skills, learning/knowledge and learner satisfaction.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1118">ARTFM: Augmented Reality Visualization of Tool Functionality Manuals in Operating Rooms</h3>
    
<p> <small><strong style="color: black;"> Booth: D12 </strong></small> <br> </p>
    
    <p><i>Constantin Kleinbeck: Friedrich-Alexander Universit&auml;t Erlangen-N&uuml;rnberg&#59; Hannah Schieber: Friedrich-Alexander University&#59; Sebastian Andress: Ludwig-Maximilians-Universit&auml;t M&uuml;nchen&#59; Christian Krautz: Universit&auml;tsklinikum Erlangen&#59; Daniel Roth: Friedrich-Alexander-Universit&auml;t Erlangen-N&uuml;rnberg</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/muAa74zvmh0" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1118" class="wrap-collabsible"> <input id="collapsibleabstractPO1118" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1118" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Error-free surgical procedures are crucial for a patient's health. However, with the increasing complexity and variety of surgical instruments, it is difficult for clinical staff to acquire detailed assembly and usage knowledge leading to errors in process and preparation steps. Yet, the gold standard in retrieving necessary information when problems occur is to get the paper-based manual. Reading through the necessary instructions is time-consuming and decreases care quality. We propose ARTFM, a process integrated manual, highlighting the correct parts needed, their location, and step-by-step instructions to combine the instrument using an augmented reality head-mounted display.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1120">Comparing Controller with the Hand Gestures Pinch and Grab for Picking Up and Placing Virtual Objects</h3>
    
<p> <small><strong style="color: black;"> Booth: D13 </strong></small> <br> </p>
    
    <p><i>Alexander Sch&auml;fer: TU Kaiserslautern&#59; Gerd Reis: German Research Center for Artificial Intelligence&#59; Didier Stricker: German Research Center for Artificial Intelligence</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=czur92F6vMg" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1120" class="wrap-collabsible"> <input id="collapsibleabstractPO1120" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1120" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Grabbing virtual objects is one of the essential tasks for Augmented, Virtual, and Mixed Reality applications. Modern applications usually use a simple pinch gesture for grabbing and moving objects. However, picking up objects by pinching has disadvantages. It can be an unnatural gesture to pick up objects and prevents the implementation of other gestures which would be performed with thumb and index. Therefore it is not the optimal choice for many applications. In this work, different implementations for grabbing and placing virtual objects are proposed and compared. Performance and accuracy of the proposed techniques are measured and compared.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1121">Omnidirectional Neural Radiance Field for Immersive Experience</h3>
    
<p> <small><strong style="color: black;"> Booth: D11 </strong></small> <br> </p>
    
    <p><i>Qiaoge Li: University of Tsukuba&#59; Itsuki Ueda: University of Tsukuba&#59; Chun Xie: University of Tsukuba&#59; Hidehiko Shishido: University of Tsukuba&#59; Itaru Kitahara: University of Tsukuba</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/B989wn84DxY" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1121" class="wrap-collabsible"> <input id="collapsibleabstractPO1121" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1121" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This paper proposes a method using only RGB information from multiple captured panoramas to provide an immersive observing experience for real scenes. We generated an omnidirectional neural radiance field by adopting the Fibonacci sphere model for sampling rays and several optimized positional encoding approaches. We tested our method on synthetic and real scenes and achieved satisfying empirical performance. Our result makes the immersive continuous free-viewpoint experience possible.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1122">Social Presence in VR Empathy Game for Children: Empathic Interaction with the Virtual Characters</h3>
    
<p> <small><strong style="color: black;"> Booth: D14 </strong></small> <br> </p>
    
    <p><i>Ekaterina Muravevskaia: University of Florida&#59; Christina Gardner-McCune: University of Florida</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/hCM3QpLRxO0" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1122" class="wrap-collabsible"> <input id="collapsibleabstractPO1122" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1122" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This paper discusses children's empathic interactions with the VR characters. The study and findings in this paper were derived from a larger research study that explored the design and evaluation of empathic experiences for young children (6-9 years old) in VR environments. We found similarities between how children interacted with the VR characters and with real people (i.e., cognitive empathy and emotional contagion). This provides initial insight into children's experience of social presence with the VR characters. We suggest follow-up research on connection between empathy and social presence to explore the ways to create empathic VR experiences for children.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1131">Who do you look like? - Gaze-based authentication for workers in VR</h3>
    
<p> <small><strong style="color: black;"> Booth: D14 </strong></small> <br> </p>
    
    <p><i>Karina LaRubbio: University of Florida&#59; Jeremiah Wright: University of Florida&#59; Brendan David-John: University of Florida&#59; Andreas Enqvist: University of Florida&#59; Eakta JAIN: University of Florida</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/5Q-Yhv7c0dw" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1131" class="wrap-collabsible"> <input id="collapsibleabstractPO1131" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1131" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Behavior-based authentication methods are actively being developed for XR. In particular, gaze-based methods promise continuous authentication of remote users. However, gaze behavior depends on the task being performed. Identification rate is typically highest when comparing data from the same task. In this study, we compared authentication performance using VR gaze data during random dot viewing, 360-degree image viewing, and a nuclear training simulation. We found that within-task authentication performed best for image viewing (72&#37;). The implication for practitioners is to integrate image viewing into a VR workflow to collect gaze data that is viable for authentication.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1134">Depth Reduction in Light-Field Head-Mounted Displays by Generating Intermediate Images as Virtual Images</h3>
    
<p> <small><strong style="color: black;"> Booth: D12 </strong></small> <br> </p>
    
    <p><i>Yasutaka Maeda: Japan Broadcasting Corporation (NHK)&#59; Daiichi Koide: Japan Broadcasting Corporation (NHK)&#59; Hisayuki Sasaki: Japan Broadcasting Corporation (NHK)&#59; Kensuke Hisatomi: Japan Broadcasting Corporation (NHK)</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/HNNd_3NLoL0" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1134" class="wrap-collabsible"> <input id="collapsibleabstractPO1134" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1134" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Light-field head-mounted displays (HMDs) can resolve vergence-accommodation conflicts, which may cause visual discomfort and fatigue. However, light-field HMDs have a narrow field of view (FOV), owing to their small display size and optical configuration. We increased the FOV by adopting a large display and shifting the elemental image from the back of the corresponding microlens. In this study, we proposed a method to generate intermediate images as virtual images to reduce the distance between the microlens array and the eyepiece. We also designed a microlens array suitable for the proposed method and successfully reduced the device depth by 44&#37;.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1137">Supervised Machine Learning Hand Gesture Classification in VR for Immersive Training</h3>
    
<p> <small><strong style="color: black;"> Booth: D24 </strong></small> <br> </p>
    
    <p><i>Ozkan Cem Bahceci: BT&#59; Anasol Pena-Rios: BT&#59; Gavin Buckingham: University of Exeter&#59; Anthony Conway: BT</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/_vqOF-bg4No" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1137" class="wrap-collabsible"> <input id="collapsibleabstractPO1137" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1137" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>The fast adoption of immersive wearables evidences the need for more intuitive interaction methods between virtual environments and their users. Advances in wearables are making in-built real-time hand tracking mechanisms more common. However, wearable providers only include limited gestures available for developer use. This limits their use for VR-based training solutions, where users need to complete tasks that mimic real-world activities as much as possible to gain valuable insights on those tasks. We present a virtual reality application that collects data on hand characteristics and analyses the collected data to identify hand gestures towards achieving a more natural interaction.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1141">Perceptually-Based Optimization for Radiometric Projector Compensation</h3>
    
<p> <small><strong style="color: black;"> Booth: D13 </strong></small> <br> </p>
    
    <p><i>Ryo Akiyama: NTT&#59; Taiki Fukiage: NTT&#59; Shin'ya Nishida: Kyoto University</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/50TtJgfIyvI" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1141" class="wrap-collabsible"> <input id="collapsibleabstractPO1141" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1141" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Radiometric compensation techniques have been proposed to manipulate the appearance of arbitrarily textured surfaces using projectors. However, due to the limited dynamic range of the projectors, these compensation techniques often fail under bright environmental lighting or when the projection surface contains high contrast textures, resulting in clipping artifacts. To address this issue, we propose to apply a perceptually-based tone mapping technique to generate compensated projection images. The experimental results demonstrated that our approach minimizes the clipping artifacts and contrast degradation under challenging conditions.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1145">Effects of Mirrors on User Behavior in Social Virtual Reality Environments</h3>
    
<p> <small><strong style="color: black;"> Booth: D14 </strong></small> <br> </p>
    
    <p><i>Takayuki Kameoka: The University of Electro-Communications&#59; Seitaro Kaneko: The University of Electro-Communications</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/_ndl4GEVsk4" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1145" class="wrap-collabsible"> <input id="collapsibleabstractPO1145" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1145" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>The authors have observed that users gather in front of mirrors on VRSNS such as VRChat. Based on these observations, we hypothesized that mirrors attracted users and conducted an experiment in a controlled environment. The participants were requested to converse in pairs in a VR space with mirrors and posters, and their behavior was recorded. Results showed that, although a certain number of users gathered in front of the mirror, it did not significantly increase their chance of staying. Conversely, we received comments such as &quot;I feel relaxed when I go in front of the mirror&quot;.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1151">Implementation of an Authoring Tool for Wheelchair Simulation with Visual and Vestibular Feedback</h3>
    
<p> <small><strong style="color: black;"> Booth: D21 </strong></small> <br> </p>
    
    <p><i>Takumi Okawara: Nihon University&#59; Kousuke Motooka: Graduate School of integrated Basic Sciences, Nihon University&#59; Kazuki Okugawa: Nihon University&#59; Akihiro Miyata: Nihon University</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=Zi1uw93XmjY" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1151" class="wrap-collabsible"> <input id="collapsibleabstractPO1151" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1151" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In this study, we develop a wheelchair simulator that provides visual and vestibular feedback at a low cost by leveraging the combination of vection-inducing movies displayed on a head-mounted display and vestibular feedback provided by an electric-powered wheelchair. However, this simulator requires users to manually create a synchronized pair of a VR movie and a motion scenario (chronological control commands for the wheelchair), which necessitates considerable effort and experience. In this paper, we introduce a novel authoring tool that generates a pair of a VR movie and a motion scenario based on a few parameters entered by the user.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1154">Robust Tangible Projection Mapping with Multi-View Contour-Based Object Tracking</h3>
    
<p> <small><strong style="color: black;"> Booth: D22 </strong></small> <br> </p>
    
    <p><i>Yuta Halvorson: The University of Electro-Communications&#59; Takumi Saito: The University of Electro-Communications&#59; Naoki Hashimoto: The University of Electro-Communications</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/C6BhwqSlHsE" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1154" class="wrap-collabsible"> <input id="collapsibleabstractPO1154" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1154" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Dynamic projection mapping for moving objects can greatly enhance an augmented reality representation by using projected images because the target object can be freely moved. However, the interaction between the target object and the user has not been sufficiently considered. In this research, we propose tangible projection mapping, with which a user can grasp an object with their hands and move it freely. It uses a simple configuration of two infrared cameras to realize dynamic projection mapping that is robust to hand occlusion.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1155">High-Quality Surface-Based 3D Reconstruction Using 2.5D Maps</h3>
    
<p> <small><strong style="color: black;"> Booth: D24 </strong></small> <br> </p>
    
    <p><i>Lingxiao Song: Beijing Institute of Technology&#59; Xiao Yu: Beijing Institute of Technology&#59; Huijun Di: Beijing Institute of Technology&#59; Weiran Wang: Beijing Institute of Technology</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/s-mgZu_Jk7M" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1155" class="wrap-collabsible"> <input id="collapsibleabstractPO1155" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1155" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Previous works on RGB-D reconstruction are based on voxels, points, or meshes, which are either too computationally expensive to represent high-resolution 3D models, or not convenient for model regularization to deal with noise and errors. In this paper, we propose a new method to reconstruct 3D models with more accurate geometric information and better texture. Our method uses abstract surfaces consisting of different points with similar information as units of the model. To reduce the complexity, we use 2.5D heightmaps to represent each surface in the reconstructed model, making it convenient to do regularization. Experiments demonstrate the effectiveness of our method.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1158">MeasVRe: Measurement Tools for Unity VR Applications</h3>
    
<p> <small><strong style="color: black;"> Booth: D23 </strong></small> <br> </p>
    
    <p><i>Jolly Chen: University of Amsterdam&#59; Robert G. Belleman: University of Amsterdam</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/e2MZfG5DNPU" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1158" class="wrap-collabsible"> <input id="collapsibleabstractPO1158" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1158" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>An indispensable facility that is often missing in VR applications for studying complex 3D models, is the ability to take measurements. This paper describes a toolkit for Unity VR applications that allows for taking distance, angle, trace, surface area, and bounding box volume measurements by placing markers. We show that markers can be efficiently snapped to a model by ray casting. Moreover, we validated taking distance measurements with our toolkit, by performing an experiment where we measure inter-branch distances of corals. Lastly, the toolkit can save measurements locally or upload them to a logging server.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1161">Design of a VR Action Observation Tool for Rhythmic Coordination Training</h3>
    
<p> <small><strong style="color: black;"> Booth: D23 </strong></small> <br> </p>
    
    <p><i>James Jonathan Pinkl: University of Aizu&#59; Michael Cohen: University of Aizu</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=l209jE22g5A" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1161" class="wrap-collabsible"> <input id="collapsibleabstractPO1161" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1161" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Motor learning applications, particularly those with first-person virtual reality Action Observation features, have achieved positive results in a variety of fields. This project entails development of a first-person VR application designed to help musicians learn rhythm and the body coordination needed to express rhythm. Utilizing realtime tracking of user hand position, various accompanying visual media, and spatial audio with outside-the-head localization, the tool provides a new way to learn and improve rhythm.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1167">Automatic 3D Avatar Generation from a Single RBG Frontal Image</h3>
    
<p> <small><strong style="color: black;"> Booth: D22 </strong></small> <br> </p>
    
    <p><i>Alejandro Beacco: Universitat de Barcelona&#59; Jaime Gallego: University of Barcelona&#59; Mel Slater: University of Barcelona</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/JzsghODuZBg" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1167" class="wrap-collabsible"> <input id="collapsibleabstractPO1167" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1167" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We present a complete automatic system to obtain a realistic 3D avatar reconstruction of a person using only a frontal RGB image. Our proposed workflow first determines the pose, shape and semantic information from the input image. All this information is processed to create the skeleton and the 3D skinned textured mesh that conform the final avatar. We use a specific head reconstruction method to correctly match our final mesh to a realistic avatar. Our pipeline focuses on three main aspects: automation of the process, identification of the person, and usability of the avatar.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1168">MR-RIEW: An MR Toolkit for Designing Remote Immersive Experiment Workflows</h3>
    
<p> <small><strong style="color: black;"> Booth: D21 </strong></small> <br> </p>
    
    <p><i>Daniele Giunchi: University College London&#59; Riccardo Bovo: Imperial College London&#59; Anthony Steed: University College London&#59; Thomas Heinis: Imperial College</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/Ra9uwnenAxE" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1168" class="wrap-collabsible"> <input id="collapsibleabstractPO1168" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1168" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We present MR-RIEW, a toolkit for virtual and mixed reality that provides researchers with a dynamic way to design an immersive experiment workflow including instructions, environments, sessions, trials and questionnaires. It is implemented in Unity via scriptable objects, allowing simple customisation. The graphic elements, the scenes and the questionnaires can be selected and associated without code. MR-RIEW can save locally into the headset and remotely the questionnaire's answers. MR-RIEW is connected to Google Firebase service for the remote solution requiring a minimal configuration.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1173">Using Direct Volume Rendering for Augmented Reality in Resource-constrained Platforms</h3>
    
<p> <small><strong style="color: black;"> Booth: D23 </strong></small> <br> </p>
    
    <p><i>Berk Cetinsaya: University of Central Florida&#59; Carsten Neumann: University of Central Florida&#59; Dirk Reiners: University of Central Florida</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=4zpvQR9BtV0" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1173" class="wrap-collabsible"> <input id="collapsibleabstractPO1173" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1173" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Rendering a large volume is a challenging task on mobile and Augmented Reality (AR) devices due to lack of memory space and device limitations. Therefore, we implemented an Empty Space Skipping (ESS) optimization algorithm to render the high-quality large models on HoloLens. We designed and developed a system to visualize the computerized tomography (CT) scan data and Digital Imaging and Communications in Medicine (DICOM) files on Microsoft HoloLens 2. We used the Unity3D game engine to develop the system. As a result, we achieved about 10 times more frames per second (fps) on a high-quality model than the non-optimized version.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1174">Emotional Empathy and Facial Mimicry of Avatar Faces</h3>
    
<p> <small><strong style="color: black;"> Booth: D22 </strong></small> <br> </p>
    
    <p><i>Angela Saquinaula: Western Connecticut State University&#59; Adriel Juarez: Monmouth University&#59; Joe Geigel: Rochester Institute of Technology&#59; Reynold Bailey: Rochester Institute of Technology&#59; Cecilia Ovesdotter Alm: Rochester Institute of Technology</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=ar6LgzBntFE" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1174" class="wrap-collabsible"> <input id="collapsibleabstractPO1174" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1174" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We explore the extent to which empathetic reactions are elicited when subjects view 3D motion-capture driven avatar faces compared to viewing human faces. Through a remote study, we captured subjects' facial reactions when viewing avatar and humans faces, and elicited self reported feedback regarding empathy. Avatar faces varied by gender and realism. Results show no sign of facial mimicry&#59; only mimicking of slight facial movements with no solid consistency. Participants tended to empathize with avatars when they could adequately identify the stimulus' emotion. As avatar realism increased, it negatively impacted the subjects' feelings towards the stimuli.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1178">A Time Reversal Symmetry Based Real-time Optical Motion Capture Missing Marker Recovery Method</h3>
    
<p> <small><strong style="color: black;"> Booth: D21 </strong></small> <br> </p>
    
    <p><i>Dongdong Weng: Beijing Institute of Technology&#59; Yihan Wang: Beijing Institute of Technology&#59; Dong Li: Beijing Institute of Technology</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/1QLTzN3k4h8" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1178" class="wrap-collabsible"> <input id="collapsibleabstractPO1178" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1178" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This paper proposes a deep learning model based on time reversal symmetry for real-time recovery of continuous missing marker sequences in optical motion capture. This paper firstly uses time reversal symmetry of human motion as a constraint of the model. BiLSTM is used to describe the constraint and extract the bidirectional spatiotemporal features. This paper proposes a weight position loss function for model training, which describes the effect of different joints on the pose. Compared with the existing methods, the experimental results show that the proposed method has higher accuracy and good real-time performance.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1180">Interpersonal Distance to a Speaking Avatar: Loudness Matters Irrespective of Contents</h3>
    
<p> <small><strong style="color: black;"> Booth: D24 </strong></small> <br> </p>
    
    <p><i>Kota Takahashi: Toyohashi University of Technology&#59; Yasuyuki Inoue: Toyohashi University of Technology&#59; Michiteru Kitazaki: Toyohashi University of Technology</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/DV1v8-KbTXw" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1180" class="wrap-collabsible"> <input id="collapsibleabstractPO1180" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1180" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>It is important for us to maintain appropriate interpersonal distance depending on situations in effective and safe communications. We aimed to investigate the effects of speech loudness and clarity on the interpersonal distance towards an avatar in a virtual environment. We found that the louder speech of the avatar made the distance between the participants and the avatar larger than the quiet speech, but the clarity of the speech did not significantly affect the distance. These results suggest that the perception of loudness modulates the interpersonal distance towards the virtual avatar to maintain the intimate equilibrium.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1181">Let Every Seat Be Perfect! A Case Study on Combining BIM and VR for Room Planning</h3>
    
<p> <small><strong style="color: black;"> Booth: D38 </strong></small> <br> </p>
    
    <p><i>Wai Tong: The Hong Kong University of Science and Technology&#59; Haotian Li: The Hong Kong University of Science and Technology&#59; Huan Wei: The Hong Kong University of Science and Technology&#59; Liwenhan Xie: Hong Kong University of Science and Technology&#59; Yanna Lin: The Hong Kong University of Science and Technology&#59; Huamin Qu: The Hong Kong University of Science and Technology</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/-nz2LG7Gzio" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1181" class="wrap-collabsible"> <input id="collapsibleabstractPO1181" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1181" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>When communicating indoor room design, professional designers normally rely on software like Revit to export walk-through videos for their clients. However, a lack of in-situ experience restricts the ultimate users from evaluating the design and hence provides limited feedback, which may lead to a rework after actual construction. In this case study, we explore empowering end-users by exposing rich design details through a Virtual Reality (VR) application based on building an information model. Qualitative feedback in our user study shows promising results. We further discuss the benefits of the approach and opportunities for future research.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1182">A Skin Pressure-type Grasping Device to Reproduce Impulse Force for Virtual Ball Games</h3>
    
<p> <small><strong style="color: black;"> Booth: D38 </strong></small> <br> </p>
    
    <p><i>Kazuma Yoshimura: Nara Institute of Science and Technology&#59; Naoya Isoyama: Nara Institute of Science and Technology&#59; Hideaki Uchiyama: Nara Institute of Science and Technology&#59; Nobuchika Sakata: Ryukoku University&#59; Kiyoshi Kiyokawa: Nara Institute of Science and Technology&#59; Yoshihiro Kuroda: University of Tsukuba</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/V_xgCKtYs44" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1182" class="wrap-collabsible"> <input id="collapsibleabstractPO1182" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1182" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Tactile feedback is crucial to improve the realism of virtual sports, which are popular applications of virtual reality (VR). However, few wearable tactile feedback devices can produce an impulse force in midair. We propose a graspable impulse force presentation device by using a voice coil motor. In the evaluation experiment, the result suggests that the proposed device improves the sense of realism compared to a conventional VR controller with vibration.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1185">Virtual reality-based distraction on pain and performance during and after moderate-vigorous intensity cycling</h3>
    
<p> <small><strong style="color: black;"> Booth: D37 </strong></small> <br> </p>
    
    <p><i>Carly Wender: Kessler Foundation&#59; Phillip Tomporowski: University of Georgia&#59; Sun Joo (Grace) Ahn: University of Georgia&#59; Patrick O'Connor: University of Georgia</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/KlCCzJWAsc0" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1185" class="wrap-collabsible"> <input id="collapsibleabstractPO1185" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1185" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This experiment measured effects of visual perceptual load (PL) within immersive virtual reality (VR) on exercise-induced pain during cycling. Using a within-subjects design (n=43), participants cycled at a perceptually &quot;hard&quot; intensity for 10 minutes without VR (i.e., no PL - NPL) or with VR of low or high PL (i.e., LPL or HPL). Mean quadriceps pain was significantly lower in the NPL condition than either the LPL (d=0.472) or HPL conditions (d=0.391). Mean cycling performance was significantly greater during the LPL condition. Compared to traditional cycling (NPL), cycling in the LPL condition resulted in greater exercise performance despite greater pain.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1187">Virtual Touch Modulates Perception of Pleasant Touch</h3>
    
<p> <small><strong style="color: black;"> Booth: D37 </strong></small> <br> </p>
    
    <p><i>Gakumaru HARAGUCHI: Toyohashi University of Tecknology&#59; Michiteru Kitazaki: Toyohashi University of Technology</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/oa9gJ7Hq9tw" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1187" class="wrap-collabsible"> <input id="collapsibleabstractPO1187" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1187" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Pleasant touch by gentle stroking is processed by C-tactile afferents, and important for emotional and social touch in human communication. We aimed to investigate the effects of visual touch in a virtual environment on the perception of pleasantness of tactile touch. Five velocities were used for the tactile brushing and virtual brushing (0.3 - 30 cm/s), and participants answered the perceived pleasantness of the tactile touch irrespective of visual stimulus. We found that the perception of pleasant touch was significantly modulated by the velocity of visual brushing. Thus, the pleasant touch would be perceived by integrating vision and tactile perception.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1188">Evaluating 3D Visual Fatigue Induced by VR Headset Using EEG and Self-attention CNN</h3>
    
<p> <small><strong style="color: black;"> Booth: D36 </strong></small> <br> </p>
    
    <p><i>Haochen Hu: Beijing Institution of Technology&#59; Yue Liu: Beijing Institute of Technology&#59; Kang Yue: Beijing INstitute of Technology</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/0snuEkI5VfQ" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1188" class="wrap-collabsible"> <input id="collapsibleabstractPO1188" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1188" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>3D visual fatigue is one of the major factors that hinder the development of virtual reality contents towards larger population. We proposed an EEG-based self-attention CNN model to evaluate user's 3D visual fatigue in an end-to-end fashion. We adopted a wavelet-based convolution to extract spatiotemporal information and prevent overfitting. Besides, a self-attention layer was added to the feature extractor backbone to cope with the subject-variation problem in EEG-decoding. The proposed method is compared with four state-of-the-art methods, and the results demonstrate that our model has the best performance among all methods in subject-dependent and cross-subject scenarios.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1189">An Examination on Reduction of Displayed Character Shake while Walking in Place with AR Glasses</h3>
    
<p> <small><strong style="color: black;"> Booth: D36 </strong></small> <br> </p>
    
    <p><i>Hiromu Koide: Utsunomiya University&#59; Kei Kanari: college&#59; Mie Sato: Utsunomiya University</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/9OwTyee6uME" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1189" class="wrap-collabsible"> <input id="collapsibleabstractPO1189" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1189" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In recent years, augmented reality (AR) has started to be used in our daily lives. AR glasses are used when walking, which is a normal part of daily life, but walking causes the text displayed on the glasses to shake. This reduces both readability and our attention to what is in front of us, and increases discomfort. We propose a method of fixing the text to take account of shaking while walking to reduce these adverse effects. Experiments revealed the effectiveness of our reduction method and its influence on the distance of the text display.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1191">Virtual Human Coherence and Plausibility - Towards a Validated Scale</h3>
    
<p> <small><strong style="color: black;"> Booth: D35 </strong></small> <br> </p>
    
    <p><i>David Mal: University of W&uuml;rzburg, Department of Computer Science, HCI Group&#59; Erik Wolf: University of W&uuml;rzburg, Department of Computer Science, HCI Group&#59; Nina D&ouml;llinger: Human-Technology-Systems Group&#59; Mario Botsch: TU Dortmund University&#59; Carolin Wienrich: University W&uuml;rzburg&#59; Marc Erich Latoschik: Department of Computer Science, HCI Group</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/qG6L4mweT0w" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1191" class="wrap-collabsible"> <input id="collapsibleabstractPO1191" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1191" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Virtual humans contribute to users' state of plausibility in various XR applications. We present the development and preliminary evaluation of a self-assessment questionnaire to quantify virtual human's plausibility in virtual environments based on eleven concise items. A principal component analysis of 650 appraisals collected in an online survey revealed two highly reliable components within the items. We interpret the components as possible factors, i.e., appearance and behavior plausibility and match to the virtual environment, and propose future work aiming towards a standardized virtual human plausibility scale by validating the structure and sensitivity of both sub-components in XR environments.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1194">Democratic Video Pass-Through for Commercial Virtual Reality Devices</h3>
    
<p> <small><strong style="color: black;"> Booth: D36 </strong></small> <br> </p>
    
    <p><i>Diego Gonz&aacute;lez Mor&iacute;n: Nokia&#59; Francisco Pereira: Bell Labs&#59; Ester Gonzalez-Sosa: Bell Labs&#59; Pablo Perez: Bell Labs&#59; Alvaro Villegas: Bell Labs</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=F_u-sJZ5ONc" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1194" class="wrap-collabsible"> <input id="collapsibleabstractPO1194" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1194" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Video pass-through Extended Reality (XR) is rapidly gaining interest from developers and researchers. However, video pass-through enabled XR devices' ecosystem is still bounded to expensive hardware. In this paper, we describe our custom hardware and software setup for providing effective video pass-through capabilities to inexpensive commercial Virtual Reality (VR) devices. The proposed hardware setup incorporates a low-cost HD stereo camera rigidly hooked to the VR device using a custom 3D printed attachment. Our software solution, implemented in Unity, overcomes hardware-specific limitations, such as cameras' delays, in a simple yet successful manner.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1196">Perception of Symmetry of Actual and Modulated Self-Avatar Gait Movements During Treadmill Walking</h3>
    
<p> <small><strong style="color: black;"> Booth: D35 </strong></small> <br> </p>
    
    <p><i>Iris Willaert: Ecole de technologie superieure&#59; Rachid Aissaoui: CHUM research center&#59; Sylvie Nadeau: University of Montreal&#59; Cyril Duclos: Universit&eacute; de Montreal&#59; David Labbe PhD: Ecole de technologie superieure</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=EDTC8OcuHPA&ab_channel=IrisWillaert" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1196" class="wrap-collabsible"> <input id="collapsibleabstractPO1196" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1196" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In virtual reality, it is possible to simulate one's visual self-representation by mapping one's body movements to those of an avatar. Accepting the virtual body as part of one's own body creates an ownership illusion. This study aimed to assess the perception threshold between a subject's actual gait movements and those of their modulated self-avatar during treadmill walking.
Preliminary results on two subjects suggest that healthy subjects
can detect the mismatch, but differences may exist between
subjects</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1199">Bringing Real Body as Self-Avatar into Mixed Reality: A Gamified Volcano Experience</h3>
    
<p> <small><strong style="color: black;"> Booth: D37 </strong></small> <br> </p>
    
    <p><i>Diego Gonz&aacute;lez Mor&iacute;n: Nokia&#59; Ester Gonzalez-Sosa: Bell Labs&#59; Pablo Perez: Bell Labs&#59; Alvaro Villegas: Bell Labs</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/XiMmD1UzDiI" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1199" class="wrap-collabsible"> <input id="collapsibleabstractPO1199" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1199" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In this work we showcase a Mixed Reality experience that bring users' own body as a self-avatar. This is mainly based on an algorithm that segment in real time the egocentric full body placing a regular stereo camera in front of a commercial Virtual Reality device. To the best of our knowledge, this is the first work that integrates real body as self-avatar both meeting real-time and segmentation quality in real-life conditions. We conclude the paper with some preliminary subjective evaluation measuring Presence and Embodiment Factors, that suggests the potential of using your own body as self-avatar in Mixed Reality.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1200">A Replication Study to Measure the Perceived Three-Dimensional Location of Virtual Objects in Optical See Through Augmented Reality</h3>
    
<p> <small><strong style="color: black;"> Booth: E26 </strong></small> <br> </p>
    
    <p><i>Farzana Alam Khan: Mississippi State University&#59; Mohammed Safayet Arefin: Mississippi State University&#59; Nate Phillips: Mississippi State University&#59; J. Edward Swan II: Mississippi State University</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/w94ak9ukRKA" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1200" class="wrap-collabsible"> <input id="collapsibleabstractPO1200" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1200" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>An important research question in optical see-through (OST) augmented reality (AR) is, how accurately and precisely can a virtual object's real world location be perceived? Previously, a method was developed to measure the perceived three-dimensional location of virtual objects in OST AR. In this research, a replication study is reported, which examined whether the perceived location of virtual objects are biased in the direction of the dominant eye. The successful replication analysis suggests that perceptual accuracy is not biased in the direction of the dominant eye. Compared to the previous study's findings, overall perceptual accuracy increased, and precision was similar.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1204">Moving Soon? Rearranging Furniture using Mixed Reality</h3>
    
<p> <small><strong style="color: black;"> Booth: B31 </strong></small> <br> </p>
    
    <p><i>Shihao Song: Beijing Institute of Technology&#59; Yujia Wang: Beijing Institute of Technology&#59; Wei Liang: Beijing Institute of Technology&#59; Xiangyuan Li: Beijing Forestry University</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/6m-JRvDGkvA" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1204" class="wrap-collabsible"> <input id="collapsibleabstractPO1204" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1204" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We present a mixed reality (MR) system to help users with a houseful of furniture moving from an existing home into a new space, inheriting the preferences of furniture layout from the previous scene.<br>With the RGB-D cameras mounted on a mixed reality device, Microsoft HoloLens 2, our system first reconstructs the 3D model of the existing scene and leverages a deep learning-based approach to detect and to group objects. Then, our system generates a personalized furniture layout by optimizing a cost function, incorporating the analyzed relevance of between and within groups, and the spatial constraints of the new layout.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1205">Add-on Occlusion: An External Module for Optical See-through Augmented Reality Displays to Support Mutual Occlusion</h3>
    
<p> <small><strong style="color: black;"> Booth: D42 </strong></small> <br> </p>
    
    <p><i>Yan Zhang: Nara Institute of Science and Technology&#59; Kiyoshi Kiyokawa: Nara Institute of Science and Technology&#59; Naoya Isoyama: Nara Institute of Science and Technology&#59; Hideaki Uchiyama: Nara Institute of Science and Technology&#59; Xubo Yang: SHANGHAI JIAO TONG UNIVERSITY</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/_yom8z7jKvE" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1205" class="wrap-collabsible"> <input id="collapsibleabstractPO1205" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1205" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>The occlusion function benefits augmented reality (AR) in many aspects. However, existing occlusion-capable optical see-through augmented reality (OC-OST-AR) displays are designed by integrating virtual displays into a dedicated occlusion-capable architecture, hereby, we miss merits from emerging OST-AR displays. In this article, we propose an external occlusion module that can be added to common OST-AR displays. Per-pixel occlusion is supported with a small form-factor by using polarization-based optical path compression. The occlusion function can be switched on/off by controlling the incident light polarization. A prototype within a volume of 6x6x3cm is built. A preliminary experiment proves that occlusion is realized successfully.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1207">Knowing the Partner's Objective Increases Embodiment towards a Limb Controlled by the Partner</h3>
    
<p> <small><strong style="color: black;"> Booth: D35 </strong></small> <br> </p>
    
    <p><i>Harin Manujaya Hapuarachchi: Toyohashi University of Technology&#59; Michiteru Kitazaki: Toyohashi University of Technology</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/Ht86aJO09Vo" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1207" class="wrap-collabsible"> <input id="collapsibleabstractPO1207" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1207" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We have developed a joint avatar of which two participants simultaneously control left and right-side limbs in first-person view in a virtual environment. We aimed to investigate whether having a common objective with a partner affects sense of embodiment towards a limb controlled by the partner. Participants performed reaching tasks using the joint avatar. We found that the embodiment towards the arm controlled by the partner was significantly higher when the participant dyads shared a common objective or when they were allowed to see their partner's goal, compared to when their partner's goal was unknown to them.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1213">Assist Home Training Table Tennis Skill Acquisition via Immersive Learning and Web Technologies</h3>
    
<p> <small><strong style="color: black;"> Booth: D41 </strong></small> <br> </p>
    
    <p><i>Jian-Jia Weng: National Tsing Hua University&#59; Yu-Hsin Wang: National Tsing Hua University&#59; Calvin Ku: National Tsing Hua University&#59; Dong-Xian Wu: National Tsing Hua University&#59; Yi-Min Lau: National Tsing Hua University&#59; Wan-Lun Tsai: National Cheng Kung University&#59; Tse-Yu Pan: National Tsing Hua University&#59; Min-Chun Hu: National Tsing Hua University&#59; Hung-Kuo Chu: National Tsing Hua University&#59; Te-Cheng Wu: National Tsing Hua University</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=oQF9e4T2rhM" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1213" class="wrap-collabsible"> <input id="collapsibleabstractPO1213" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1213" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Sports applications in Virtual Reality (VR) have become immensely popular for training skill-based sports like table tennis. However, the existing researches do not focus on designing an intuitive system for efficient communication between the trainee and the coach. We developed a VR table tennis training system for table tennis skill acquisition that focuses on helping coaches to convey a player's mistake clearly. Our system consists of a VR training system where trainees can learn a skill gradually and a web-based feedback annotative tool for coaches. Trainees can examine their mistakes through a tablet or an immersive VR world.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1215">On the Effectiveness of Conveying BIM Metadata in VR Design Reviews for Healthcare Architecture</h3>
    
<p> <small><strong style="color: black;"> Booth: D41 </strong></small> <br> </p>
    
    <p><i>Emma Buchanan: University of Canterbury&#59; Giuseppe Loporcaro: University of Canterbury&#59; Stephan Lukosch: University of Canterbury</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/MQkaOB4BlaQ" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1215" class="wrap-collabsible"> <input id="collapsibleabstractPO1215" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1215" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This research seeks to assess whether Virtual Reality (VR) can be used to convey Building Information Modelling (BIM) metadata alongside geometric and spatial data in a virtual environment, and by doing so, determine if this increases the understanding of the design by the stakeholder. A user study assessed participants performance and preference for conducting design reviews in VR or using a traditional design review system of PDF drawings and a 3D model. Early results indicate VR was preferred with fewer errors made during assessment and a higher System Usability Scale SUS score.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1218">Towards a Virtual Reality Math Game for Learning In Schools - A User Study</h3>
    
<p> <small><strong style="color: black;"> Booth: D42 </strong></small> <br> </p>
    
    <p><i>Meike Belter Belter: University of Canterbury&#59; Heide Lukosch: University of Canterbury</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/D1_w_q8Xur4" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1218" class="wrap-collabsible"> <input id="collapsibleabstractPO1218" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1218" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In recent years, immersive Virtual Reality (VR) has gained popularity among young users as a new technology for entertainment gaming. While VR remains majorly used for entertainment purposes, 3D desktop games are already used in schools. This study takes a closer look at the suitability for VR games to be used in a formal educational environment, and its potential to enrich existing game based learning approaches. Based on learning needs of in particular easily distracted and inattentive children, an immersive VR math game was created and tested on 15 children aged 11-12.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1219">Motion Correction of Interactive CG Avatars Using Machine Learning</h3>
    
<p> <small><strong style="color: black;"> Booth: D43 </strong></small> <br> </p>
    
    <p><i>Ko Suzuki: Utsunomiya University&#59; Hiroshi Mori: Utsunomiya University&#59; Fubito Toyama: Utsunomiya University</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=7mX7fx44ljM" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1219" class="wrap-collabsible"> <input id="collapsibleabstractPO1219" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1219" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Motion capture allows users to control their CG avatar via their own movements. However, the composed avatar motion fails to deliver the actual input movements if the user's motion information is not accurately captured due to measurement errors. In this paper, we propose a method that complements a user's motion according to the motion of another person for a two-party motion with interaction. This method is expected to compose avatar motions that look natural to the other person while emphasizing the actual motions of the user.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1220">Adding Difference Flow between Virtual and Actual Motion to Reduce Sensory Mismatch and VR Sickness while Moving</h3>
    
<p> <small><strong style="color: black;"> Booth: D44 </strong></small> <br> </p>
    
    <p><i>Kwan Yun: Korea University&#59; Gerard Jounghyun Kim: Korea University</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=R7cAmW5iyMU&ab_channel=kwanyun" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1220" class="wrap-collabsible"> <input id="collapsibleabstractPO1220" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1220" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Enjoying Virtual Reality in vehicles presents a problem because of the sensory mismatch and sickness. While moving, the vestibular sense perceives actual motion in one direction, and the visual sense, visual motion in another. We propose to zero out such physiological mismatch by mixing in motion information as computed by the difference between those of the actual and virtual, namely, &quot;Difference&quot; flow. We present the system for computing and visualizing the difference flow and validate our approach through a small pilot field experiment. Although tested only with a low number of subjects, the initial results are promising.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1224">Who will Trust my Digital Twin? Maybe a Clerk in a Brick and Mortar Fashion Shop</h3>
    
<p> <small><strong style="color: black;"> Booth: D38 </strong></small> <br> </p>
    
    <p><i>Lorenzo Stacchio: University of Bologna&#59; Michele Perlino: University of Bologna&#59; Ulderico Vagnoni: University of Bologna&#59; Federica Sasso: Independent Artist&#59; Claudia Scorolli: University of Bologna&#59; Gustavo Marfia: University of Bologna</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=R2LmGI1hLuA&list=PL9DFRaAYy0LCb964qGPmndEr3QYqvSofk" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1224" class="wrap-collabsible"> <input id="collapsibleabstractPO1224" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1224" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Digital Twin (DT) models mirror the life of physical entities and are adopted to optimize several industrial processes. Although well-established in the industrial fields, one of the most exciting examples of where DTs may be employed is in the MetaVerse, with Human Digital Twins (HDTs). We present a preliminary study that examines the efficacy of HDT-human interactions in the context of a fashion shop. Based on the results obtained involving thirty-two participants in our experiments, we begin a discussion related to the pros and cons of this approach on x-commerce.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1225">Event Synthesis for Light Field Videos using Recurrent Neural Networks</h3>
    
<p> <small><strong style="color: black;"> Booth: E31 </strong></small> <br> </p>
    
    <p><i>Zhicheng Lu: The University of Sydney&#59; Xiaoming Chen: Beijing Technology and Business University&#59; Yuk Ying Chung: The University of Sydney&#59; Sen Liu: University of Science and Technology of China</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=lfTb8FNg-wc" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1225" class="wrap-collabsible"> <input id="collapsibleabstractPO1225" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1225" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Light field videos (LFVs) yield higher complexity in performing computer vision tasks. The emerging event cameras offer new means for light-weight processing of LFVs, but it is infeasible to build an event camera array due to their high costs. In this poster, we propose a novel &quot;event synthesis for light field videos&quot; (ES4LFV) model by using recurrent neural networks and build a preliminary dataset. The ES4LFV can synthesize events for light field videos from a LFV camera array and single event camera. The experimental results show that ES4LFV outperforms the traditional method by 3.1dB in PSNR.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1227">Towards Controlling Whole Body Avatars with Partial Body-Tracking and Environmental Information</h3>
    
<p> <small><strong style="color: black;"> Booth: E28 </strong></small> <br> </p>
    
    <p><i>Koji Yamada: Utsunomiya University&#59; Hiroshi Mori: Utsunomiya University&#59; Fubito Toyama: Utsunomiya University</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/qabKBsbDVZI" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1227" class="wrap-collabsible"> <input id="collapsibleabstractPO1227" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1227" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In body-tracking-based avatar manipulation, the user's motion is reflected in their avatar, which creates a high level of immersion. Conversely, the user is required to give a detailed performance similar to that of the avatar to be composed.<br>In this research, we aim to produce avatar motion that reflects the user's intended actions and maintains consistency with the VR environment by inputting the user's posture. The avatar's body motion was generated by inputting the user's motion and VR environmental information into the motion configuration network, which was developed using machine learning.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1230">Measuring Virtual Object Location with X-Ray Vision at Action Space Distances</h3>
    
<p> <small><strong style="color: black;"> Booth: D44 </strong></small> <br> </p>
    
    <p><i>Nate Phillips: Mississippi State University&#59; Farzana Alam Khan: Mississippi State University&#59; Mohammed Safayet Arefin: Mississippi State University&#59; Cindy Bethel: Mississsippi State University&#59; J. Edward Swan II: Mississippi State University</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/8MX2051CyV4" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1230" class="wrap-collabsible"> <input id="collapsibleabstractPO1230" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1230" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Accurate and usable x-ray vision is a significant goal in augmented reality (AR) development. X-ray vision, or the ability to comprehend location and object information when it is presented through an opaque barrier, needs to successfully convey scene information to be a viable use case for AR. Further, this investigation should be performed in an ecologically valid context in order to best test x-ray vision. This research seeks to experimentally evaluate the perceived object location of stimuli presented with x-ray vision, as compared to real-world perceived object location through a window, at action space distances of 1.5 to 15 meters.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1231">Preliminary evaluation of an IVR user experience design model using eye-tracking attention measurements</h3>
    
<p> <small><strong style="color: black;"> Booth: D43 </strong></small> <br> </p>
    
    <p><i>Elena Dzardanova: University of the Aegean&#59; Vlasios Kasapakis: University of the Aegean</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=c1KOXyLNtJM" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1231" class="wrap-collabsible"> <input id="collapsibleabstractPO1231" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1231" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>The present study drafts a simplified IVR user experience design model to guideline a preliminary evaluation of attention variance for semantically distinct elements. 27 participants freely explored an interactive state of the art virtual setting, whilst equipped with eye-tracking technology which procured attention duration measurements. Initial results confirm significant element attention discrepancy and provide the first indication toward a more detailed categorical organization of experience components for follow-up experimentation.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1236">Touch the History in Virtuality: Combine Passive Haptic with 360&deg; videos in history learning</h3>
    
<p> <small><strong style="color: black;"> Booth: E23 </strong></small> <br> </p>
    
    <p><i>YanXiang Zhang: University of Science and Technology of China&#59; YingNa Wang: University of Science and Technology of China&#59; QingQin Liu: University of Science and Technology of China</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=55YnY0zQi0U" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1236" class="wrap-collabsible"> <input id="collapsibleabstractPO1236" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1236" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Based on ethical principles and Asimov's three laws of robotics, this article discusses three ethical issues generated by the use of virtual reality to &quot;revive&quot; or come into contact with the deceased, including &quot;not harm humans,&quot; &quot;rights-related issues,&quot; and &quot;fairness and meaning.&quot; And religious factors are also taken into consideration. It is necessary to predict the ethical risk of virtual reality &quot; revive &quot; of the deceased, which will make ethics play a better role in its future development. In addition, it will help stakeholders to pay more attention to the ethical issues involved in virtual avatars.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1239">The Sloped Shoes: Influence Human Perception of the Virtual Slope</h3>
    
<p> <small><strong style="color: black;"> Booth: E22 </strong></small> <br> </p>
    
    <p><i>YanXiang Zhang: University of Science and Technology of China&#59; JiaLing Wu: University of Science and Technology of China&#59; QingQin Liu: University of Science and Technology of China</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=NQQlxA2YAJI" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1239" class="wrap-collabsible"> <input id="collapsibleabstractPO1239" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1239" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In this study, people were allowed to walk uphill or downhill in virtual environments by changing the slope of shoes, while they walked on a flat wooden board in physical environments. We can explore the impact of the shoe slope on users' perception of walking uphill or downhill in the virtual world. We find that the slope of the shoes affects participants' perception, increasing their sense of realism when walking uphill and downhill in the virtual world. With changing the shoe slope, participants' perception of the possibility of walking uphill or downhill will change and establish corresponding detection thresholds.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1240">Geometric Calibration with Multi-Viewpoints for Multi-Projector Systems on Arbitrary Shapes Using Homography and Pixel Maps</h3>
    
<p> <small><strong style="color: black;"> Booth: E21 </strong></small> <br> </p>
    
    <p><i>Atsuya Ueno: Wakayama University &#59; Toshiyuki Amano: Wakayama University&#59; Chisato Yamauchi: Misato astronomical observatory</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/OnK5vSUdDUw" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1240" class="wrap-collabsible"> <input id="collapsibleabstractPO1240" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1240" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We propose a geometrical calibration method for the MISATO astronomical observatory in Japan. The primary objective of our projection system is enabling a large-scale geometrical projection calibration for near-planner arbitrarily-shaped ground projection with a multi-projector system via temporally-placed camera capturing. The obtained results exhibited an average distortion reduction of 84.7&#37; less than the simple homography-based method.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1243">Redirected Walking in 360&deg; Video: Effect of Environment Size on Detection Thresholds for Translation and Rotation Gains</h3>
    
<p> <small><strong style="color: black;"> Booth: E21 </strong></small> <br> </p>
    
    <p><i>YanXiang Zhang: University of Science and Technology of China&#59; QingQin Liu: University of Science and Technology of China&#59; YingNa Wang: University of Science and Technology of China</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=RHHEWZ6gE54" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1243" class="wrap-collabsible"> <input id="collapsibleabstractPO1243" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1243" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Using real walking to control the playback of the 360&deg; videos is a natural and immersive way to match visual and self-motion perception. Redirected walking can enable users to walk in limited physical tracking space but experience larger scenes. Environment size may affect user perception in 360&deg; videos. We conducted a user study about the detection thresholds (DTs) for translation and rotation gains in 360&deg; video-based virtual environments in three scenes with different widths. Results show that environment size of the scene increases the DTs for both lower and upper translation gains but doesn't affect the DTs for rotation gains.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1246">Movement Augmentation in Virtual Reality: Impact on Sense of Agency Measured by Subjective Responses and Electroencephalography</h3>
    
<p> <small><strong style="color: black;"> Booth: E26 </strong></small> <br> </p>
    
    <p><i>Liu Wang: Xi'an Jiaotong-Liverpool University&#59; Mengjie Huang: Xi'an Jiaotong-Liverpool University&#59; Chengxuan Qin: Xi'an Jiaotong-Liverpool University&#59; Yiqi Wang: University College London&#59; Rui Yang: Xi'an Jiaotong-Liverpool University</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=xuUb2Fn-QCw" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1246" class="wrap-collabsible"> <input id="collapsibleabstractPO1246" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1246" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Virtual movement augmentation, which refers to the visual amplification of remapped movement, shows potential to be applied in motion-related virtual reality programs. Sense of agency (SoA), which measures the user's feeling of control in their action, has not been fully investigated for augmented movement. This study investigated the effect of augmented movement at three different levels (baseline, medium, and high) on users' SoA using both subjective responses and electroencephalography (EEG). Results show that SoA can be boosted slightly at medium augmentation level but drops at high level. The augmented virtual movement only helps to enhance SoA to a certain extent.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1247">Bouncing Seat: An Immersive Virtual Locomotion Interface with LSTM Based Body Gesture Estimation</h3>
    
<p> <small><strong style="color: black;"> Booth: E22 </strong></small> <br> </p>
    
    <p><i>Yoshikazu Onuki: Digital Hollywood University</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/QyitgAwqEJc" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1247" class="wrap-collabsible"> <input id="collapsibleabstractPO1247" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1247" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>A pilot study of the bouncing seat system is presented. This system consists of an air cushion and gravicorder. The former enables users to move easier and feel lively motion. The latter and its computing units detect users' body sway and recognize its gestures. Four commands (walk, right turn, left turn, and jump) and associated intuitive body gestures were designed. The estimator based on the multi-timescale LSTM was trained using newly created body gesture dataset and achieved 88&#37; inference accuracy. Results suggest that our system has potential for providing a higher sense of immersion and enjoyment than the joysticks.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1249">Hype Live: Biometric-based Sensory Feedback for Improving the Sense of Unity in VR Live Performance</h3>
    
<p> <small><strong style="color: black;"> Booth: E23 </strong></small> <br> </p>
    
    <p><i>Masashi Abe: Nara Institute of Science and Technology&#59; Akiyoshi Takuto: Nara Institute of Science and Technology&#59; Isidro Mendoza Butaslac III: Nara Institute of Science and Technology&#59; Zhou Hangyu: Nara Institute of Science and Technology&#59; Taishi Sawabe: Nara Institute of Science and Technology</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=mu7IPN9rk8o" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1249" class="wrap-collabsible"> <input id="collapsibleabstractPO1249" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1249" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We propose Hype Live, a system to improve the sense of unity by sharing responses through visual, auditory, and haptic stimuli based on the biometrics of the participants in VR live performances. In this field, the sharing of reactions among the participants is one of the most important factors in improving the sense of unity. However, not many past studies have provided feedback to the participants' senses. Therefore, as a prototype of Hype Live, we used a vibration device to provide haptic feedback and recreated moshing scenes where the participants violently collide with each other like a real live performance.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1250">Sense of Agency on Handheld AR for Virtual Object Translation</h3>
    
<p> <small><strong style="color: black;"> Booth: E14 </strong></small> <br> </p>
    
    <p><i>Wenxin Sun: Xi'an Jiaotong-Liverpool University&#59; Mengjie Huang: Xi'an Jiaotong-Liverpool University&#59; Chenxin Wu: Xi'an Jiaotong-Liverpool University&#59; Rui Yang: Xi'an Jiaotong-Liverpool University</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=LI5yXT10jdo" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1250" class="wrap-collabsible"> <input id="collapsibleabstractPO1250" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1250" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Handheld augmented reality (AR) interfaces are applied in various programs nowadays, and the degrees of freedom (DoF) of translation modes have become significant on these interfaces. Sense of agency (SoA), emphasizing one's feeling of control, has emerged as an essential index of user experience. However, little was known about users' feelings of control with different translation modes in literature. Hence, this paper focuses on users' SoA in different translation modes by assessing subjective and objective measures. Correlations between SoA and translation modes were explored on handheld AR interfaces, revealing that the 1DoF translation mode was associated with higher SoA.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1251">A Location-Triggered Augmented Reality Walking Tour Using Snap Spectacles 2021</h3>
    
<p> <small><strong style="color: black;"> Booth: E27 </strong></small> <br> </p>
    
    <p><i>AadilMehdi Sanchawala: Columbia University&#59; Mara Dimofte: Columbia University&#59; Steven Feiner: Columbia University</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/1-baRouqURk" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1251" class="wrap-collabsible"> <input id="collapsibleabstractPO1251" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1251" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We present an on-site 3D-animated audiovisual tour guide augmented reality application developed for Snap Spectacles 2021. The primary goal of this project is to explore how to use this experimental product to create an augmented reality tour guide. In addition, we present the design considerations for the user interface and the underlying system architecture. We illustrate the workflow of the tour application and discuss our experience working with Spectacles 2021 and its experimental API. We also present our design choices and directions for future work.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1252">User-Defined Interaction Using Everyday Objects for Augmented Reality First Person Action Games</h3>
    
<p> <small><strong style="color: black;"> Booth: E24 </strong></small> <br> </p>
    
    <p><i>Mac Greenslade: University of Canterbury&#59; Adrian James Clark: University of Canterbury&#59; Stephan Lukosch: University of Canterbury</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/szgK1-omCeM" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1252" class="wrap-collabsible"> <input id="collapsibleabstractPO1252" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1252" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In this paper, we present an elicitation study to explore how people use everyday objects for augmented reality first person games. 24 participants were asked to select items from a range of everyday objects to use as controllers for three different classes of virtual object. Participants completed tasks using their selected items and rated the experience using the Augmented Reality Immersion (ARI) questionnaire. Results indicate no strong consensus linking any specific everyday object to any virtual object across our testing population. Based on these findings, we recommend developers provide the ability for users to choose the everyday objects they prefer.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1254">Proposing the RecursiVerse Overlay Application for the MetaVerse</h3>
    
<p> <small><strong style="color: black;"> Booth: D42 </strong></small> <br> </p>
    
    <p><i>Lorenzo Donatiello: University of Bologna&#59; Gustavo Marfia: University of Bologna</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=W96qFXljEBg&list=PL9DFRaAYy0LCb964qGPmndEr3QYqvSofk&index=4" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1254" class="wrap-collabsible"> <input id="collapsibleabstractPO1254" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1254" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In a still uncertain future for the MetaVerse, we dare to envision one of its possible future developments, the RecursiVerse. The RecursiVerse is an overlay application that may be built upon the MetaVerse, amounting to symmetrical virtual-real space where a human being may rely on human digital twins which may move, operate and recursively replicate to collaboratively perform multiple tasks. The RecursiVerse may hence extend what will eventually be possible thanks to the MetaVerse, providing a service to a society that poses increasing cognitive and perceptual challenges due to growing work-life imbalances and increasing cognitive loads.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1256">AmbientTransfer: Presence Enhancement by Converting Video Ambient to Users' Somatosensory Feedback</h3>
    
<p> <small><strong style="color: black;"> Booth: E27 </strong></small> <br> </p>
    
    <p><i>Xunshi Li: School of Computer Science and Engineering, Beijing Technology and Business University&#59; Xiaoming Chen: Beijing Technology and Business University&#59; Yuk Ying Chung: The University of Sydney&#59; Qiang Qu: The University of Sydney</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=xSy7YdeFN6o" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1256" class="wrap-collabsible"> <input id="collapsibleabstractPO1256" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1256" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Haptic feedback can improve users' presence when watching immersive videos. In this work, we present our &quot;AmbientTransfer&quot; framework for demonstrating how to &quot;embed&quot; users in the video ambient with somatosensory feedback. Particularly, AmbientTransfer can obtain and convert video ambient factors, e.g., rain intensities, into dynamic haptic feedback by slight electrostimulation in various levels. Then AmbientTransfer maps various levels of haptic feedback to different body parts of users wearing the emerging electrostimulation suit. Preliminary experimental results show that AmbientTransfer can considerably enhance the users' presence. We believe that AmbientTransfer is worth of further exploration for exploiting its full potentials.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1257">Comparing Physiological and Emotional Effects of Happy and Sad Virtual Environments Experienced in Video and Virtual Reality</h3>
    
<p> <small><strong style="color: black;"> Booth: E26 </strong></small> <br> </p>
    
    <p><i>Yuankun Zhu: University of Queensland&#59; Arindam Dey: University of Queensland</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=s8TwrhjWBmk" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1257" class="wrap-collabsible"> <input id="collapsibleabstractPO1257" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1257" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Virtual Reality (VR) could give users a more immersive experience than other non-immersive mediums. In this study, we explored differences in emotional and physiological effects between videos and VR using two different sets of contents to evoke happy and sad emotions. In this within-subjects controlled experiment we collected real-time heart rate and positive and negative affect schedule (PANAS) to measure physiological and emotional effects. Our results showed that VR triggers stronger emotions and higher heart rate than videos.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1259">Toward Understanding the Effects of Visual and Tactile Stimuli to Reduce the Sensation of Movement with XR Mobility Platform</h3>
    
<p> <small><strong style="color: black;"> Booth: E25 </strong></small> <br> </p>
    
    <p><i>Taishi Sawabe: Nara Institute of Science and Technology&#59; Masayuki Kanbara: Nara Institute of Science and Technology&#59; Yuichiro Fujimoto: Nara Institute of Science and Technology&#59; Hirokazu Kato: Nara Institute of Science and Technology</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/nPjwXP84bmQ" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1259" class="wrap-collabsible"> <input id="collapsibleabstractPO1259" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1259" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This paper investigates a reduction method for passenger's movement sensation with the XR mobility platform mounted on an autonomous vehicle to improve passenger comfort during auto-driving. We investigate a reduction method that controls passenger's sense of movement by controlling visual and tactile perception using a multimodal XR mobility platform which consists of an immersive display and a motion platform with a tilting seat. The result of 30 subjects shows the sense of movement perceived by the passenger was reduced significantly when both visual acceleration and tactile acceleration control method was activated inside a moving autonomous vehicle.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1263">Augmented Reality In-Field Observation Creation and Visualization in Underperforming Areas</h3>
    
<p> <small><strong style="color: black;"> Booth: D41 </strong></small> <br> </p>
    
    <p><i>Mengya Zheng: University College Dublin&#59; Nestor Velasco Bermeo: University College Dublin&#59; Abraham G. Campbell: University College Dublin</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/9x0qwL9lPvs" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1263" class="wrap-collabsible"> <input id="collapsibleabstractPO1263" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1263" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>To precisely diagnose and address farming issues that led to crop yield underperformance, agronomists need to conduct field trips at the targeted underperforming areas and record the observed issues to support crop treatment decisions. Traditional field data visualization tools may not provide intuitive visualizations to support field trip investigations at targeted underperforming areas. This paper presents an Augmented Reality (AR) in-field observation tool to navigate agronomists to annotate the observed issues at precisely targeted underperforming areas. These recorded observation AR annotations are then synchronized in a Web-based Interactive Multi-Layer Mapping Tool for a complete precision farming decision-making process.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1264">Jamming in MR: Towards Real-Time Music Collaboration in Mixed Reality</h3>
    
<p> <small><strong style="color: black;"> Booth: E23 </strong></small> <br> </p>
    
    <p><i>Ruben Schlagowski: University of Augsburg&#59; Kunal Gupta: The University of Auckland&#59; Silvan Mertes: University of Augsburg&#59; Mark Billinghurst: University of Auckland&#59; Susanne Metzner: University of Augsburg&#59; Elisabeth Andr&eacute;: University of Augsburg</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/4yoZ0RHB1p8" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1264" class="wrap-collabsible"> <input id="collapsibleabstractPO1264" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1264" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Recent pandemic-related contact restrictions have made it difficult for musicians to meet in person to make music. As a result, there has been an increased demand for applications that enable remote and real-time music collaboration. One desirable goal here is to give musicians a sense of social presence, to make them feel that they are &quot;on site&quot; with their musical partners. We conducted a focus group study to investigate the impact of remote jamming on users' affect. Further, we gathered user requirements for a Mixed Reality system that enables real-time jamming and developed a prototype based on these findings.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1265">Creating 3D Personal Avatars with High Quality Facial Expressions for Telecommunication and Telepresence</h3>
    
<p> <small><strong style="color: black;"> Booth: E32 </strong></small> <br> </p>
    
    <p><i>Michal Joachimczak: NICT UCRI&#59; Juan Liu: NICT UCRI&#59; Hiroshi Ando: NICT UCRI</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/fN5D6lwXfLc" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1265" class="wrap-collabsible"> <input id="collapsibleabstractPO1265" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1265" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This study aims at providing a low-cost solution for telepresence where people are reconstructed as 3D avatars using an ordinary webcam, while still exhibiting abundant facial information (such as micro-expressions) that are critical for face-to-face communication. We estimate the basic 3D shape and texture of the body from a set of video frames, and then subsequently update its body pose, facial expression, and facial texture in each frame. Our method is expected to reduce the entry barrier of VR systems and create an embodied telecommunication that conveys rich information and subtle emotional changes to deepen mutual understanding at a distance.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1266">Video2Force: Experiencing Object Motion in Video with Dynamic Force Feedback based on Bio-Inspired Sensing and Processing</h3>
    
<p> <small><strong style="color: black;"> Booth: E28 </strong></small> <br> </p>
    
    <p><i>Guangxin Zhao: School of Computer Science and Engineering, Beijing Technology and Business University&#59; Zhaobo Wang: The University of Sydney&#59; Xiaoming Chen: Beijing Technology and Business University&#59; Zhicheng Lu: The University of Sydney&#59; Yuk Ying Chung: The University of Sydney&#59; Haisheng LI: Beijing Technology and Business University</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=fYy3WRRLCHA" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1266" class="wrap-collabsible"> <input id="collapsibleabstractPO1266" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1266" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In this work, we propose Video2Force, a framework that can estimate the video object motion and its resulting &quot;force&quot; by leveraging the emerging bio-inspired vision sensors for low-complexity and low-latency visual information processing. While the user watches video, the estimated force is &quot;delivered&quot; to the user's sensation with dynamic force feedback via emerging haptic gloves, synchronized with the video content. Consequently, the user is enabled to experience the &quot;virtual existence&quot; of the video object and its motion. Preliminary experimental results demonstrate that Video2Force is feasible and can enhance the users' presence in video experience.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1267">Effects of the Level of Detail on the Recognition of City Landmarks in Virtual Environments</h3>
    
<p> <small><strong style="color: black;"> Booth: E22 </strong></small> <br> </p>
    
    <p><i>Achref Doula: TU Darmstadt&#59; Philipp Kaufmann: TU Darmstadt&#59; Alejandro Sanchez Guinea: TU Darmstadt&#59; Max M&uuml;hlh&auml;user: TU Darmstadt</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/OZIaR3Ha4fM" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1267" class="wrap-collabsible"> <input id="collapsibleabstractPO1267" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1267" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>The reconstruction of city landmarks is central to creating recognizable virtual environments representing real cities. Despite the recent advances, it is still not clear what level of detail (LOD) to adopt when reconstructing those landmarks for their correct recognition, and if particular architectural styles represent specific challenges in this respect. In this paper, we investigate the effect of LOD on landmark recognition, generally, and on some architectural styles, specifically. The results of our user study show that higher LOD lead to a better landmark identification. Particularly, Neoclassical-style buildings need more details to be individually distinguished from similar ones.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1268">Facial emotion recognition analysis using deep learning through RGB-D imagery of VR participants through partially occluded facial types</h3>
    
<p> <small><strong style="color: black;"> Booth: E21 </strong></small> <br> </p>
    
    <p><i>Ian Mills: Walton Institute for Information and Communication Systems Science&#59; Frances cleary: Walton Institute for Information and Communication Systems Science</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/8aUmRQV0QH8" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1268" class="wrap-collabsible"> <input id="collapsibleabstractPO1268" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1268" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This research poster outlines the initial development of a facial emotion recognition (FER) evaluation system based on RGB-D imagery captured via a mobile based device. The study outlined features control group of non-occluded facial types and a set of participants wearing a head mounted display (HMD) in order to demonstrate an occluded facial type. We explore an architecture to develop a FER system that is suitable for occluded facial analysis. The paper details the methodology, experimental design and future work to be carried out to deliver such a system.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1272">Immersive Visualization of Sneeze Simulation Data on Mobile Devices</h3>
    
<p> <small><strong style="color: black;"> Booth: E33 </strong></small> <br> </p>
    
    <p><i>Liangding Li: University of Central Florida&#59; Douglas Fontes: University of Central Florida&#59; Carsten Neumann: University of Central Florida&#59; Dirk Reiners: University of Central Florida&#59; Carolina Cruz-Neira: University of Central Florida&#59; Michael Kinzel: University of Central Florida</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=X18fi9krrBg&list=PLZtzOwu_ts4gnc-wixx1a_FRC8x8YcehP" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1272" class="wrap-collabsible"> <input id="collapsibleabstractPO1272" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1272" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>One key factor in stopping the spread of COVID-19 is practicing social distancing. Visualizing possible sneeze droplets' transmission routes in front of an infected person might be an effective way to help people understand the importance of social distancing. This paper presents a mobile virtual reality (VR) interface that helps people visualize droplet dispersion from the target person's view. We implemented a VR application to visualize and interact with the sneeze simulation data immersively. Our application provides an easy way to communicate the correlation between social distance and infected droplets exposure, which is difficult to achieve in the real world.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1273">Irish Sign Language in a Virtual Reality Environment</h3>
    
<p> <small><strong style="color: black;"> Booth: E27 </strong></small> <br> </p>
    
    <p><i>Ryan McCloskey: Walton Institute for Information and Communication Systems Science</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/zTb1iuJt-5w" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1273" class="wrap-collabsible"> <input id="collapsibleabstractPO1273" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1273" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This paper describes a study whereby groups of users are tasked with learning Irish Sign Language (ISL) gestures in distinct environments and performing a comparison of outcomes to determine the general effectiveness of each method. One groups of users are tasked with learning via traditional tuition methods, and the second groups is placed in a virtual reality environment with a custom training module for teaching ISL gestures. This paper discusses such a study and details the processes, motivations and potential future work.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1274">A validation study to trigger nicotine craving in virtual reality</h3>
    
<p> <small><strong style="color: black;"> Booth: E28 </strong></small> <br> </p>
    
    <p><i>Chun-Jou Yu: Goldsmiths&#59; Aitor Rovira: Oxford Health NHS Foundation Trust&#59; Xueni Pan: Goldsmiths&#59; Daniel Freeman: University of Oxford</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=7BhQdzFND8s&ab_channel=celinethesavage" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1274" class="wrap-collabsible"> <input id="collapsibleabstractPO1274" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1274" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We built a virtual beer garden that contained various smoking cues (both verbal and non-verbal) using a motion capture system to record the realistic smoking behaviour related animations. Our 3-min long VR experience was optimized for Oculus Quest 2 with the hand tracking function enabled. We conducted a pilot study with 13 non-treatment-seeking nicotine-dependent cigarette smokers. The preliminary results indicate that this VR experience led to high levels of presence, and that there is a significant increase of nicotine craving - but only for those who reported a high level of immersion.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1280">X-Ray Device Positioning with Augmented Reality Visual Feedback</h3>
    
<p> <small><strong style="color: black;"> Booth: E32 </strong></small> <br> </p>
    
    <p><i>Kartikay Tehlan: Technical University of Munich&#59; Alexander Winkler: Technical University of Munich&#59; Daniel Roth: Human-Centered Computing and Extended Reality&#59; Nassir Navab: Technische Universit&auml;t M&uuml;nchen</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/jJ7q7rMvJFg" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1280" class="wrap-collabsible"> <input id="collapsibleabstractPO1280" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1280" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In minimally invasive surgeries one common way to verify progress is the use of an intraoperative X-ray device (due to its characteristic shape called a C-arm). Its control, however, remains challenging owing to its complex movements. We propose the use of an Augmented Reality Head-Mounted Display (AR-HMD) to let the surgeon choose a desired X-ray view interventionally providing the corresponding C-arm configuration as visual feedback. The study participants' feedback, despite being critical of the HMD hardware limitations, suggests an inclination towards using AR for orthopaedic surgeries on especially complex or unusual anatomies.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1282">Towards Retargetable Animations for Industrial Augmented Reality</h3>
    
<p> <small><strong style="color: black;"> Booth: E32 </strong></small> <br> </p>
    
    <p><i>Reza Manuel Mirzaiee: University of Virginia&#59; Teodor I Vernica: Aarhus University&#59; Kurt Scheuringer: Lockheed Martin Corporation&#59; William Bernstein: Air Force Research Lab</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/l4BYlPQqTWs" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1282" class="wrap-collabsible"> <input id="collapsibleabstractPO1282" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1282" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>The adoption of Augmented Reality (AR) within manufacturing has surged. However, updating animations due to upstream changes or design variations is expensive. Currently, platform-agnostic standards for computer-aided design files do not adequately specify kinematic animations. Consequently, animations require manual, individual updates. We showcase a method for implementing geometry-independent AR assembly animations using skeletal armatures, a technique widely used in the entertainment industry. This technique allows upstream engineering changes to propagate through to the AR assembly visualization, leading to a more automated pipeline for handling animations in Industrial AR systems.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1285">Synesthesia AR: Creating Sound-to-Color Synesthesia in Augmented Reality</h3>
    
<p> <small><strong style="color: black;"> Booth: E31 </strong></small> <br> </p>
    
    <p><i>Shashaank N: Columbia University&#59; Steven Feiner: Columbia University</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/QbmLwAd2UOc " target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1285" class="wrap-collabsible"> <input id="collapsibleabstractPO1285" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1285" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Sound-to-color synesthesia is a neurological condition where people experience different colors and shapes when listening to music. We present an augmented reality application that aims to create an interactive synesthesia experience for non-synesthetes. In this application, users can visualize colors corresponding to each unique note in the 12-tone equal-temperament tuning system, and the auditory input can be selected from audio files or real-time microphone. A gestural hand-tracking interface allows users to paint the world space in visualized synesthetic colors.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1286">HoloCMDS: Investigating Around Field of View Glanceable Commands Selection in AR-HMDs</h3>
    
<p> <small><strong style="color: black;"> Booth: E33 </strong></small> <br> </p>
    
    <p><i>Rajkumar Darbar: INRIA Bordeaux&#59; Arnaud Prouzeau: Inria&#59; Martin HACHET: Inria</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/Ye-UDkTqNpo" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1286" class="wrap-collabsible"> <input id="collapsibleabstractPO1286" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1286" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Augmented reality merges the real and virtual worlds seamlessly in real-time. However, we need contextual menus to manipulate virtual objects rendered in our physical space. Unfortunately, designing a menu for augmented reality head-mounted displays (AR-HMDs) is challenging because of their limited display field of view (FOV). In this paper, we propose HoloCMDS to support quick access of contextual commands in AR-HMDs and conduct an initial experiment to get users' feedback about this technique.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1290">Jitsi360: Using 360 Images for Live Tours</h3>
    
<p> <small><strong style="color: black;"> Booth: E33 </strong></small> <br> </p>
    
    <p><i>Alaeddin Nassani: University of Auckland&#59; Huidong Bai: The University of Auckland&#59; Mark Billinghurst: University of South Australia</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/lLSCJWu4ncU" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1290" class="wrap-collabsible"> <input id="collapsibleabstractPO1290" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1290" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In this poster, we present a system for sharing immersive 360-degree images for live tours. While the sharing of prerecorded 360 video and images is becoming commonplace, there have been fewer systems presented that support live-sharing of 360 images. We customised a video conferencing platform to enable dozens of people to see the same 360-degree content together while having a live call. We describe our system and pilot user study results from using it for a virtual guided tour. Compared to sharing non 360-degree images on Zoom, our system was felt to be more immersive, enjoyable, and easy to use.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1293">Apparent shape manipulation by light-field projection onto a retroreflective surface</h3>
    
<p> <small><strong style="color: black;"> Booth: E34 </strong></small> <br> </p>
    
    <p><i>jion kanaya: Wakayama University&#59; Toshiyuki Amano: Wakayama University</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/IULjfYJMfrE" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1293" class="wrap-collabsible"> <input id="collapsibleabstractPO1293" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1293" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>In order to optically correctly present metallic luster and structural color, it is necessary to reproduce the changes in brilliance and color that accompany the movement of the viewpoint.<br>Light-field projection onto a retroreflective surface can optically present texture depending on the viewpoint. By applying this, it is thought that the apparent shape can be manipulated depending on the viewpoint. This research proposes an optical illusion that manipulates the apparent shape of the 3D object with lightfield projection based on a perceptual normal map transformation.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1296">Enabling Augmented Reality Incorporate with Audio on Indoor Navigation for People with Low Vision</h3>
    
<p> <small><strong style="color: black;"> Booth: E13 </strong></small> <br> </p>
    
    <p><i>Zihao Chi: Nara Institute of Science and Technology&#59; Zhaofeng Niu: Nara Institute of Science and Technology&#59; Taishi Sawabe: Nara Institute of Science and Technology</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/-rkDuUTwOFY" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1296" class="wrap-collabsible"> <input id="collapsibleabstractPO1296" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1296" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Indoor navigation is difficult for low vision people, even they can benefit from visual cue. However, visual rating has been neglected, which can provide assistant for different visual impairments. In this paper, we propose an Augmented Reality (AR) application that measures the visual rating firstly. According to visual rating, different navigation service, including visual and audio cue, is provided for users. Object detection and depth estimation are utilized for avoiding obstacles. We conducted an exploratory design study to investigate our idea. In experiment, Snellen chart is displayed on Hololens2 and a pilot study has been conducted. The strategy on both visual aid and audio cue will be tested by a user study for the next step</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1297">Studying the Effect of Physical Realism on Time Perception in a HAZMAT VR Simulation</h3>
    
<p> <small><strong style="color: black;"> Booth: C32 </strong></small> <br> </p>
    
    <p><i>Kadir Baturalp Lofca: University of North Carolina at Greensboro&#59; Jason Haskins: Nextgen Interactions&#59; Jason Jerald: Nextgen Interactions&#59; Regis Kopper: University of North Carolina at Greensboro</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/_q_88EgLuBk" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1297" class="wrap-collabsible"> <input id="collapsibleabstractPO1297" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1297" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Our research focuses on how physical props in virtual reality (VR) can affect users' time perception. We designed an experiment with the goal of comparing users' perception of time when using physical props in VR as compared to standard controllers and only virtual elements. In order to quantify this effect, time estimates for both conditions are compared to time estimates for a matching real-world task. In this experiment, participants assume the role of a firefighter trainee, going through a HAZMAT scenario, where they touch and interact with different physical props that match the virtual elements of the scene.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1303">Flick Typing: Toward A New XR Text Input System Based on 3D Gestures and Machine Learning</h3>
    
<p> <small><strong style="color: black;"> Booth: E12 </strong></small> <br> </p>
    
    <p><i>Tian Yang: University of Southern California&#59; Powen Yao: University of Southern California&#59; Michael Zyda: USC</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/3U6LZ25O-xc" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1303" class="wrap-collabsible"> <input id="collapsibleabstractPO1303" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1303" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We propose a new text entry input method in Extended Reality that we call Flick Typing. Flick Typing utilizes the user's knowledge of a QWERTY keyboard layout, but does not explicitly provide visualization of the keys, and is agnostic to user posture or keyboard position. To type with Flick Typing, users will move their controller to where they think the target key is with respect to the controller's starting position and orientation, often with a simple flick of their wrists. Machine learning model is trained and used to adapt to the user's mental map of the keys in 3D space.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1304">Feasibility of mapping engagement ratios to levels of task complexity within VR environments</h3>
    
<p> <small><strong style="color: black;"> Booth: E11 </strong></small> <br> </p>
    
    <p><i>Yobbahim J Vite: University of Calgary&#59; Yaoping Hu: University of Calgary</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/u5ZrHZnFDrI" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1304" class="wrap-collabsible"> <input id="collapsibleabstractPO1304" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1304" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This paper studied the feasibility of mapping an engagement ratio onto a level of task complexity, when human participants undertook interactive tasks within a virtual reality (VR) environment. Each human participant used a haptic device to push a ball-shaped object through a pipe. There were a total of three pipes, which had three different shapes corresponding to the levels of task complexity mathematically. An electroencephalogram (EEG) device recorded the brain activity of the participant while undertaking the task. The outcomes of the study confirmed the feasibility of mapping the engagement ratio with the levels of task complexity.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1309">Learning Environments in AR Comparing Tablet and Head-mounted Augmented Reality Devices at Room and Table Scale</h3>
    
<p> <small><strong style="color: black;"> Booth: C33 </strong></small> <br> </p>
    
    <p><i>Paul Craig: University of Minnesota&#59; Peter Willemsen: University of Minnesota Duluth&#59; Edward Downs: University of Minnesota Duluth&#59; Alex Lover: University of Minnesota Duluth&#59; William Barber: University of Minnesota Duluth</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/i1wYTf48WW4" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1309" class="wrap-collabsible"> <input id="collapsibleabstractPO1309" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1309" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This paper presents work to examine how presentation scale and device form factor may affect learning in augmented reality (AR) environments. We conducted a 2 (form factor) x 2 (scale) experiment in which 131 participants explored an AR learning environment using either tablet AR or a Hololens2 crossed with either full room scale or at table scale. Dependent variables measured participants declarative knowledge about information acquired in the environment as well as their understanding of spatial layout. Initial analysis suggests comparable outcomes across all manipulations with respect to acquiring declarative and spatial knowledge.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1310">The Digital Twins of Thor's Hammer Based on Motion Sensing</h3>
    
<p> <small><strong style="color: black;"> Booth: C34 </strong></small> <br> </p>
    
    <p><i>Zengxu Bian: College of Computer Science and Technology&#59; Yuqi Liu: College of Computer Science and Technology&#59; Jinkang Guo: Qingdao University&#59; Zhihan Lv: Uppsala University</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/JpHYGdFWp2w" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1310" class="wrap-collabsible"> <input id="collapsibleabstractPO1310" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1310" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Ancient humans attribute the phenomenon of thunder and lightning to divine power. The power of Thor that can lift Thor's Hammer, the body not be hurt by thunder and lightning. It's not impossible for us to control thunder and lightning like Thor. The Digital Twins system of the robotic arm designed in this paper integrates the physical device of the robotic arm, the digital model of robotic arm, the body sense interaction, and the virtual-reality mapping module. It can digitally control the robotic arm.With this system, we can all lift Thor's hammer in the future.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1312">Rereading the Narrative Paradox for Virtual Reality Theatre</h3>
    
<p> <small><strong style="color: black;"> Booth: E34 </strong></small> <br> </p>
    
    <p><i>Xiaotian Jiang: Goldsmiths, University of London&#59; Xueni Pan: Goldsmiths&#59; Jonathan Freeman: Goldsmiths University</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://www.youtube.com/watch?v=EjciBNYMV5M" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1312" class="wrap-collabsible"> <input id="collapsibleabstractPO1312" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1312" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>We examined several key issues around audience autonomy in VR theatre. Informed by a literature review and a qualitative user study (grounded theory), we developed a conceptual model that enables a quantifiable evaluation of audience experience in VR theatre. A second user study inspired by the 'narrative paradox', investigates the relationship between spatial exploration and narrative comprehension in two VR performances. Our results show that although navigation distracted the participants from following the full story, they were more engaged, attached and had a better overall experience as a result of their freedom to move and interact.</p>
            </div>
        </div>
    </div>
    

    <h3 id="PO1318">Investigation of the potential use of Virtual Reality for Agoraphobia Exposure therapy</h3>
    
<p> <small><strong style="color: black;"> Booth: E33 </strong></small> <br> </p>
    
    <p><i>Sinead Barnett: Walton Institute&#59; Ian Mills: Walton Institute&#59; Frances cleary: Walton Institute</i></p>
    
    
    
    
        <p>Teaser Video: <a href="https://youtu.be/D91eAlPDeB4" target="_blank">Watch Now</a></p>
    
    <div id="abstractPO1318" class="wrap-collabsible"> <input id="collapsibleabstractPO1318" class="toggle" type="checkbox"> <label for="collapsibleabstractPO1318" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Preliminary research study to evaluate the potential and need for virtual reality in the mental health sector specifically focusing on the treatment of agoraphobia. A survey was sent to numerous participants that have been diagnosed and currently receiving treatment for agoraphobia. Results have concluded there is a demand for virtual reality treatment for agoraphobia and this in turn can lead to future studies into the VR therapy</p>
            </div>
        </div>
    </div>
    

    <h3 id="DC1032">Immersive Analytics for Understanding Ecosystem Services Tradeoffs</h3>
    
<p> <small><strong style="color: black;"> Booth: D28 </strong></small> <br> </p>
    
    <p><i>Benjamin Powley</i></p>
    
    
    
    
    <div id="abstractDC1032" class="wrap-collabsible"> <input id="collapsibleabstractDC1032" class="toggle" type="checkbox"> <label for="collapsibleabstractDC1032" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Existing immersive systems for analysing geospatial data relating to ecosystem services are not designed for all groups involved with land use decision making. Land management scientists have different requirements compared to non-experts as the tasks they perform are different. Land use decision making needs better tools for assisting the analysis and exploration of land use decisions, and their effect on ecosystem services. In this research, a user centred design process is applied for developing and evaluating an immersive VR visualization tool to assist with better decision making around land use. Interviews with experts found issues with how their current tool presents analysis results, and problems with communicating their results to stakeholders. A literature review found no pre-existing immersive VR systems specifically for analysing tradeoffs among ecosystem services.</p>
            </div>
        </div>
    </div>
    

    <h3 id="DC1050"> Exploration of Context and Physiological Cues for Personalized Emotion-Adaptive Virtual Reality</h3>
    
<p> <small><strong style="color: black;"> Booth: D31 </strong></small> <br> </p>
    
    <p><i>Mr Kunal Gupta</i></p>
    
    
    
    
    <div id="abstractDC1050" class="wrap-collabsible"> <input id="collapsibleabstractDC1050" class="toggle" type="checkbox"> <label for="collapsibleabstractDC1050" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Immersive Virtual Reality (VR) can create compelling context-specific emotional experiences, but very few studies explore the importance of emotion-relevant contextual cues in VR. In this thesis, I investigate how to use combined contextual and physiological cues to improve emotion recognition in VR and enhance shared VR experiences. The main novelty is the creation of the first Personalized Real-time Emotion-Adaptive Context-Aware VR (PerAffectly VR) system which provides significant insight into how to create, measure, and share emotional VR experiences.</p>
            </div>
        </div>
    </div>
    

    <h3 id="DC1037">Improving Multi-User Interaction for Mixed Reality Telecollaboration</h3>
    
<p> <small><strong style="color: black;"> Booth: D27 </strong></small> <br> </p>
    
    <p><i>Faisal Zaman</i></p>
    
    
    
    
    <div id="abstractDC1037" class="wrap-collabsible"> <input id="collapsibleabstractDC1037" class="toggle" type="checkbox"> <label for="collapsibleabstractDC1037" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Mixed reality approaches offer merging of real and virtual worlds to create new environments and visualizations for real-time interaction. However, existing systems do not utilise user real environment, lack detail in dynamic environments, and often lack multi-user capabilities. This research focuses on exploring this multi-user aspect of immersive collaboration, where an arbitrary number of co-located and remotely located users can immerse into a single or merged collaborative mixed reality space. The aim is to enable users to experience VR/AR together, irrespective of the type of HMD, and facilitate users with their collaborative tasks. The main goal is to develop an immersive collaboration platform in which users can utilize the space around them and at the same time can collaborate and switch between different perspectives of other co-located and remote users.</p>
            </div>
        </div>
    </div>
    

    <h3 id="DC1038"> A Mobile Intervention to Promote Social Skills in Children with Autism Spectrum Disorder Using AR Face Masks</h3>
    
<p> <small><strong style="color: black;"> Booth: D32 </strong></small> <br> </p>
    
    <p><i>Ms. Hiroshika Nadeeshani Premarathne</i></p>
    
    
    
    
    <div id="abstractDC1038" class="wrap-collabsible"> <input id="collapsibleabstractDC1038" class="toggle" type="checkbox"> <label for="collapsibleabstractDC1038" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Autism Spectrum Disorder is a lifelong neuro-developmental disorder characterized by several behaviors including the deficits in emotion recognition and face-directed eye gaze. Although, behavioral therapists are there to help children with ASD to improve the skills, due to both high demand for services and lack of resources, there is a need for providing alternative interventions to therapy sessions. My PhD aims to develop an Augmented Reality (AR) based mobile application to provide an alternative method and assist children with ASD in identifying emotions and communicating with their close family members using face-directed eye gaze.</p>
            </div>
        </div>
    </div>
    

    <h3 id="DC1048">Using Multimodal Input in Augmented Virtual Teleportation</h3>
    
<p> <small><strong style="color: black;"> Booth: D26 </strong></small> <br> </p>
    
    <p><i>Prasanth Sasikumar</i></p>
    
    
    
    
    <div id="abstractDC1048" class="wrap-collabsible"> <input id="collapsibleabstractDC1048" class="toggle" type="checkbox"> <label for="collapsibleabstractDC1048" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Augmented (AR) and Virtual Reality (VR) can create compelling emotional collaborative experiences, but very few studies have explored the importance of sharing a user's live environment and their physiological cues. In this Ph.D. thesis, I am investigating how to use scene reconstruction and emotion recognition to enhance shared collaborative AR/VR experiences. I have developed a framework that can be broadly classified into two sections: 1) Live scene capturing for real-time environment reconstruction, 2) Sharing multimodal input such as gaze, gesture, and physiological cues. The main novelty of the research is that it is one of the first systems for real-time sharing of environment and emotion cues. It provides significant insight into how to create, measure, and share remote collaborative experiences. The research will be helpful in multiple application domains such as remote assistance, tourism, training, and entertainment. It will also enable the creation of interfaces that automatically adapt to the user's emotional needs and environment and provide a better collaborative experience.</p>
            </div>
        </div>
    </div>
    

    <h3 id="DC1049">A Tangible Augmented Reality Programming Learning Environment (TARPLE) for Active, Guided Learning</h3>
    
<p> <small><strong style="color: black;"> Booth: D33 </strong></small> <br> </p>
    
    <p><i>Dmitry Resnyansky</i></p>
    
    
    
    
    <div id="abstractDC1049" class="wrap-collabsible"> <input id="collapsibleabstractDC1049" class="toggle" type="checkbox"> <label for="collapsibleabstractDC1049" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This PhD project aims to bring together technological and educational perspectives by understanding how objectives of programming learning and principles of active learning and embodied learning can be supported and enhanced with AR and TUI technologies. This work presents an approach to the design and evaluation of a TARPLE prototype with enhanced functionality to encourage active guided learning of a text-based OOP language. The system supports natural interaction with learning material, and embodiment and contextualisation of information in 3D space. One of the goals of this thesis has been to understand how empirical studies can inform the design of a TARPLE that supports learning of text-based programming languages and development of a basic debugging skillset.</p>
            </div>
        </div>
    </div>
    

    <h3 id="DC1031">Designing and Optimizing Daily-wear Photophobic Smart Sunglasses</h3>
    
<p> <small><strong style="color: black;"> Booth: D25 </strong></small> <br> </p>
    
    <p><i>Xiaodan Hu</i></p>
    
    
    
    
    <div id="abstractDC1031" class="wrap-collabsible"> <input id="collapsibleabstractDC1031" class="toggle" type="checkbox"> <label for="collapsibleabstractDC1031" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Photophobia, also known as light sensitivity, is a condition in which there is a fear of light. Traditional sunglasses and tinted glasses typically worn by individuals with photophobia only provide linear dimming, leading to difficulty to see the contents in the dark region of a high-contrast environment (e.g., indoors at night). This paper presents a smart dimming sunglass that uses a spatial light modular (SLM) to flexibly dim the user's field of view based on scene detection from an HDR camera. To address the problem when the user views a distant object, the occlusion mask displayed on the SLM becomes blurred due to out-of-focus, thus providing an insufficient modulation. An optimization model is designed to dilate the occlusion mask appropriately. The optimized dimming effect is verified by the camera and preliminary test by real users to be able to filter the desired amount of incoming light through a blurred mask.</p>
            </div>
        </div>
    </div>
    

    <h3 id="DC1040">The impact of the Informational load of Presence Illusions on Users Attention and Memory</h3>
    
<p> <small><strong style="color: black;"> Booth: D34 </strong></small> <br> </p>
    
    <p><i>Daniel A. Mu&ntilde;oz Daniel A. Mu&ntilde;oz</i></p>
    
    
    
    
    <div id="abstractDC1040" class="wrap-collabsible"> <input id="collapsibleabstractDC1040" class="toggle" type="checkbox"> <label for="collapsibleabstractDC1040" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Presence has become an expected outcome for most VR experiences due to its positive relationship with engagement, task performance, behavior change, and other experiences. However, current literature reports contradictory results regarding presence, cognitive load, working memory, and attention. This study explores the cognitive load of presence under the framework of illusions to understand how these illusions impact users' attention and memory. Quantify and hierarchize the information built by presence (Place and Plausibility of illusion), attempt to contribute knowledge to further design, and effectively manage attention and distraction on Virtual reality experiences. This document discussed our theoretical direction and a proposed psychophysiological study.</p>
            </div>
        </div>
    </div>
    

    <h3 id="DC1023">Gamified VR for Socially Isolated Adolescents with Significant Illness</h3>
    
<p> <small><strong style="color: black;"> Booth: D25 </strong></small> <br> </p>
    
    <p><i>Ms Udapola Balage Hansi Shashiprabha Udapola</i></p>
    
    
    
    
    <div id="abstractDC1023" class="wrap-collabsible"> <input id="collapsibleabstractDC1023" class="toggle" type="checkbox"> <label for="collapsibleabstractDC1023" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Adolescents with significant illness face various psychosocial and mental wellbeing challenges during hospitalisation. Social isolation from family and peers is identified as a significant concern for this group. Several digital interventions have been proposed to connect these young people with others, such as video conferencing, social media, social robots, and online games. Research so far has found those to be beneficial for adolescents' wellbeing. Social VR is a novel social interaction mechanism that allows users to interact socially within an immersive 3D virtual environment with embodiment experience. Playing in the social VR space is generally identified as a motivational factor for adolescents to engage in social interactions. Therefore, integrating game technologies into social VR space would encourage and motivate socially isolated adolescents to engage socially intrinsically. The main goal of this research project is to enhance the social engagement of socially isolated adolescents by fostering positive interactions with their peers within a safe gamified virtual environment. In order to achieve that, this research will design and develop an intervention using game and VR technologies. Then, the designed intervention will be evaluated with the target user group to investigate the impact of implemented game mechanics on social engagement and connectedness.</p>
            </div>
        </div>
    </div>
    

    <h3 id="DC1030">XR for Improving Cardiac Catheter Ablation Procedure</h3>
    
<p> <small><strong style="color: black;"> Booth: D34 </strong></small> <br> </p>
    
    <p><i>Mr. Nisal Manisha Udawatta Kankanamge Don</i></p>
    
    
    
    
    <div id="abstractDC1030" class="wrap-collabsible"> <input id="collapsibleabstractDC1030" class="toggle" type="checkbox"> <label for="collapsibleabstractDC1030" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Cardiac arrhythmia refers to abnormalities of heart rhythm, and cardiac catheter ablation procedure provides the best therapeutic outcomes to cure this life-threatening pathology. An electrophysiologist clinically performs the procedure that involves navigating 'catheters' into the chambers of the heart through peripheral blood vessels, studying the cardiac electrophysiology and performing ablations. An electrophysiologist must possess a comprehensive understanding of cardiac electrophysiology and precise instrument handling due to the sensitiveness of the procedure. In the conventional approach, electroanatomical mapping systems and fluoroscopic visualizations are utilized to assist the procedure&#59; however, their limitations reduce the procedure's effectiveness. Two main scenarios have been identified to improve the effectiveness of the procedure: intraoperative guidance and procedure training. This study aims to examine how extended reality technologies (eg. AR/VR) can be used to improve the effectiveness of the cardiac catheter ablation procedure.</p>
            </div>
        </div>
    </div>
    

    <h3 id="DC1027">Mixed Reality Interaction for Mobile Knowledge Work</h3>
    
<p> <small><strong style="color: black;"> Booth: D26 </strong></small> <br> </p>
    
    <p><i>Verena Biener</i></p>
    
    
    
    
    <div id="abstractDC1027" class="wrap-collabsible"> <input id="collapsibleabstractDC1027" class="toggle" type="checkbox"> <label for="collapsibleabstractDC1027" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Knowledge workers typically work on some kind of computer and other than an internet connection, they rarely need access to specific work environments or devices. This makes it feasible for them to also work in different environments or in mobile settings, like public transportation. In such spaces comfort and productivity could be decreased due to hardware limitations like small screen sizes or input devices and environmental clutter. Mixed reality (MR) has the potential to solve such issues. It can provide the user with additional display space that can even include the third dimension. It can open up new possibilities for interacting with virtual content using gestures or spatially tracked devices. And it can allow the users to modify the work environment according to their personal preferences. This doctoral thesis aims at exploring the challenges of using MR for mobile knowledge work and how to effectively support knowledge worker tasks through appropriate interaction techniques.</p>
            </div>
        </div>
    </div>
    

    <h3 id="DC1024">Dynamic facial expressions on virtual humans to facilitate virtual reality (VR) mental health therapy</h3>
    
<p> <small><strong style="color: black;"> Booth: D31 </strong></small> <br> </p>
    
    <p><i>Miss Shu Wei</i></p>
    
    
    
    
    <div id="abstractDC1024" class="wrap-collabsible"> <input id="collapsibleabstractDC1024" class="toggle" type="checkbox"> <label for="collapsibleabstractDC1024" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>This ongoing project aims to utilize dynamic facial expressions on virtual humans to enhance the effectiveness and efficiency of virtual reality (VR) mental health therapy. A systematic review of virtual humans in mental health VR indicated that only around 10&#37; applications used dynamic facial expressions. The potentials of virtual characters' emotion richness is understudied and it is unclear how the facial expressions affect the individuals differently in the virtual environment. Therefore, we will focus on understanding people's behavioural, physiological, and psychological reactions toward facial-animated characters in VR experimental studies. The first study examines whether particular non-verbal behaviours can enhance people's therapy engagement, by applying warmness facial expressions and head nod on a virtual coach. Future experiments will further look at mixed facial expressions, and assess people's visual attention (through eye-tracking) and interpretation of the emotional faces. This research will explore how best to use facial expressions to facilitate VR therapy through the practice of psychiatric research, VR programming and 3D animation.</p>
            </div>
        </div>
    </div>
    

    <h3 id="DC1000">Robust Redirected Walking in the Wild</h3>
    
<p> <small><strong style="color: black;"> Booth: D27 </strong></small> <br> </p>
    
    <p><i>Niall L. Williams</i></p>
    
    
    
    
    <div id="abstractDC1000" class="wrap-collabsible"> <input id="collapsibleabstractDC1000" class="toggle" type="checkbox"> <label for="collapsibleabstractDC1000" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Locomotion is a fundamental component of experiences in virtual reality (VR). However, locomotion in VR is often difficult because the layouts of the physical and virtual environments are often different, which may cause unobstructed paths in the virtual world to correspond to obstructed paths in the physical world. Thus, in order to deliver a comfortable and immersive virtual experience to users, it is important that the user can explore the virtual world using techniques that help them avoid collisions with unseen physical objects. Redirected walking (RDW) is one such technique that enables collision-free locomotion in VR using real walking. Although RDW shows promise as an effective locomotion interface, it has seen relatively little adoption in the consumer market due to the difficulty in deploying effective RDW algorithms that are robust to different environment layouts and different users' perceptual thresholds. For my thesis, I am focused on developing RDW methods that are capable of enabling collision-free locomotion in arbitrary physical and virtual environments for a wide range of users.</p>
            </div>
        </div>
    </div>
    

    <h3 id="DC1033">Context-Aware Inference and Adaptation in Augmented Reality</h3>
    
<p> <small><strong style="color: black;"> Booth: D32 </strong></small> <br> </p>
    
    <p><i>Shakiba Davari</i></p>
    
    
    
    
    <div id="abstractDC1033" class="wrap-collabsible"> <input id="collapsibleabstractDC1033" class="toggle" type="checkbox"> <label for="collapsibleabstractDC1033" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Augmented Reality(AR) offers the potential for easy and efficient information access, reinforcing the wide belief that AR Glasses are the next-generation of personal computing devices. However, to realize this all-day AR vision, the AR interface must be able to address the challenges that constant and pervasive presence of virtual content can cause for the user. The optimal interface, that is the most efficient yet least intrusive, in one context may be the worst interface for another context. Throughout the day, as the user switches context, an optimal all-day interface must adapts its virtual content display and interactions as well. This work aims to propose a research agenda to design and validate different adaptation techniques and context-aware AR interfaces and introduce a framework for the design of such intelligent interfaces.</p>
            </div>
        </div>
    </div>
    

    <h3 id="DC1039">Balancing Realities by Smoothing Cross-Reality Interactions</h3>
    
<p> <small><strong style="color: black;"> Booth: D28 </strong></small> <br> </p>
    
    <p><i>Matt Gottsacker</i></p>
    
    
    
    
    <div id="abstractDC1039" class="wrap-collabsible"> <input id="collapsibleabstractDC1039" class="toggle" type="checkbox"> <label for="collapsibleabstractDC1039" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Virtual reality (VR) devices have a demonstrated capability to make users feel present in a virtual world. Research has shown that, at times, users desire a less immersive system that provides them awareness of and the ability to interact with elements from the real world and with a variety of devices. Understanding such cross-reality interactions is an under-explored research area that will become increasingly important as immersive devices become more ubiquitous. The planned focus of my dissertation is to investigate the social norms that are complicated by these interactions and design solutions that lead to meaningful interactions. As a second-year PhD student, I am excited about the possibility of discussing my research at the IEEE VR 2022 Doctoral Consortium and getting feedback from peers and mentors about the direction of my dissertation.</p>
            </div>
        </div>
    </div>
    

    <h3 id="DC1042">Designing Immersive Tools for Supporting Cognition in Remote Scientific Collaboration</h3>
    
<p> <small><strong style="color: black;"> Booth: D33 </strong></small> <br> </p>
    
    <p><i>Monsurat Olaosebikan</i></p>
    
    
    
    
    <div id="abstractDC1042" class="wrap-collabsible"> <input id="collapsibleabstractDC1042" class="toggle" type="checkbox"> <label for="collapsibleabstractDC1042" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Scientists collaborate remotely across institutions, countries and continents. However, collaborating remotely is challenging. Video-conferencing tools used for meetings limit the cognitive practices that collaborators can partake in. In virtual reality (VR) users can gain back spatial affordances present in collocated collaboration and we can design interactions that would not be possible in the real world. My research aims to investigate how VR can support cognition in remote scientific collaboration through the design, development, and study of Embodied Notes: a cognitive support tool designed to be used in a collaborative virtual environment.</p>
            </div>
        </div>
    </div>
    

    <h3 id="DC1047">Improving presence of virtual humans through paralinguistics</h3>
    
<p> <small><strong style="color: black;"> Booth: D25 </strong></small> <br> </p>
    
    <p><i>Andrew H Maxim</i></p>
    
    
    
    
    <div id="abstractDC1047" class="wrap-collabsible"> <input id="collapsibleabstractDC1047" class="toggle" type="checkbox"> <label for="collapsibleabstractDC1047" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Social presence and plausibility are two of the most important constructs being studied in IEEE VR to improve user experience in virtual environments. However, virtual humans that are used in these virtual environments lack the capability of adaptive speech that directly influences social presence and plausibility of the virtual humans that are used since humans use adaptive speech during conversations. Virtual humans lack the adaptability of their speech in the manner that humans do. Research has been explored in how pitch, rate of speech, and intensity can be manipulated in human discourse. However, little research has been done in using adaptable speech to affect dialog with virtual humans. This dissertation attempts to create a framework and a system that enables adaptive virtual human speech. This includes using machine learning and user modeling to manipulate paralinguistic features such as pitch, rate of speech, pause duration, and intensity using Speech Synthesis Markup Language (SSML). Understanding the interplay of paralinguistic features between virtual human and human will improve the social presence of virtual humans. By achieving this, virtual interactions can be moved toward the dynamic level of human-to-human interactions, thus increasing the co-presence and plausibility of these virtual human characters.</p>
            </div>
        </div>
    </div>
    

    <h3 id="DC1029">Leveraging AR Cues towards New Navigation Assistant Paradigm</h3>
    
<p> <small><strong style="color: black;"> Booth: D34 </strong></small> <br> </p>
    
    <p><i>Yu Zhao</i></p>
    
    
    
    
    <div id="abstractDC1029" class="wrap-collabsible"> <input id="collapsibleabstractDC1029" class="toggle" type="checkbox"> <label for="collapsibleabstractDC1029" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Extensive research has shown that the knowledge required to navigate an unfamiliar environment has been greatly reduced as many of the planning and decision-making tasks can be supplanted by the use of automated navigation systems. The progress in augmented reality (AR), particularly AR head-mounted displays (HMDs) foreshadows the prevalence of such devices as computational platforms of the future. AR displays open a new design space on navigational aids for solving this problem by superimposing virtual imagery over the environment. This dissertation abstract proposes a research agenda that investigates how to effectively leverage AR cues to help both navigation efficiency and spatial learning in walking scenarios.</p>
            </div>
        </div>
    </div>
    

    <h3 id="DC1045">Annotation in Asynchronous Collaborative Immersive Analytic Environments using Augmented Reality</h3>
    
<p> <small><strong style="color: black;"> Booth: D26 </strong></small> <br> </p>
    
    <p><i>Zahra Borhani</i></p>
    
    
    
    
    <div id="abstractDC1045" class="wrap-collabsible"> <input id="collapsibleabstractDC1045" class="toggle" type="checkbox"> <label for="collapsibleabstractDC1045" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Immersive Analysts(IA) and Augmented Reality (AR) head-mounted displays provide a different paradigm for people to analyze multidimensional data and externalize their thoughts by utilizing the stereoscopic nature of headsets. However, using annotation in IA-AR is challenging and not well-understood. In addition, IA collaborative environments add another complexity level for users operating on complex visualized datasets. Current AR systems focus mainly on synchronized collaboration, while asynchronous collaboration has remained unexplored. This project investigates annotation in IA for asynchronous collaborative environments. We present our research studies on virtual annotation types and introduce a new filtering annotation technique for IA.</p>
            </div>
        </div>
    </div>
    

    <h3 id="DC1046">Effects of Asymmetric Locomotion Methods on Collaborative Navigation and Wayfinding in Shared Virtual Environments</h3>
    
<p> <small><strong style="color: black;"> Booth: D33 </strong></small> <br> </p>
    
    <p><i>Soumyajit Chakraborty</i></p>
    
    
    
    
    <div id="abstractDC1046" class="wrap-collabsible"> <input id="collapsibleabstractDC1046" class="toggle" type="checkbox"> <label for="collapsibleabstractDC1046" class="lbl-toggle">Abstract</label>
        <div class="collapsible-content">
            <div class="content-inner">
                <p>Navigation and wayfinding can be accomplished either by single person or a group of people. Using the help of immersive virtual reality technology, significant research has been conducted to find out how a person can navigate and wayfind in a virtual world. How- ever, there has been little work done that asks how multiple people can collaboratively navigate and wayfind in a virtual world. In this proposal, we investigate this question with a specific interest on how different locomotion methods can affect the acquired knowledge of a group of individuals in a distributed, shared virtual environment.</p>
            </div>
        </div>
    </div>
    
</div>
-->



                </div>
                <div class="column center" style=""></div>

                <!-- Main Content -->
                <div class="column right" style="text-align: center">
            
                    <!-- <img class="sidebar-header-icon" src="/assets/images/ieeevr-logo.jpg">-->
                    <!-- <a class="twitter-timeline" data-height="450" href="https://twitter.com/IEEEVR?ref_src=twsrc%5Etfw">Tweets by IEEEVR</a> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> -->
                    <div>
                        <ul class="social-icons">
                            <li><a href="https://facebook.com/ieeevr"><img src=/assets/images/social/circle-color/Facebook.png alt="Facebook" /></a></li>
                            <li><a href="http://www.twitter.com/ieeevr"><img src=/assets/images/social/circle-color/Twitter.png alt='Twitter' /></a></li>
                        </ul>
                    </div>

                    <div class="confsponsors" id="sponsors">
                        <h4 style="border-bottom: 1px solid black;padding-bottom: 5px;">Conference Sponsors</h4>

                        <div style="background-color: #fec10d; border-radius: 7px;">
                            <b style="color: #363636">Diamond</b>
                        </div>
                        <br>
                        <a href="https://www.virbela.com/" target="_blank">
                            <img class="conf-icon" style="width: 90%;" src=/assets/images/sponsors/Virbela.png alt="Virbela Logo. Their name next to a stylised green, red, and blue circle.">
                        </a>
                        <br>
                        <br>
                        <div style="background-color: #fec10d; border-radius: 7px;">
                            <b style="color: #363636">Gold</b>
                        </div>
                        <br>
                        <a href="https://www.christchurchnz.com/" target="_blank">
                            <img class="conf-icon" style="width: 90%;" src=/assets/images/sponsors/ChristchurchNZ_Logo.png alt="ChristchurchNZ Logo. Their name is written in a red font.">
                        </a>
                        <br>
                        <br>
                        <a href="https://immersivelrn.org/" target="_blank">
                            <img class="conf-icon" style="width: 90%;" src=/assets/images/sponsors/iLRN.png alt="iLRN Logo. Their name is written in an orange font.">
                        </a>
                        <br>
                        <a href="https://www.canterbury.ac.nz/" target="_blank">
                            <img class="conf-icon" style="width: 90%;" src=/assets/images/sponsors/UCRed_RGB.jpg alt="University of Canterbury Logo. Their name is written in a red font.">
                        </a>
                        <br>
                        <br>
                        <div style="background-color: #fec10d; border-radius: 7px;">
                            <b style="color: #363636">Silver</b>
                        </div>
                        <br>
                        <a href="https://www.qualcomm.com/research/extended-reality" target="_blank">
                            <img class="conf-icon" style="width: 90%;" src=/assets/images/sponsors/Qualcomm.png alt="The Qualcomm logo. Their name is written in a blue font.">
                        </a>
                        <br>
                        <br>
                        <div style="background-color: #fec10d; border-radius: 7px;">
                            <b style="color: #363636">Bronze</b>
                        </div>
                        <br>
                        <a href="https://www.hitlabnz.org/" target="_blank">
                            <img class="conf-icon" style="width: 90%;" src=/assets/images/sponsors/HITL-AIGI.jpg alt="HITLab NZ logo. A Kiwi wearing a VR headset.">
                        </a>
                        <br>

                        <h4 style="border-bottom: 1px solid black;padding-bottom: 5px;">Supporters</h4>
                        <a href="https://arive.me/" target="_blank">
                            <img class="conf-icon" style="width: 90%;" src=/assets/images/sponsors/ARIVE.jpg alt="ARIVE logo. Their name is written next to picture of Australia and New Zealand.">
                        </a>
                        <br>
                        <br>
                        <a href="https://www.mdpi.com/journal/mti" target="_blank">
                            <img class="conf-icon" style="width: 90%;" src=/assets/images/sponsors/MTI.png alt="Multimodal Technologies and Interaction logo. Their name is written next to a stylised blue M.">
                        </a>
                        <br>
                        <br>
                        <a href="https://www.nvidia.com/en-us/" target="_blank">
                            <img class="conf-icon" style="width: 90%;" src=/assets/images/sponsors/NVIDIA.png alt="NVIDIA's logo. Their name is written under a green and white spiral with a green rectangle covering its right half.">
                        </a>
                        <br>
                        <br>
                        <a href="https://www.pico-interactive.com/" target="_blank">
                            <img class="conf-icon" style="width: 90%;" src=/assets/images/sponsors/pico-vr.png alt="Pico's logo.">
                        </a>
                        <br>
                        <br>
                        <a href="https://www.xrbootcamp.com/" target="_blank">
                            <img class="conf-icon" style="width: 90%;" src=/assets/images/sponsors/XRbootcamp.png alt="XR bootcamp logo.">
                        </a>
                        <br>    
                        <br>
                        <!--<a href="https://gpcg.pt/website/en/" target="_blank">
        <img class="conf-icon" style="width: 95%;padding-top: 5px;" src=/assets/images/sponsors/CGILogo.pngalt="GPCG Logo">
    </a>
    <br>-->

                        <h4 style="border-bottom: 1px solid black;padding-bottom: 5px;">Doctoral Consortium Sponsors</h4>
                        <!--<a href="https://www.nsf.gov" target="_blank">
        <img class="conf-icon" style="width: 65%;" src=/assets/images/sponsors/nsf.jpg alt="NSF Logo">
    </a>
    <br>-->
                        <a href="https://www.nsf.gov/" target="_blank">
                            <img class="conf-icon" style="width: 90%;" src=/assets/images/sponsors/NSF.png alt="The National Science Foundation's logo. NSF is written in white over a globe surrounding by silhouettes of people holding hands.">
                        </a>
                        <br>

                        <h4 style="border-bottom: 1px solid black;padding-bottom: 5px;">Conference Partner</h4>
                        <!--<a href="https://www.cioapplicationseurope.com" target="_blank">
        <img class="conf-icon" style="width: 90%;" src=/assets/images/sponsors/cio-applications-europe.png alt="CIO Applications Europe Website">
    </a>
    <br>
    <br>-->
                    </div>

                </div>
            </div>

            <!--  footer -->
            <div class="ieeevrfooter">
                <hr>
                <small><a href=/attend/code-of-conduct/>Code of Conduct</a></small><br/>
                <p style="text-align:center ! important;">© IEEEVR Conference</p>
            </div>
        </section>

    </article>
</div>

    </div>

      
      

    

    

  </body>
</html>
