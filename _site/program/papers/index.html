<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.19.2 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->


<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Papers | IEEE VR 2022</title>
<meta name="description" content=" ">


  <meta name="author" content="IEEE VR">


<meta property="og:type" content="website">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="IEEE VR 2022">
<meta property="og:title" content="Papers">
<meta property="og:url" content="http://localhost:4000/program/papers/">


  <meta property="og:description" content=" ">





  <meta name="twitter:site" content="@ieeevr">
  <meta name="twitter:title" content="Papers">
  <meta name="twitter:description" content=" ">
  <meta name="twitter:url" content="http://localhost:4000/program/papers/">

  
    <meta name="twitter:card" content="summary">
    
  

  







  

  


<link rel="canonical" href="http://localhost:4000/program/papers/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "IEEE VR 2022",
      "url": "http://localhost:4000/"
    
  }
</script>






<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="IEEE VR 2022 Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



      <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->
<link rel="shortcut icon" type="image/png" href="https://www.ieeevr.org/2022/favicon.png"/> 

<!-- end custom head snippets -->

      
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
      
    <!--<script src="jquery.js"></script>-->
    
      
      <!--<link rel="stylesheet" href="styles.css">-->
      <script src="http://code.jquery.com/jquery-latest.min.js" type="text/javascript"></script>
      <!--<script src="script.js"></script>-->
      
  </head>
    
    

  <body class="layout--ieeevr-default">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    <!-- 

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/"><img src="/2022/favicon.png" alt=""></a>
        
        <a class="site-title" href="/">
          IEEE VR 2022
          <span class="site-subtitle"></span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/about/">About</a>
            </li><li class="masthead__menu-item">
              <a href="/contribute/">Contribute</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>
-->
      

      
      
    <div class="initial-content">
      <style>

    p {
        text-align: justify;
        text-justify: auto;
    }

    .topnav {
        #position: fixed;
        #top: 0px;
        #width: 100%;
        overflow: hidden;
        background-color: white;
        #border-bottom: 1px solid #00aeef;
    }

    .topnav a {
        float: left;
        display: block;
        color: black;
        text-align: center;
        padding: 14px 16px;
        text-decoration: none;
        font-size: 17px;
    }

    .active {
        background-color: white;
        color: black;
    }

    .topnav .icon {
        display: none;
    }

    .dropdown {
        float: left;
        overflow: hidden;
    }

    .dropdown .dropbtn {
        font-size: 17px;
        border: none;
        outline: none;
        color: black;
        padding: 16px 16px;
        background-color: inherit;
        font-family: inherit;
        margin: 0;
    }

    .dropdown-content {
        display: none;
        position: absolute;
        background-color: #fffbed;
        min-width: 160px;
        box-shadow: 0px 8px 16px 0px rgba(0, 0, 0, 0.2);
        z-index: 1;
    }

    .dropdown-content a {
        float: none;
        color: #363636;
        padding: 12px 16px;
        text-decoration: none;
        display: block;
        text-align: left;
    }

    .topnav a:hover,
    .dropdown:hover .dropbtn {
        background-color: #fec10d;
        color: #103b62;
    }

    .dropdown:hover .dropdown-content {
        display: block;
    }

    .dropdown:checked .dropdown-content {
        display: block;
    }

    @media screen and (max-width: 600px) {

        .topnav a:not(:first-child),
        .dropdown .dropbtn {
            display: none;
        }

        .topnav a.icon {
            float: right;
            display: block;
        }
    }

    @media screen and (max-width: 600px) {
        .topnav.responsive {
            position: relative;
        }

        .topnav.responsive .icon {
            position: absolute;
            right: 0;
            top: 0;
            height: inherit;
        }

        .topnav.responsive a {
            float: none;
            display: block;
            text-align: left;
        }

        .topnav.responsive .dropdown {
            float: none;
        }

        .topnav.responsive .dropdown-content {
            position: relative;
        }

        .topnav.responsive .dropdown .dropbtn {
            display: block;
            width: 100%;
            text-align: left;
        }
    }

    .ieeevrbanner {
        padding: 0px;
        border-radius: 7px;
    }

    .column {
        float: left;
        padding: 10px;
    }

    .left {
        width: 75%;
    }

    .center {
        width: 2%;
    }

    .right {
        width: 22%;
    }

    .row:after {
        content: "";
        display: table;
        clear: both;
    }

    .sponsorsend {
        display: block;
    }

    .ieeevrfooter {
        position: static;
        padding-top: 10px;
        bottom: 0;
        width: 100%;
        color: black;
        text-align: center;
        font-size: 16px;
    }

    .social-icons {
        text-align: center;
    }

    .social-icons li {
        display: inline-block;
        list-style-type: none;
        -webkit-user-select: none;
        -moz-user-select: none;
    }

    .social-icons li a {
        border-bottom: none;
    }

    .social-icons li img {
        width: 40px;
        height: 40px;
        margin-right: 30px;
    }

    .ieeevrmsgbox { // Any boxes with the theme-primary color as background
      background-color: #fec10d;
      border-radius: 7px;
    }

    .ieeevrmsgbox b {
        color: #363636;
    }

    .notice--info {
        background-color: $fffbed ! important;
        color: #363636 ! important;
    }

    .notice--text {
        background-color: $fffbed ! important;
        color: #363636 ! important;
    }

    @media screen and (max-width: 900px) {
        .left {
            width: 100%;
        }

        .center {
            display: none;
        }

        .right {
            width: 100%;
        }

        .sidebar-header-icon {
            display: none;
            padding: 0px 0px;
        }

        

        .conf-icon {
            width: 200px;
        }
    }

</style>



<button onclick="topFunction()" id="myBtnTop" title="Go to top">Top</button>

<script>
    var mybutton = document.getElementById("myBtnTop");

    window.onscroll = function() {
        scrollFunction()
    };

    function scrollFunction() {
        if (document.body.scrollTop > 100 || document.documentElement.scrollTop > 100) {
            mybutton.style.display = "block";
        } else {
            mybutton.style.display = "none";
        }
    }

    function topFunction() {
        document.body.scrollTop = 0;
        document.documentElement.scrollTop = 0;
    }


    function myFunction() {
        var x = document.getElementById("myTopnav");
        if (x.className === "topnav") {
            x.className += " responsive";
        } else {
            x.className = "topnav";
        }
    }

</script>



<div id="main" role="main">
    <article class="splash" style="margins:0px;" itemscope itemtype="https://schema.org/CreativeWork">



        <!-- navbar -->
        <div class="topnav" id="myTopnav">
            <a href=/ class="active">IEEE VR</a>
            <a href=/about>About</a>
            <!-- <a href="#">Online</a>  -->

            <div class="dropdown">
                <button class="dropbtn">Attend &#9662;</button>
                <div class="dropdown-content">
                    <a href=/attend/code-of-conduct/>Code of Conduct</a>
                    <a href=/attend/why-attend/>Why Attend</a>
                    <a href=/attend/registration/>Registration</a>
                    <a href=/attend/satellite-events>Satellite Events</a>
                    <a href=/attend/diversity-and-inclusion-scholarship/>Diversity and Inclusion Scholarship</a>
                    <a href=/attend/bridge-to-vr/>Bridge to VR</a>
                    <a href=/attend/mentorship-program/>Mentorship Program</a>
                    <!--<a href=/attend/accessibility-faq/>Accessibility FAQ</a>
                    <a href=/attend/virbela-instructions/>Virbela Instructions</a>
                    <a href="http://www.lindeman.com/vr2021/live.shtml" target="_blank">Ready Player 21</a>-->
                </div>
            </div>
            
            <div class="dropdown">
                <button class="dropbtn">Program &#9662;</button>
                <div class="dropdown-content">
                    <a href=/program/overview/>Overview</a>
                    <a href=/program/keynote-speakers/>Keynote Speakers</a>
                    <a href=/program/papers/>Papers</a>
                    <!--<a href=/program/exhibitors/>Exhibitors and Sponsors</a>
                    <a href=/program/plenary-sessions/>Plenary Sessions</a>
                    <a href=/contribute/workshoppapers/>Workshops</a>
                    <a href=/program/panels/>Panels</a>
                    <a href=/program/tutorials/>Tutorials</a>
                    <a href=/program/posters/>Posters</a>
                    <a href=/program/3dui-contest/>3DUI Contest Entries</a>
                    <a href=/program/doctoral-consortium/>Doctoral Consortium</a>
                    <a href=/program/demos/>Demos</a>
                    <a href=/program/videos/>Videos</a>
                    <a href=/program/bofs/>Birds of a Feather</a>-->

                </div>
            </div>
            
            <div class="dropdown">
                <button class="dropbtn">Contribute &#9662;</button>
                <div class="dropdown-content">
                    <a href=/contribute/exhibitors>Call for Exhibitors and Sponsors</a>
                    <a href=/contribute/>Call for Journal Papers</a>
                    <a href=/contribute/conference-papers/>Call for Conference Papers</a>
                    <a href=/contribute/posters/>Call for Posters</a>
                    <a href=/contribute/workshops/>Call for Workshops</a>
                    <a href=/contribute/3dui-contest/>Call for 3DUI Contest Entries</a>
                    <a href=/contribute/demos/>Call for Research Demos</a>
                    <a href=/contribute/doctoral-consortium/>Call for Doctoral Consortium</a>
                    <a href=/contribute/tutorials/>Call for Tutorials</a>
                    <a href=/contribute/workshoppapers/>Call for Workshop Papers</a>
            <!--    <a href=/contribute/panels/ style="color:#919191">Call for Panels</a> -->
            <!--    <a href=/contribute/videos/ style="color:#919191">Call for Videos</a> -->
                    <a href=/contribute/studentVolunteers/>Call for Student Volunteers</a>
            <!--    <a href=/contribute/presenterInfo>Presenter Guidelines</a>                  -->
            <!--    <a href=/contribute/videoInstructions/" | relative_url }}>Video Guidelines</a>   -->

                </div>
            </div>
            
            <div class="dropdown">
                <button class="dropbtn">Awards &#9662;</button>
                <div class="dropdown-content">
                    <a href=/awards/vgtc>VGTC Awards</a>
                    <!--<a href=/awards/vgtc-award-winners/>2022 Awards Winners</a>
                    <a href=/awards/conference-awards/>Conference Awards</a>-->
                </div>
            </div>

            <div class="dropdown">
                <button class="dropbtn">Committees &#9662;</button>
                <div class="dropdown-content">
                    <a href=/committees/conference-committee/>Conference Committee</a>
                    <a href=/committees/steering-committee/>Steering Committee</a> 
                    <a href=/committees/program-committee/>Program Committee</a>
                </div>
            </div>

            <div class="dropdown">
                <button class="dropbtn">Resources &#9662;</button>
                <div class="dropdown-content">
                    <a href=/resources#slide-templates>Slide Templates</a>
                    <a href=/resources#zoom-backgrounds>Zoom Backgrounds</a>
                    <a href=/resources#banner>Banners</a>
                    <a href=/resources#logos>Conference Logos</a>
                </div>
            </div>

            <a href=/past-conferences/>Past Conferences</a>

            <a href="javascript:void(0);" style="font-size:16px;" class="icon" onclick="myFunction()">&#9776;</a>
        </div>


        <!-- banner -->
        <img class="ieeevrbanner" src=/assets/images/logos/IEEE%20VR%202020%20Logo%20BANNER.jpg alt="The official banner for the IEEE Conference on Virtual Reality + User Interfaces, comprised of a Kiwi wearing a VR headset overlaid on an image of Mount Cook and a braided river.">

        <!-- content -->
        <section class="page__content" itemprop="text">
            <div class="row">

                <!-- left sidebar -->
                <div class="column left" style="">


                    <style>
    .styled-table {
        border-collapse: collapse;
        margin: 25px 0;
        font-size: 0.9em;
        font-family: sans-serif;
        /*min-width: 400px;*/
        box-shadow: 0 0 20px rgba(0, 0, 0, 0.15);
        display: table;
    }

    .styled-table thead tr {
        background-color: #fec10d;
        color: #ffffff;
        text-align: left;
    }

    .styled-table th,
    .styled-table td {
        padding: 12px 15px;
    }

    .styled-table tbody tr {
        border-bottom: 1px solid #dddddd;
    }

    .styled-table tbody tr:nth-of-type(even) {
        background-color: #f3f3f3;
    }

    .styled-table tbody tr:last-of-type {
        border-bottom: 2px solid #fec10d;
    }

    .styled-table tbody tr.active-row {
        font-weight: bold;
        color: #fec10d;
    }

    /* Collapsible */
    input[type='checkbox'] {
        display: none;
    }

    .wrap-collabsible {
        margin: 1rem 0;
    }

    .lbl-toggle {
        display: block;
        font-weight: bold;
        /* font-family: monospace; */
        font-size: 0.8rem;
        text-align: left;
        padding: 0rem;
        color: #fec10d;
        background: #ffffff;
        cursor: pointer;
        border-radius: 7px;
        transition: all 0.25s ease-out;
    }

    .lbl-toggle:hover {
        /*color: #FFF;*/
    }

    .lbl-toggle::before {
        content: ' ';
        display: inline-block;
        border-top: 5px solid transparent;
        border-bottom: 5px solid transparent;
        border-left: 5px solid currentColor;
        vertical-align: middle;
        margin-right: .7rem;
        transform: translateY(-2px);
        transition: transform .2s ease-out;
    }

    .toggle:checked+.lbl-toggle::before {
        transform: rotate(90deg) translateX(-3px);
    }

    .collapsible-content {
        max-height: 0px;
        overflow: hidden;
        transition: max-height .25s ease-in-out;
    }

    .toggle:checked+.lbl-toggle+.collapsible-content {
        max-height: 1500px;
    }

    .toggle:checked+.lbl-toggle {
        border-bottom-right-radius: 0;
        border-bottom-left-radius: 0;
    }

    .collapsible-content .content-inner {
        background: white;
        /* rgba(0, 105, 255, .2);*/
        border-bottom: 1px solid white;
        border-bottom-left-radius: 7px;
        border-bottom-right-radius: 7px;
        padding: .5rem 1rem;
    }

    .collapsible-content p {
        margin-bottom: 0;
    }

</style>

<h1>Papers</h1>

<div>
    <table class="styled-table" style="font-size: 0.9em; ">
        <tr>
            <th>Monday, March 14, NZDT, UTC+13</th>
            <th></th>
        </tr>
        
        
        <tr>
            <td style="font-size: 0.9em;"><a href="#1.1">Displays</a></td>
            <td>12:00 - 13:00</td>
        </tr>
        
        
        
        <tr>
            <td style="font-size: 0.9em;"><a href="#1.2">3DUI</a></td>
            <td>14:00 - 15:00</td>
        </tr>
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        <tr>
            <td style="font-size: 0.9em;"><a href="#2.1">Security</a></td>
            <td>12:00 - 13:00</td>
        </tr>
        
        
        
        <tr>
            <td style="font-size: 0.9em;"><a href="#2.2">Locomotion (Americas)</a></td>
            <td>14:00 - 15:00</td>
        </tr>
        
        
        
        <tr>
            <td style="font-size: 0.9em;"><a href="#2.3">Multimodal VR</a></td>
            <td>16:30 - 17:30</td>
        </tr>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <tr>
            <td style="font-size: 0.9em;"><a href="#3.1">Emotion and Cognition</a></td>
            <td>12:00 - 13:00</td>
        </tr>
        
        
        
        <tr>
            <td style="font-size: 0.9em;"><a href="#3.2">Immersive Visualization and Virtual Production</a></td>
            <td>14:00 - 15:00</td>
        </tr>
        
        
        
        
        
        
        
        
        
        
        
        
        
        
    </table>
</div>
<div>
    <table class="styled-table" style="font-size: 0.9em; ">
        <tr>
            <th>Tuesday, March 15, NZDT, UTC+13</th>
            <th></th>
        </tr>
        
        
        
        
        
        
        <tr>
            <td style="font-size: 0.9em;"><a href="#1.3">Embodiment</a></td>
            <td>8:30 - 9:30</td>
        </tr>
        
        
        
        <tr>
            <td style="font-size: 0.9em;"><a href="#1.4">Collaboration</a></td>
            <td>11:00 - 12:00</td>
        </tr>
        
        
        
        <tr>
            <td style="font-size: 0.9em;"><a href="#1.5">Augmented Reality</a></td>
            <td>13:00 - 14:00</td>
        </tr>
        
        
        
        <tr>
            <td style="font-size: 0.9em;"><a href="#1.6">Machine Learning</a></td>
            <td>18:00 - 19:00</td>
        </tr>
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        <tr>
            <td style="font-size: 0.9em;"><a href="#2.4">Presence</a></td>
            <td>8:30 - 9:30</td>
        </tr>
        
        
        
        <tr>
            <td style="font-size: 0.9em;"><a href="#2.5">Perception in AR</a></td>
            <td>11:00 - 12:00</td>
        </tr>
        
        
        
        <tr>
            <td style="font-size: 0.9em;"><a href="#2.6">Locomotion (Asia-Pacific)</a></td>
            <td>13:00 - 14:00</td>
        </tr>
        
        
        
        
        
        
        
        
        
        
        
        <tr>
            <td style="font-size: 0.9em;"><a href="#3.3">Interaction Design</a></td>
            <td>8:30 - 9:30</td>
        </tr>
        
        
        
        <tr>
            <td style="font-size: 0.9em;"><a href="#3.4">Virtual Humans and Agents</a></td>
            <td>11:00 - 12:00</td>
        </tr>
        
        
        
        <tr>
            <td style="font-size: 0.9em;"><a href="#3.5">Perception</a></td>
            <td>13:00 - 14:00</td>
        </tr>
        
        
        
        <tr>
            <td style="font-size: 0.9em;"><a href="#3.6">Medical and Health Care</a></td>
            <td>18:00 - 19:00</td>
        </tr>
        
        
        
        
        
        
    </table>
</div>
<div>
    <table class="styled-table" style="font-size: 0.9em; ">
        <tr>
            <th>Wednesday, March 16, NZDT, UTC+13</th>
            <th></th>
        </tr>
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        <tr>
            <td style="font-size: 0.9em;"><a href="#1.7">Audio in VR</a></td>
            <td>8:30 - 9:30</td>
        </tr>
        
        
        
        <tr>
            <td style="font-size: 0.9em;"><a href="#1.8">Rendering</a></td>
            <td>13:00 - 14:00</td>
        </tr>
        
        
        
        <tr>
            <td style="font-size: 0.9em;"><a href="#1.9">Advanced UI</a></td>
            <td>14:00 - 15:00</td>
        </tr>
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        <tr>
            <td style="font-size: 0.9em;"><a href="#2.7">Haptics</a></td>
            <td>8:30 - 9:30</td>
        </tr>
        
        
        
        <tr>
            <td style="font-size: 0.9em;"><a href="#2.8">Computer Vision</a></td>
            <td>13:00 - 14:00</td>
        </tr>
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        <tr>
            <td style="font-size: 0.9em;"><a href="#3.7">Locomotion (Europe)</a></td>
            <td>8:30 - 9:30</td>
        </tr>
        
        
        
        <tr>
            <td style="font-size: 0.9em;"><a href="#3.8">Negative Effects</a></td>
            <td>13:00 - 14:00</td>
        </tr>
        
        
    </table>
</div>

<!-- 
INVITED MISSING
-->

<div>
    
    

    <h2 id="1.1">Session: Displays</h2>
    

    
    
    <p><strong>Monday, March 14, 12:00, NZDT UTC+13</strong></p>
    
    
        <!-- TAKE ME TO THE EVENT START -->
    <!--
    
    
    <div class="notice--info" style="background-color: $theme-yellow ! important; color: $theme-text ! important;">
        <strong style="padding-bottom: 5px;">Take me to the event:</strong>
        <p>
            <strong style="color: black;">Virbela Location:</strong> Auditorium A (<a href="/2021/attend/virbela-instructions/#map">MAP</a>)

            
            <br />
            
            <strong style="color: black;">Watch the recorded video stream:</strong> <a href="https://youtu.be/zT1-4IDbFr8?t=4100" target="_blank">HERE</a>
            
            
            
            <br />
            <strong style="color: black;">Discord Channel:</strong> <a href="https://discord.com/channels/785628120471699507/825745847366451220" target="_blank">Open in Browser</a>, <a href="discord://discord.com/channels/785628120471699507/825745847366451220">Open in App</a> (Participants only)
            
            
        </p>
    </div>
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    -->
    <!-- TAKE ME TO THE EVENT END-->
    
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1393">Content Presentation on 3D Augmented Reality Windshield Displays in the Context of Automated Driving</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1393" class="wrap-collabsible"> <input id="collapsibleC1393" class="toggle" type="checkbox" /> <label for="collapsibleC1393" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Increasing vehicle automation presents challenges as drivers of automated vehicles become more disengaged from the primary driving task, as there will still be activities that require interfaces for vehicle-passenger interactions. Augmented reality windshield displays provide large-content areas supporting drivers in both driving and non-driving related tasks. Participants of a user study were presented with two modes of content presentation (multiple content-specific windows vs. one main window) in a virtual reality driving simulator. Using one main content window resulted in better task performance and lower take-over times, however, subjective user experience was higher for the multi-window user interface.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    

    <h4 id="C1396">Sparse Nanophotonic Phased Arrays for Energy-Efficient Holographic Displays</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1396" class="wrap-collabsible"> <input id="collapsibleC1396" class="toggle" type="checkbox" /> <label for="collapsibleC1396" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>The Nanophotonic Phased Array (NPA) is an emerging holographic display technology. With chip-scaled sizes, high refresh rates, and integrated light sources, a large-scale NPA can enable high-resolution real-time holography. One of the critical challenges is the high electrical power consumption required to modulate the amplitude and phase of the pixels. We propose a simple method that outputs a sparse NPA configuration to generate the desired image. Using as few as 10% of dense 2D array pixels, we show, through computational simulations, that a perceptually acceptable holographic image can be generated. Our study can advance research on sparse NPAs for holography.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1536">Metameric Varifocal Holograms</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1536" class="wrap-collabsible"> <input id="collapsibleC1536" class="toggle" type="checkbox" /> <label for="collapsibleC1536" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Computer-Generated Holography (CGH) is a promising display technology, but holographic images suffer from noise and artefacts. We propose a new method using gaze-contingency and perceptual graphics to help address these. Firstly, our method infers the user's focal depth and generates images at their focus plane. Second, it displays metamers&#59; in the user's peripheral vision, we only match local statistics of the target. We use a novel metameric loss function with an accurate display model in a gradient descent solver. Our method improves foveal quality, avoiding perceptible artefacts in the periphery. We demonstrate our method on a real prototype holographic display.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="I1004">Design of a Pupil-Matched Occlusion-Capable Optical See-Through Wearable Display</h4>
    <p><strong><small>Invited Journal</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
            
                <p><i>Austin Wilson, Hong Hua</i></p>
            
            
                <p><small>URL: <a href="https://doi.org/10.1109/TVCG.2021.3076069" target="_blank">https://doi.org/10.1109/TVCG.2021.3076069</a></small></p>
            
            
                <div id="I1004" class="wrap-collabsible"> <input id="collapsibleI1004" class="toggle" type="checkbox" /> <label for="collapsibleI1004" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>The state-of-the-art optical see-through head-mounted displays (OST-HMD) for augmented reality applications lack the ability to render correct light interaction behavior between digital and physical objects, known as mutual occlusion capability. This paper presents a novel optical architecture for enabling a compact, high performance, occlusion-capable optical see-through head-mounted display (OCOST-HMD) with correct, pupil-matched viewing perspective. The proposed design utilizes a single-layer, double-pass architecture, offering a compact OCOST-HMD solution that is capable of rendering per-pixel mutual occlusion, a correctly pupil-matched viewing between virtual and real views, and a very wide see-through field of view (FOV). Based on this architecture, we demonstrate a design embodiment and a compact prototype implementation. The prototype offers a virtual display with an FOV of 34 by 22, an angular resolution of 1.06 arc minutes per pixel, and an average image contrast greater than 40% at the Nyquist frequency of 53 cycles/mm. Further, the prototype system affords a wide see-though FOV of 90 by 50, within which about 40 diagonally is occlusion-enabled, along with an angular resolution of 1.0 arc minutes, comparable to a 20/20 vision and a dynamic range greater than 100:1. Lastly, we composed a quantitative color study that compares the effects of occlusion between a conventional HMD system and our OCOST-HMD system and the resulting response exhibited in different studies.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="J1327">Video See-Through Mixed Reality with Focus Cues</h4>
    <p><strong><small>Journal</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Christoph Ebner, Shohei Mori, Peter Mohr, Yifan (Evan) Peng, Dieter Schmalstieg, Gordon Wetzstein, Denis Kalkofen</i></p>
            
            
                <p><small>URL: <a href="https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150504" target="_blank">https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150504</a></small></p>
            
            
                <div id="J1327" class="wrap-collabsible"> <input id="collapsibleJ1327" class="toggle" type="checkbox" /> <label for="collapsibleJ1327" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We introduce the first approach to video see-through mixed reality with support for focus cues. By combining the flexibility to adjust the focus distance found in varifocal designs with the robustness to eye-tracking error of multifocal designs, our novel display architecture delivers focus cues over large workspaces. In particular, we introduce gaze-contingent layered displays and mixed reality focal stacks, an efficient representation of mixed reality content that lends itself to fast processing for driving layered displays in real time. We evaluate this approach by building an end-to-end pipeline for capture, render, and display of focus cues in video see-through displays.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    

    
    
    

    <h2 id="1.2">Session: 3DUI</h2>
    

    
    
    <p><strong>Monday, March 14, 14:00, NZDT UTC+13</strong></p>
    
    
        <!-- TAKE ME TO THE EVENT START -->
    <!--
    
    
    
    
    <div class="notice--info" style="background-color: $theme-yellow ! important; color: $theme-text ! important;">
        <strong style="padding-bottom: 5px;">Take me to the event:</strong>
        <p>
            <strong style="color: black;">Virbela Location:</strong> Auditorium B (<a href="/2021/attend/virbela-instructions/#map">MAP</a>)

            
            <br />
            
            <strong style="color: black;">Watch the recorded video stream:</strong> <a href="https://youtu.be/ob7y6ms48fc?t=4147" target="_blank">HERE</a>
            
            
            
            <br />
            <strong style="color: black;">Discord Channel:</strong> <a href="https://discord.com/channels/785628120471699507/825745888109527050" target="_blank">Open in Browser</a>, <a href="discord://discord.com/channels/785628120471699507/825745888109527050">Open in App</a> (Participants only)
            
            
        </p>
    </div>
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    -->
    <!-- TAKE ME TO THE EVENT END-->
    
    

    
    

    <h4 id="C1014">Bullet Comments for 360&#176; Video</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1014" class="wrap-collabsible"> <input id="collapsibleC1014" class="toggle" type="checkbox" /> <label for="collapsibleC1014" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Time-anchored on-screen comments, as known as bullet comments, are a popular feature for online video streaming. Bullet comments reflect audiences' feelings and opinions at specific video timings, which have been shown to be beneficial to video content understanding and social connection level. In this paper, we for the first time investigate the problem of bullet comment display and insertion for 360&#176; video via head-mounted display and controller. We design four bullet comment display methods and propose two controller-based methods for bullet comment insertion. User study results revealed how the factors of display and insertion methods affect 360&#176; video experience.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1408">An Evaluation of Virtual Reality Maintenance Training for Industrial Hydraulic Machines</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1408" class="wrap-collabsible"> <input id="collapsibleC1408" class="toggle" type="checkbox" /> <label for="collapsibleC1408" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Virtual reality applications for industrial training have widespread benefits for simulating various scenarios and conditions. Through our collaboration with a leading industry partner, a remote multi-user industry maintenance training VR platform applying kinesthetics learning strategy using head-mounted display was designed and implemented. We present the evaluation of the platform with two diverse cohorts of novice users and industry contractors, in comparison to traditional training. The results show that VR training is engaging and effective in boosting trainee's confidence, especially for novice users, with reflections on the differences between novice and industry trainees.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1602">GazeDock: Gaze-Only Menu Selection in Virtual Reality using Auto-Triggering Peripheral Menu</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1602" class="wrap-collabsible"> <input id="collapsibleC1602" class="toggle" type="checkbox" /> <label for="collapsibleC1602" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>In this paper, we proposed GazeDock, a technique for enabling fast and robust gaze-based menu selection in VR. GazeDock features a view-fixed peripheral menu layout that automatically triggers appearing and selection when the user's gaze approaches and leaves the menu zone. By analyzing the user's natural gaze movement patterns, we designed the menu UI personalization and optimized selection detection algorithm of GazeDock. We also examined users' gaze selection precision for targets on the peripheral menu. In a VR navigation game that contains both scene exploration and menu selection, GazeDock received higher user preference ratings compared with dwell-based and pursuit-based techniques.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    

    
    
    
    
    

    <h4 id="I1000">OctoPocus in VR: Using a Dynamic Guide for 3D Mid-Air Gestures in Virtual Reality</h4>
    <p><strong><small>Invited Journal</small></strong></p>

    

    

    

    
        
            
                <p><i>Katherine Fennedy, Jeremy Hartmann, Quentin Roy, Simon Tangi Perrault, Daniel Vogel</i></p>
            
            
                <p><small>URL: <a href="https://doi.org/10.1109/TVCG.2021.3101854" target="_blank">https://doi.org/10.1109/TVCG.2021.3101854</a></small></p>
            
            
                <div id="I1000" class="wrap-collabsible"> <input id="collapsibleI1000" class="toggle" type="checkbox" /> <label for="collapsibleI1000" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Bau and Mackays OctoPocus dynamic guide helps novices learn, execute, and remember 2D surface gestures. We adapt OctoPocus to 3D mid-air gestures in Virtual Reality (VR) using an optimization-based recognizer, and by introducing an optional exploration mode to help visualize the spatial complexity of guides in a 3D gesture set. A replication of the original experiment protocol is used to compare OctoPocus in VR with a VR implementation of a crib-sheet. Results show that despite requiring 0.9s more reaction time than crib-sheet, OctoPocus enables participants to execute gestures 1.8s faster with 13.8 percent more accuracy during training,while remembering a comparable number of gestures. Subjective ratings support these results, 75 percent of participants found OctoPocus easier to learn and 83 percent found it more accurate. We contribute an implementation and empirical evidence demonstrating that an adaptation of the OctoPocus guide to VR is feasible and beneficial.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    

    <h4 id="I1001">EHTask: Recognizing User Tasks from Eye and Head Movements in Immersive Virtual Reality</h4>
    <p><strong><small>Invited Journal</small></strong></p>

    

    

    

    
        
    
        
            
                <p><i>Zhiming Hu, Andreas Bulling, Sheng Li, Guoping Wang</i></p>
            
            
                <p><small>URL: <a href="https://doi.org/10.1109/TVCG.2021.3138902" target="_blank">https://doi.org/10.1109/TVCG.2021.3138902</a></small></p>
            
            
                <div id="I1001" class="wrap-collabsible"> <input id="collapsibleI1001" class="toggle" type="checkbox" /> <label for="collapsibleI1001" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Understanding human visual attention in immersive virtual reality (VR) is crucial for many important applications, including gaze prediction, gaze guidance, and gaze-contingent rendering. However, previous works on visual attention analysis typically only explored one specific VR task and paid less attention to the differences between different tasks. Moreover, existing task recognition methods typically focused on 2D viewing conditions and only explored the effectiveness of human eye movements. We first collect eye and head movements of 30 participants performing four tasks, i.e. Free viewing, Visual search, Saliency, and Track, in 15 360-degree VR videos. Using this dataset, we analyze the patterns of human eye and head movements and reveal significant differences across different tasks in terms of fixation duration, saccade amplitude, head rotation velocity, and eye-head coordination. We then propose EHTask -- a novel learning-based method that employs eye and head movements to recognize user tasks in VR. We show that our method significantly outperforms the state-of-the-art methods derived from 2D viewing conditions both on our dataset (accuracy of 84.4% vs. 62.8%) and on a real-world dataset (61.9% vs. 44.1%). As such, our work provides meaningful insights into human visual attention under different VR tasks and guides future work on recognizing user tasks in VR.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h2 id="2.1">Session: Security</h2>
    

    
    
    <p><strong>Monday, March 14, 12:00, NZDT UTC+13</strong></p>
    
    
        <!-- TAKE ME TO THE EVENT START -->
    <!--
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <div class="notice--info" style="background-color: $theme-yellow ! important; color: $theme-text ! important;">
        <strong style="padding-bottom: 5px;">Take me to the event:</strong>
        <p>
            <strong style="color: black;">Virbela Location:</strong> Auditorium A (<a href="/2021/attend/virbela-instructions/#map">MAP</a>)

            
            <br />
            
            <strong style="color: black;">Watch the recorded video stream:</strong> <a href="https://youtu.be/zT1-4IDbFr8?t=13075" target="_blank">HERE</a>
            
            
            
            <br />
            <strong style="color: black;">Discord Channel:</strong> <a href="https://discord.com/channels/785628120471699507/825745938915786762" target="_blank">Open in Browser</a>, <a href="discord://discord.com/channels/785628120471699507/825745938915786762">Open in App</a> (Participants only)
            
            
        </p>
    </div>
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    -->
    <!-- TAKE ME TO THE EVENT END-->
    
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1263">Virtual Reality Observations: Using Virtual Reality to Augment Lab-Based Shoulder Surfing Research</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1263" class="wrap-collabsible"> <input id="collapsibleC1263" class="toggle" type="checkbox" /> <label for="collapsibleC1263" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We exploit VR's unique characteristics and study the use of non-immersive/immersive VR recordings for real-world shoulder surfing research. We demonstrate the strengths of VR-based shoulder surfing research by exploring three different authentication scenarios: automated-teller-machine (ATM), smartphone PIN, and smartphone pattern authentication. Our results show that applying VR for shoulder surfing research is advantageous in many ways compared to currently used approaches (e.g., lab-based 2D video recordings). We discuss the strengths and weaknesses of using VR for shoulder surfing research and conclude with four recommendations to help researchers decide when (and when not) to employ VR for shoulder surfing research.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    

    <h4 id="C1264">Can I Borrow Your ATM? Using Virtual Reality for (Simulated) In Situ Authentication Research</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1264" class="wrap-collabsible"> <input id="collapsibleC1264" class="toggle" type="checkbox" /> <label for="collapsibleC1264" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>In situ evaluations of novel authentication systems, where the system is evaluated in its intended usage context, are often infeasible due to ethical and legal constraints. In this work, we explore how VR can overcome the shortcomings of authentication studies conducted in the lab and contribute towards more realistic authentication research. Our findings highlight VR's great potential to circumvent potential restrictions researchers experience when evaluating authentication schemes. We provide researchers with a novel research approach to conduct (simulated) in situ authentication research and conclude with three key lessons to support researchers in deciding when to use VR for authentication research.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1314">Combining Real-World Constraints on User Behavior with Deep Neural Networks for Virtual Reality (VR) Biometrics</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1314" class="wrap-collabsible"> <input id="collapsibleC1314" class="toggle" type="checkbox" /> <label for="collapsibleC1314" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Deep networks demonstrate enormous potential for VR security using behavioral biometrics. Existing datasets are small, and make automated learning of real-world behavior features using deep networks challenging. We incorporate real-world constraints such as spatial relationships between devices in the form of displacement vectors and trajectory smoothing in input data for deep networks performing behavior-based identification and authentication. We evaluate our approach against baseline methods that use raw data directly and that perform global normalization. Using displacement vectors, we show higher success over baseline methods in 36 out of 42 cases of varying user sets, VR systems, and sessions.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    

    <h4 id="C1328">HoloLogger: Keystroke Inference on Mixed Reality Head Mounted Displays</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1328" class="wrap-collabsible"> <input id="collapsibleC1328" class="toggle" type="checkbox" /> <label for="collapsibleC1328" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>When using services in mixed reality (MR) such as online payment and social media, sensitive information must be typed in MR. Keystroke inference attacks have been conducted to hack such information. However, previous attacks require placing extra hardware near the user, which is easily noticeable in practice. In this paper, we expose a more dangerous malware-based attack through the vulnerability that no permission is required for accessing MR motion data. Extensive evaluations of our HoloLogger system demonstrate that the proposed attack is accurate and robust in various environments such as different user positions and input categories.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1401">Temporal Effects in Motion Behavior for Virtual Reality (VR) Biometrics</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1401" class="wrap-collabsible"> <input id="collapsibleC1401" class="toggle" type="checkbox" /> <label for="collapsibleC1401" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We evaluate how deep learning approaches for security through matching VR trajectories are influenced by natural behavior variabilities over varying timescales. On short timescales of seconds to minutes, we observe no statistically significant relationship between temporal placement of enrollment trajectories and matches to input trajectories. We observe similar median accuracy for users with high and low medium-scale enrollment/input separation of days to weeks. Over long timescales of 7 to 18 months, we obtain optimal performance for short and long enrollment/input separations by using training sets from users providing long-timescale data, as these sets encompass coarse and fine behavior changes.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1544">A Keylogging Inference Attack on Air-Tapping Keyboards in Virtual Environments</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1544" class="wrap-collabsible"> <input id="collapsibleC1544" class="toggle" type="checkbox" /> <label for="collapsibleC1544" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Enabling users to push the physical world's limits, AR/VR opened a new chapter in perception. Novel immersive experiences resulted in novel interaction methods, which came with unprecedented security and privacy risks. This paper presents a keylogging attack to infer inputs typed with in-air tapping keyboards. We exploit the observation that hands follow specific patterns when typing in-air and build a 5-stage pipeline. Through various experiments, we showed that our attack achieves 40% - 89% accuracy. Finally, we discuss countermeasures, while the results presented provide a cautionary tale of the security and privacy risk of the immersive mobile technology.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    
    
    

    <h2 id="2.2">Session: Locomotion (Americas)</h2>
    

    
    
    <p><strong>Monday, March 14, 14:00, NZDT UTC+13</strong></p>
    
    
        <!-- TAKE ME TO THE EVENT START -->
    <!--
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <div class="notice--info" style="background-color: $theme-yellow ! important; color: $theme-text ! important;">
        <strong style="padding-bottom: 5px;">Take me to the event:</strong>
        <p>
            <strong style="color: black;">Virbela Location:</strong> Auditorium B (<a href="/2021/attend/virbela-instructions/#map">MAP</a>)

            
            <br />
            
            <strong style="color: black;">Watch the recorded video stream:</strong> <a href="https://youtu.be/ob7y6ms48fc?t=13061" target="_blank">HERE</a>
            
            
            
            <br />
            <strong style="color: black;">Discord Channel:</strong> <a href="https://discord.com/channels/785628120471699507/825745991889453056" target="_blank">Open in Browser</a>, <a href="discord://discord.com/channels/785628120471699507/825745991889453056">Open in App</a> (Participants only)
            
            
        </p>
    </div>
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    -->
    <!-- TAKE ME TO THE EVENT END-->
    
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1244">Research Trends in Virtual Reality Locomotion Techniques</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1244" class="wrap-collabsible"> <input id="collapsibleC1244" class="toggle" type="checkbox" /> <label for="collapsibleC1244" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Virtual reality (VR) researchers have had a long-standing interest in studying locomotion for developing new techniques, improving upon prior ones, and analyzing their effects on users. In this paper, we present a systematic review of locomotion techniques based on a well-established taxonomy, and we use k-means clustering to identify to what extent locomotion techniques have been explored. Our results indicate that selection-based, walking-based, and steering-based locomotion techniques have been moderately to highly explored while manipulation-based and automated locomotion techniques have been less explored. We also present results on what types of metrics have been used to evaluate locomotion techniques.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1315">ENI: Quantifying Environment Compatibility for Natural Walking in Virtual Reality</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1315" class="wrap-collabsible"> <input id="collapsibleC1315" class="toggle" type="checkbox" /> <label for="collapsibleC1315" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We present a metric to measure the similarity between physical and virtual environments for natural walking in VR. We use geometric techniques based on visibility polygons to compute the Environment Navigation Incompatibility (ENI) metric to measure the complexity VR locomotion. ENI is useful for highlighting regions of incompatibility and guiding the design of the virtual environments to make them more compatible. User studies and simulations show that ENI identifies environments where users are able to walk larger distances before colliding with objects. ENI is the first general metric that automatically quantifies environment navigability for VR locomotion. Project page: gamma.umd.edu/eni</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1595">Evaluating the Impact of Limited Physical Space on the Navigation Performance of Two Locomotion Methods in Immersive Virtual Environments</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1595" class="wrap-collabsible"> <input id="collapsibleC1595" class="toggle" type="checkbox" /> <label for="collapsibleC1595" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Consumer level virtual experiences almost always occur when physical space is limited, either by the constraints of an indoor space or of a tracked area. Some locomotion interfaces support movement in the real world, while some do not. This paper examines how the amount of physical space used in the real world by one popular locomotion interface, resetting, compared to a locomotion interface that requires minimal physical space, walking in place. We compared them by determining how well people could acquire survey knowledge using them. While there are trade-offs between the methods, walking in place is preferable in small spaces.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="J1039">Remote research on locomotion interfaces for virtual reality: Replication of lab-based research on the teleporting interface</h4>
    <p><strong><small>Journal</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Jonathan Kelly, Melynda Hoover, Taylor A Doty, Alex Renner, Moriah Zimmerman, Kimberly Knuth, Lucia Cherep, Stephen B. Gilbert</i></p>
            
            
                <p><small>URL: <a href="https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150475" target="_blank">https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150475</a></small></p>
            
            
                <div id="J1039" class="wrap-collabsible"> <input id="collapsibleJ1039" class="toggle" type="checkbox" /> <label for="collapsibleJ1039" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Researchers can now recruit virtual reality (VR) equipment owners to participate remotely. Yet, there are many differences between lab and home environments, as well as differences between participant samples recruited for lab and remote studies. This project replicates a lab-based experiment on VR locomotion interfaces using a remote sample. Participants completed a triangle-completion task (travel two path legs, then point to the path origin) in a remote, unsupervised setting. Locomotion was accomplished using two versions of the teleporting interface varying in available rotational self-motion cues. Remote results largely mirrored lab results, with better performance when rotational cues were available.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="J1334">Adaptive Redirection: A Context-Aware Redirected Walking Meta-Strategy</h4>
    <p><strong><small>Journal</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Mahdi Azmandian, Rhys Yahata, Timofey Grechkin, Evan Suma Rosenberg</i></p>
            
            
                <p><small>URL: <a href="https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150500" target="_blank">https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150500</a></small></p>
            
            
                <div id="J1334" class="wrap-collabsible"> <input id="collapsibleJ1334" class="toggle" type="checkbox" /> <label for="collapsibleJ1334" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This work establishes the theoretical foundations for adaptive redirection, a meta-strategy that switches between a suite of redirected walking strategies with a priori knowledge of their performance under the various circumstances. We also introduce a novel static planning strategy that optimizes gain parameters for a predetermined virtual path and conduct a simulation-based experiment that demonstrates how adaptation rules can be determined empirically using machine learning. Adaptive redirection provides a foundation for making redirected walking work in practice and can be extended to improve performance in the future as new techniques are integrated into the framework.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    

    
    
    

    <h4 id="J1335">Validating Simulation-Based Evaluation of Redirected Walking Systems</h4>
    <p><strong><small>Journal</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Mahdi Azmandian, Rhys Yahata, Timofey Grechkin, Jerald Thomas, Evan Suma Rosenberg</i></p>
            
            
                <p><small>URL: <a href="https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150466" target="_blank">https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150466</a></small></p>
            
            
                <div id="J1335" class="wrap-collabsible"> <input id="collapsibleJ1335" class="toggle" type="checkbox" /> <label for="collapsibleJ1335" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We present an experiment comparing redirected walking simulation and live user data to understand interaction between locomotion behavior and redirection gains at a micro-level (across small path segments) and macro-level (across an entire experience). The results identify specific properties of locomotion behavior that influence the application of redirected walking. Overall, we found that the simulation provided a conservative estimate of the average performance with real users and observed that performance trends when comparing two redirected walking algorithms were preserved. In general, these results indicate that simulation is an empirically valid evaluation methodology for redirected walking algorithms.</p>
                        </div>
                    </div>
                </div>
            
        
    

    
    

    
    
    

    <h2 id="2.3">Session: Multimodal VR</h2>
    

    
    
    <p><strong>Monday, March 14, 16:30, NZDT UTC+13</strong></p>
    
    
        <!-- TAKE ME TO THE EVENT START -->
    <!--
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    -->
    <!-- TAKE ME TO THE EVENT END-->
    
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1208">Evaluating Visual Cues for Future Airborne Surveillance Using Simulated Augmented Reality Displays</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1208" class="wrap-collabsible"> <input id="collapsibleC1208" class="toggle" type="checkbox" /> <label for="collapsibleC1208" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This work explores bringing Augmented Reality (AR) to airborne surveillance operators. In a Virtual Reality (VR) simulation replicating the environment of surveillance aircrafts, we introduce AR cues and an AR control panel to support search tasks and secondary tasks, respectively. Our high-fidelity simulation factors in the focal plane of the AR technology and simulates the user's eye accommodation reflex. Results collected from 24 participants show that the effectiveness of the cues depends on the modality of the secondary task and that, under certain situations, choosing the right focal plane for the AR display may improve search task performances.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    

    <h4 id="C1216">Empirical Evaluation of Calibration and Long-term Carryover Effects of Reverberation on Egocentric Auditory Depth Perception in VR</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1216" class="wrap-collabsible"> <input id="collapsibleC1216" class="toggle" type="checkbox" /> <label for="collapsibleC1216" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We conducted a study to understand the perceptual learning and carryover effects by using RT as stimuli for users to perceive distance in IVEs. The results show that the carryover effect exists after calibration, which indicates people can learn to perceive distances by attuning reverberation time, and the accuracy even remains a constant level after 6 months.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1341">Simulating Olfactory Cocktail Party Effect in VR: A Multi-odor Display Approach Based on Attention</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1341" class="wrap-collabsible"> <input id="collapsibleC1341" class="toggle" type="checkbox" /> <label for="collapsibleC1341" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We present a VR multi-odor display approach that dynamically changes the intensity combinations of different scent sources in the virtual environment according to the user's attention, hence simulating a virtual cocktail party effect of smell. We acquire attention from the eye-tracking sensors and increase the display intensity of the scent that the user is focusing on to simulate the cocktail party effect of smell, enabling the user to distinguish their desired scent source. The user study showed our approach was able to improve olfactory experience and sense of presence in VR compared to the non-dynamic odor display method.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1443">Shape Aware Haptic Retargeting for Accurate Hand Interactions</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1443" class="wrap-collabsible"> <input id="collapsibleC1443" class="toggle" type="checkbox" /> <label for="collapsibleC1443" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Shape Aware Haptic Retargeting (SAHR) is an extension of &quot;state-of-the-art&quot; haptic retargeting and the first to support retargeted interaction between any part of the user's hand and any part of the target object. In previous methods, the maximum retargeting is applied only when the hand position aligns with the target position. SAHR generalizes the distance computation to consider the full hand and target geometry. Simulated evaluations demonstrated SAHR can provide improved interaction accuracy over existing methods with full mesh geometry being the most accurate and a primitive approximation being the preferred method for combined computational performance and interaction accuracy.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="I1009">Adaptive Reset Techniques for Haptic Retargeted Interaction</h4>
    <p><strong><small>Invited Journal</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Brandon Matthews, Bruce H Thomas, Stewart Von Itzstein, Ross Smith</i></p>
            
            
                <p><small>URL: <a href="https://doi.org/10.1109/TVCG.2021.3120410" target="_blank">https://doi.org/10.1109/TVCG.2021.3120410</a></small></p>
            
            
                <div id="I1009" class="wrap-collabsible"> <input id="collapsibleI1009" class="toggle" type="checkbox" /> <label for="collapsibleI1009" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This paper presents a set of adaptive reset techniques for use with haptic retargeting systems focusing on interaction with hybrid virtual reality interfaces that align with a physical interface. Haptic retargeting between changing physical and virtual targets requires a reset where the physical and virtual hand positions are re-aligned. We present a modified Point technique to guide the user in the direction of their next interaction such that the remaining distance to the target is minimized upon completion of the reset. This, along with techniques drawn from existing work are further modified to consider the angular and translational gain of each redirection and identify the optimal position for the reset to take place. When the angular and translational gain is within an acceptable range, the reset can be entirely omitted. This enables continuous retargeting between targets removing interruptions from a sequence of retargeted interactions. These techniques were evaluated in a user study which showed that adaptive reset techniques can provide a significant decrease in task completion time, travel distance, and the number of user errors.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="J1087">Studying the Effects of Congruence of Auditory and Visual Stimuli on Virtual Reality Experiences</h4>
    <p><strong><small>Journal</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Hayeon Kim, In-Kwon Lee</i></p>
            
            
                <p><small>URL: <a href="https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150514" target="_blank">https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150514</a></small></p>
            
            
                <div id="J1087" class="wrap-collabsible"> <input id="collapsibleJ1087" class="toggle" type="checkbox" /> <label for="collapsibleJ1087" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This paper explores how the congruence between auditory and visual (AV) stimuli, which are the sensory stimuli typically provided by VR devices. We defined the types of (in)congruence between AV stimuli, and then designed 12 virtual spaces with different degrees of congruence between AV stimuli with evaluating user experience changes. We observed the following key findings: 1) there is a limit to the degree of temporal or spatial incongruence that can be tolerated&#59; 2) users are tolerant of semantic incongruence&#59; 3) a simulation that considers synesthetic congruence contributes to the user's sense of immersion and presence.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    
    
    
    
    
    
    
    
    
    
    
    
    

    <h2 id="3.1">Session: Emotion and Cognition</h2>
    

    
    
    <p><strong>Monday, March 14, 12:00, NZDT UTC+13</strong></p>
    
    
        <!-- TAKE ME TO THE EVENT START -->
    <!--
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <div class="notice--info" style="background-color: $theme-yellow ! important; color: $theme-text ! important;">
        <strong style="padding-bottom: 5px;">Take me to the event:</strong>
        <p>
            <strong style="color: black;">Virbela Location:</strong> Auditorium A (<a href="/2021/attend/virbela-instructions/#map">MAP</a>)

            
            <br />
            
            <strong style="color: black;">Watch the recorded video stream:</strong> <a href="https://youtu.be/Ag3LRcqxpo4?t=1805" target="_blank">HERE</a>
            
            
            
            <br />
            <strong style="color: black;">Discord Channel:</strong> <a href="https://discord.com/channels/785628120471699507/825744740246880307" target="_blank">Open in Browser</a>, <a href="discord://discord.com/channels/785628120471699507/825744740246880307">Open in App</a> (Participants only)
            
            
        </p>
    </div>
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    -->
    <!-- TAKE ME TO THE EVENT END-->
    
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1098">Do You Notice Me? How Bystanders Affect the Cognitive Load in Virtual Reality</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1098" class="wrap-collabsible"> <input id="collapsibleC1098" class="toggle" type="checkbox" /> <label for="collapsibleC1098" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>In contrast to the real world, users are not able to perceive bystanders in virtual reality. This involves users to feel discomfort at the thought of unintentionally touching or even bumping into a physical bystander while interacting with the virtual environment. Therefore, we investigate how a bystander affects a user's cognitive load. In a between-subjects lab study, three conditions were compared: 1) no bystander, 2) an invisible bystander, and 3) a visible bystander. The results of our study demonstrate that a bystander acting as an avatar in the virtual environment increases the user's cognitive load more than an invisible bystander. Moreover, the cognitive load of a VR user is significantly increased by a bystander.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1172">Empathic AuRea: Exploring the Effects of an Augmented Reality Cue for Emotional Sharing Across Three Face-to-Face Tasks</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1172" class="wrap-collabsible"> <input id="collapsibleC1172" class="toggle" type="checkbox" /> <label for="collapsibleC1172" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>The better a speaker can understand their listener's emotions, the better can they transmit information&#59; and the better a listener can understand the speaker's emotions, the better can they apprehend that information. Previous emotional sharing works have managed to elicit emotional understanding between remote collaborators using bio-sensing, but how face-to-face communication can benefit from bio-feedback is still fairly unexplored. This paper introduces an AR communication cue from an emotion recognition neural network model and ECG data. A study where pairs of participants engaged in three tasks found our system to positively effect performance and emotional understanding, but negatively effect memorization.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1442">Supporting Jury Understanding of Expert Evidence in a Virtual Environment</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1442" class="wrap-collabsible"> <input id="collapsibleC1442" class="toggle" type="checkbox" /> <label for="collapsibleC1442" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This work investigates the use of Virtual Reality (VR) to present forensic evidence to the jury in a courtroom trial. We performed a between-participant study comparing comprehension of an expert statement in VR to the traditional courtroom presentation of still images. We measured understanding of the expert domain, mental effort and content recall and found that VR significantly improves the understanding of spatial information and knowledge acquisition. We also identify different patterns of user behaviour depending on the display method. We conclude with suggestions on how to best adapt evidence presentation to VR.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="I1006">A Wheelchair Locomotion Interface in a VR Disability Simulation Reduces Implicit Bias</h4>
    <p><strong><small>Invited Journal</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Tanvir Irfan Chowdhury, John Quarles</i></p>
            
            
                <p><small>URL: <a href="https://doi.org/10.1109/TVCG.2021.3099115" target="_blank">https://doi.org/10.1109/TVCG.2021.3099115</a></small></p>
            
            
                <div id="I1006" class="wrap-collabsible"> <input id="collapsibleI1006" class="toggle" type="checkbox" /> <label for="collapsibleI1006" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This research investigates how experiencing virtual embodiment in a wheelchair affects implicit bias towards people who use wheelchairs. We also investigate how receiving information from a virtual instructor who uses a wheelchair affects implicit bias towards people who use wheelchairs. Implicit biases are actions or judgments of people towards various concepts or stereotypes (e.g., races). We hypothesized that experiencing a Disability Simulation (DS) through an avatar in a wheelchair and receiving information from an instructor with a disability will have a significant effect on participants' ability to recall disability-related information and will reduce implicit biases towards people who use wheelchairs. To investigate this hypothesis, a 2x2 between-subjects user study was conducted where participants experienced an immersive VR DS that presents information about Multiple Sclerosis (MS) with factors of instructor (i.e., instructor with a disability vs instructor without a disability) and locomotion interface (i.e., without a disability -- locomotion through in-place-walking, with a disability -- locomotion in a wheelchair). Participants took a disability-focused Implicit Association Test two times, once before and once after experiencing the DS. They also took a test of knowledge retention about MS. The primary result is: experiencing the DS through locomotion in a wheelchair was better for both the disability-related information recall task and reducing implicit bias towards people who use wheelchairs.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="J1072">Mood-Driven Colorization of Virtual Indoor Scenes</h4>
    <p><strong><small>Journal</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Michael S Solah, Haikun Huang, Jiachuan Sheng, Tian Feng, Marc Pomplun, Lap-Fai Yu</i></p>
            
            
                <p><small>URL: <a href="https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150513" target="_blank">https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150513</a></small></p>
            
            
                <div id="J1072" class="wrap-collabsible"> <input id="collapsibleJ1072" class="toggle" type="checkbox" /> <label for="collapsibleJ1072" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>A challenging task in virtual scene design for Virtual Reality (VR) is invoking particular moods in viewers. The subjective nature of moods brings uncertainty to this purpose. We propose a novel approach for automatic color adjustment of textures for objects in virtual indoor scenes, enabling them to match target moods. A dataset of 25,000 indoor environment images was used to train a classifier with features extracted via deep learning. We use an optimization process that colorizes virtual scenes automatically according to the target mood. Our approach was tested on four indoor scenes used in user studies with a VR headset.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    
    
    

    <h2 id="3.2">Session: Immersive Visualization and Virtual Production</h2>
    

    
    
    <p><strong>Monday, March 14, 14:00, NZDT UTC+13</strong></p>
    
    
        <!-- TAKE ME TO THE EVENT START -->
    <!--
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <div class="notice--info" style="background-color: $theme-yellow ! important; color: $theme-text ! important;">
        <strong style="padding-bottom: 5px;">Take me to the event:</strong>
        <p>
            <strong style="color: black;">Virbela Location:</strong> Auditorium B (<a href="/2021/attend/virbela-instructions/#map">MAP</a>)

            
            <br />
            
            <strong style="color: black;">Watch the recorded video stream:</strong> <a href="https://youtu.be/vU1ooD8uu3A?t=1778" target="_blank">HERE</a>
            
            
            
            <br />
            <strong style="color: black;">Discord Channel:</strong> <a href="https://discord.com/channels/785628120471699507/825744882605621278" target="_blank">Open in Browser</a>, <a href="discord://discord.com/channels/785628120471699507/825744882605621278">Open in App</a> (Participants only)
            
            
        </p>
    </div>
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    -->
    <!-- TAKE ME TO THE EVENT END-->
    
    

    
    
    
    
    
    
    
    

    <h4 id="C1046">The Virtual Production Studio Concept - An Emerging Game Changer in filmmaking</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1046" class="wrap-collabsible"> <input id="collapsibleC1046" class="toggle" type="checkbox" /> <label for="collapsibleC1046" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Virtual Production (VP) integrates virtual and augmented reality technologies with CGI and VFX using a game engine to enable on set production crews to capture and unwrap scenes in real time. In this sense, it is a game changer for the traditional film industry. This paper analyses the VP studio and the use of LED as the pivotal element of this process from a technical and conceptual perspective. As the interest in the field is growing rapidly, we endeavour to evaluate the developments to date in Australia to build a snapshot of emerging VP practices and research supported by two case studies.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1102">Exploring the Design Space for Immersive Embodiment in Dance</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1102" class="wrap-collabsible"> <input id="collapsibleC1102" class="toggle" type="checkbox" /> <label for="collapsibleC1102" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>How can virtual reality (VR) support people in more deeply inhabiting their own bodies? Through action research, we explored the design space for how VR can support feeling embodied and generating movement within a dance context. We observed and reflected on participants' experiences with games intended to stimulate physical movement, single player and multiplayer 3D painting, 360 live video, and custom real-time audio and visual feedback based on motion capture. We present a design space that encapsulates our insights, with axes for control, collaboration, auditory and visual feedback. We discuss implications for the support of immersive embodiment in dance.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1168">Mixed Reality Co-Design for Indigenous Culture Preservation &amp; Continuation</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1168" class="wrap-collabsible"> <input id="collapsibleC1168" class="toggle" type="checkbox" /> <label for="collapsibleC1168" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Decades of M&#x101;ori urbanisation, colonisation and globalisation have dispersed marae communities away from their tribal home all around NZ and overseas. This has left many feeling disconnected from learning their cultural identity. M&#x101;ori have sought digital solutions to mitigate this problem by finding ways of preserving and continuing their culture using immersive technologies. We achieve this by developing an application which places M&#x101;ori back into their ancestral meeting house and allow them to hear 3D stories about their people. For other researchers and developers involved in an indigenous project, we recommend using a participatory co-design process when developing MR applications for indigenous preservation and continuation.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1429">TimeTables: Embodied Exploration of Immersive Spatio-Temporal Data</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1429" class="wrap-collabsible"> <input id="collapsibleC1429" class="toggle" type="checkbox" /> <label for="collapsibleC1429" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We propose TimeTables, a novel prototype system that aims to support data exploration, using embodiment with space-time cubes in virtual reality. TimeTables uses multiple space-time cubes on virtual tabletops, which users can manipulate by extracting time layers to create new tabletop views. The system presents information at different time scales by stretching layers to drill down in time. Users can also jump into tabletops to inspect data from an egocentric perspective. From our use case scenario of energy consumption displayed on a university campus, we believe the system has a high potential in supporting spatio-temporal data exploration and analysis.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="I1007">Virtual replicas of real places: Experimental investigations</h4>
    <p><strong><small>Invited Journal</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Richard Skarbez, Joe Gabbard, Doug A Bowman, Todd Ogle, Thomas Tucker</i></p>
            
            
                <p><small>URL: <a href="https://doi.org/10.1109/TVCG.2021.3096494" target="_blank">https://doi.org/10.1109/TVCG.2021.3096494</a></small></p>
            
            
                <div id="I1007" class="wrap-collabsible"> <input id="collapsibleI1007" class="toggle" type="checkbox" /> <label for="collapsibleI1007" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>As virtual reality (VR) technology becomes cheaper, higher-quality, and more widely available, it is seeing increasing use in a variety of applications including cultural heritage, real estate, and architecture. A common goal for all these applications is a compelling virtual recreation of a real place. Despite this, there has been very little research into how users perceive and experience such replicated spaces. This paper reports the results from a series of three user studies investigating this topic. Results include that the scale of the room and large objects in it are most important for users to perceive the room as real and that non-physical behaviors such as objects floating in air are readily noticeable and have a negative effect even when the errors are small in scale.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="J1131">VirtualCube: An Immersive 3D Video Communication System</h4>
    <p><strong><small>Journal</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Yizhong Zhang, Jiaolong Yang, Zhen Liu, Ruicheng Wang, Guojun Chen, Xin Tong, Baining Guo</i></p>
            
            
            
                <div id="J1131" class="wrap-collabsible"> <input id="collapsibleJ1131" class="toggle" type="checkbox" /> <label for="collapsibleJ1131" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>The VirtualCube system is a 3D video conference system that attempts to overcome some limitations of conventional technologies. The physical setup of VirtualCube is a standardized cubicle installed with off-the-shelf hardware including 3 TV displays and 6 RGBD cameras. With high-quality 3D capturing and rendering algorithm, the system teleports the remote participants into a virtual meeting room to achieve immersive in-person meeting experience with correct eye gaze. A set of VirtualCubes can be easily assembled into a V-Cube Assembly to model different video communication and shared workspace scenarios, as if all participants were in the same room.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
</div>

<div>
    
    
    
    
    
    

    <h2 id="1.3">Session: Embodiment</h2>
    

    
    
    <p><strong>Tuesday, March 15, 8:30, NZDT UTC+13</strong></p>
    
    
        <!-- TAKE ME TO THE EVENT START -->
    <!--
    
    
    
    
    
    
    <div class="notice--info" style="background-color: $theme-yellow ! important; color: $theme-text ! important;">
        <strong style="padding-bottom: 5px;">Take me to the event:</strong>
        <p>
            <strong style="color: black;">Virbela Location:</strong> Auditorium A (<a href="/2021/attend/virbela-instructions/#map">MAP</a>)

            
            <br />
            
            <strong style="color: black;">Watch the recorded video stream:</strong> <a href="https://youtu.be/Ds-h1J4MFMI?t=4495" target="_blank">HERE</a>
            
            
            
            <br />
            <strong style="color: black;">Discord Channel:</strong> <a href="https://discord.com/channels/785628120471699507/825746204645261343" target="_blank">Open in Browser</a>, <a href="discord://discord.com/channels/785628120471699507/825746204645261343">Open in App</a> (Participants only)
            
            
        </p>
    </div>
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    -->
    <!-- TAKE ME TO THE EVENT END-->
    
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1082">Visual Fidelity Effects on Expressive Self-avatar in Virtual Reality: First Impressions Matter</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1082" class="wrap-collabsible"> <input id="collapsibleC1082" class="toggle" type="checkbox" /> <label for="collapsibleC1082" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>In this work we present a technical pipeline of creating self-alike avatars whose facial expressions can be then controlled in real-time, from inside the HMD. Using this pipeline we conducted two within-group studies on the psychological impact of the appearance realism of self-avatar. We found a &quot;first trial effect&quot; in both studies where participants gave more positive feedback over the avatar from their first trial, regardless of it being realistic or cartoon-like. Our eye-tracking data suggested that although participants were mainly facing their avatar during their presentation, their eye-gaze was focused elsewhere half of the time.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1198">Within or Between? Comparing Experimental Designs for Virtual Embodiment Studies</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1198" class="wrap-collabsible"> <input id="collapsibleC1198" class="toggle" type="checkbox" /> <label for="collapsibleC1198" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This paper reports a within-subjects experiment with 92 participants comparing embodiment under a visuomotor task with two conditions: synchronous and asynchronous motions. We explored the impact of the number of participants on the replicability of the results from the 92 within-subjects experiment. Results showed that while the replicability of the results increased with the number of participants for the within-subjects simulations, no matter the number of participants, between-subjects simulations were not able to replicate the initial results. We discuss potential reasons that could have lead to this and potential methodological practices to mitigate them.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="I1005">Being an Avatar &quot;for Real&quot;: a Survey on Virtual Embodiment in Augmented Reality</h4>
    <p><strong><small>Invited Journal</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Adelaide Charlotte Sarah Genay, Anatole Lecuyer, Martin Hachet</i></p>
            
            
                <p><small>URL: <a href="https://doi.org/10.1109/TVCG.2021.3099290" target="_blank">https://doi.org/10.1109/TVCG.2021.3099290</a></small></p>
            
            
                <div id="I1005" class="wrap-collabsible"> <input id="collapsibleI1005" class="toggle" type="checkbox" /> <label for="collapsibleI1005" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Virtual self-avatars have been increasingly used in Augmented Reality (AR) where one can see virtual content embedded into physical space. However, little is known about the perception of self-avatars in such a context. The possibility that their embodiment could be achieved in a similar way as in Virtual Reality opens the door to numerous applications in education, communication, entertainment, or the medical field. This article aims to review the literature covering the embodiment of virtual self-avatars in AR. Our goal is (i) to guide readers through the different options and challenges linked to the implementation of AR embodiment systems, (ii) to provide a better understanding of AR embodiment perception by classifying the existing knowledge, and (iii) to offer insight on future research topics and trends for AR and avatar research. To do so, we introduce a taxonomy of virtual embodiment experiences by defining a &quot;body avatarization&quot; continuum. The presented knowledge suggests that the sense of embodiment evolves in the same way in AR as in other settings, but this possibility has yet to be fully investigated. We suggest that, whilst it is yet to be well understood, the embodiment of avatars has a promising future in AR and conclude by discussing possible directions for research.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="J1050">Do You Need Another Hand? Investigating Dual Body Representations During Anisomorphic 3D Manipulation</h4>
    <p><strong><small>Journal</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Diane Dewez, Ludovic Hoyet, Anatole L&eacute;cuyer, Ferran Argelaguet Sanz</i></p>
            
            
                <p><small>URL: <a href="https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150501" target="_blank">https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150501</a></small></p>
            
            
                <div id="J1050" class="wrap-collabsible"> <input id="collapsibleJ1050" class="toggle" type="checkbox" /> <label for="collapsibleJ1050" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Manipulation techniques that distort motion can negatively impact the sense of embodiment as they create a mismatch between the real action and the displayed action. In this paper, we propose to use a dual representation during anisomorphic interaction. A co-located representation reproduces the users' motion, while an interactive representation is used for distorted interaction. We conducted two experiments, investigating the use of dual representations with amplified motion (with the Go-Go technique) and decreased motion (with the PRISM technique). Two visual appearances for the interactive representation and the co-located one were explored (ghost and realistic). </p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    

    <h4 id="J1093">The Impact of Embodiment and Avatar Sizing on Personal Space in Immersive Virtual Environments</h4>
    <p><strong><small>Journal</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Lauren Buck, Soumyajit Chakraborty, Bobby Bodenheimer</i></p>
            
            
                <p><small>URL: <a href="https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150483" target="_blank">https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150483</a></small></p>
            
            
                <div id="J1093" class="wrap-collabsible"> <input id="collapsibleJ1093" class="toggle" type="checkbox" /> <label for="collapsibleJ1093" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Our work examines how degree of embodiment and avatar sizing affect the way personal space is perceived in virtual reality. We look at two components of personal space: interpersonal and peripersonal space. We hypothesized that higher levels of embodiment would result in differing measures of interpersonal and peripersonal space, and that, only interpersonal space would change with arm length. We found that interpersonal and peripersonal space change in the presence of differing levels of embodiment, and that only interpersonal space is sensitive to changes in arm dimension. These findings provide understanding for improved design of social interaction in virtual environments.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    
    
    

    <h2 id="1.4">Session: Collaboration</h2>
    

    
    
    <p><strong>Tuesday, March 15, 11:00, NZDT UTC+13</strong></p>
    
    
        <!-- TAKE ME TO THE EVENT START -->
    <!--
    
    
    
    
    
    
    
    
    <div class="notice--info" style="background-color: $theme-yellow ! important; color: $theme-text ! important;">
        <strong style="padding-bottom: 5px;">Take me to the event:</strong>
        <p>
            <strong style="color: black;">Virbela Location:</strong> Auditorium B (<a href="/2021/attend/virbela-instructions/#map">MAP</a>)

            
            <br />
            
            <strong style="color: black;">Watch the recorded video stream:</strong> <a href="https://youtu.be/ktb20KDP8Xs?t=4475" target="_blank">HERE</a>
            
            
            
            <br />
            <strong style="color: black;">Discord Channel:</strong> <a href="https://discord.com/channels/785628120471699507/825746265789694021" target="_blank">Open in Browser</a>, <a href="discord://discord.com/channels/785628120471699507/825746265789694021">Open in App</a> (Participants only)
            
            
        </p>
    </div>
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    -->
    <!-- TAKE ME TO THE EVENT END-->
    
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1178">The Potential of VR-based Tactical Resource Planning on Spatial Data</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1178" class="wrap-collabsible"> <input id="collapsibleC1178" class="toggle" type="checkbox" /> <label for="collapsibleC1178" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>In this work, we leverage the benefits of immersive head-mounted displays (HMDs) and present the design, implementation, and evaluation of a collaborative Virtual Reality (VR) application for tactical resource planning on spatial data. We derived system and design requirements from observations of a military on-site staff exercise and evaluated our prototype, we conducted semi-structured interviews with domain experts. Our results show that the potential of VR-based tactical resource planning is dependent on the technical features as well as on non-technical environmental aspects, such as user attitude, prior experience, and interoperability.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1473">Virtual Workspace Positioning Techniques during Teleportation for Co-located Collaboration in Virtual Reality using HMDs</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1473" class="wrap-collabsible"> <input id="collapsibleC1473" class="toggle" type="checkbox" /> <label for="collapsibleC1473" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Many co-located collaborative Virtual Reality applications rely on a one-to-one mapping of users' relative positions in real and virtual environments. However, the users' individual virtual navigation may break this spatial configuration. This work enables the recovery of spatial consistency after individual navigation. We provide a virtual representation of the users' shared physical workspace and developed two techniques to position it. The first technique enables a single user to control the virtual workspace location, while the second allows concurrent manipulation. Experimental results show that allowing two users to co-manipulate the virtual workspace significantly reduces negotiation time.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="J1010">SEAR: Scaling Experiences in Multi-user Augmented Reality</h4>
    <p><strong><small>Journal</small></strong></p>

    

    

    

    
        
            
                <p><i>Wenxiao ZHANG, Bo Han, Pan Hui</i></p>
            
            
                <p><small>URL: <a href="https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150467" target="_blank">https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150467</a></small></p>
            
            
                <div id="J1010" class="wrap-collabsible"> <input id="collapsibleJ1010" class="toggle" type="checkbox" /> <label for="collapsibleJ1010" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>In this paper, we present SEAR, a collaborative framework for Scaling Experiences in multi-user Augmented Reality (AR). Most AR systems benefit from computer vision algorithms to detect/classify/recognize physical objects for augmentation. A widely-used acceleration method is to offload compute-intensive tasks to the network edge. However, we show that the end-to-end latency, an important metric of mobile AR, may dramatically increase when offloading tasks from a large number of concurrent users. SEAR tackles this scalability issue through the innovation of a lightweight collaborative local cache. We build a prototype of SEAR to demonstrate its efficacy in scaling AR experiences.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="J1226">Duplicated Reality for Co-located Augmented Reality Collaboration</h4>
    <p><strong><small>Journal</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Kevin Yu, Ulrich Eck, Frieder Pankratz, Marc Lazarovici, Dirk Wilhelm, Nassir Navab</i></p>
            
            
                <p><small>URL: <a href="https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150520" target="_blank">https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150520</a></small></p>
            
            
                <div id="J1226" class="wrap-collabsible"> <input id="collapsibleJ1226" class="toggle" type="checkbox" /> <label for="collapsibleJ1226" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>When multiple users collaborate in the same space with Augmented Reality, they often encounter conflicting intentions regarding the occupation of the working area. For relaxing constraints of physical co-location, we propose Duplicated Reality, where a digital copy of a 3D region of interest is reconstructed in real-time and visualized through Augmented Reality. We compare the proposed method to an in-situ augmentation. The result indicates almost identical metrics, except a decrease in the consulter's awareness of co-located users when using our method. Duplicating the working area into a designated consulting area opens up new paradigms for future co-located Augmented Reality systems.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    
    
    

    <h2 id="1.5">Session: Augmented Reality</h2>
    

    
    
    <p><strong>Tuesday, March 15, 13:00, NZDT UTC+13</strong></p>
    
    
        <!-- TAKE ME TO THE EVENT START -->
    <!--
    
    
    
    
    
    
    
    
    
    
    <div class="notice--info" style="background-color: $theme-yellow ! important; color: $theme-text ! important;">
        <strong style="padding-bottom: 5px;">Take me to the event:</strong>
        <p>
            <strong style="color: black;">Virbela Location:</strong> Auditorium A (<a href="/2021/attend/virbela-instructions/#map">MAP</a>)

            
            <br />
            
            <strong style="color: black;">Watch the recorded video stream:</strong> <a href="https://youtu.be/ijfvmKMLpQg?t=3597" target="_blank">HERE</a>
            
            
            
            <br />
            <strong style="color: black;">Discord Channel:</strong> <a href="https://discord.com/channels/785628120471699507/825746490651574302" target="_blank">Open in Browser</a>, <a href="discord://discord.com/channels/785628120471699507/825746490651574302">Open in App</a> (Participants only)
            
            
        </p>
    </div>
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    -->
    <!-- TAKE ME TO THE EVENT END-->
    
    

    
    
    
    
    
    
    
    
    
    

    <h4 id="C1052">An improved augmented-reality method of inserting virtual objects into the scene with transparent objects</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1052" class="wrap-collabsible"> <input id="collapsibleC1052" class="toggle" type="checkbox" /> <label for="collapsibleC1052" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>In augmented reality, the insertion of virtual objects into the real scene needs to meet the requirements of visual consistency. When there are transparent objects in the real scene, the difference in refractive index and roughness of transparent objects will influence the effect of the virtual and real fusion. To tackle this problem, we solve for the material parameters of objects and illumination simultaneously by nesting microfacet model and hemispherical area illumination model into inverse path tracing. Multiple experiments verify that the proposed approach performs better than the state-of-the-art method.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1229">Using Speech to Visualise Shared Gaze Cues in MR Remote Collaboration</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1229" class="wrap-collabsible"> <input id="collapsibleC1229" class="toggle" type="checkbox" /> <label for="collapsibleC1229" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>In this paper, we present a 360 panoramic Mixed Reality (MR) system that visualises shared gaze cues using contextual speech input to improve task coordination. We conducted two studies to evaluate the design of the MR gaze-speech interface exploring the combinations of visualisation style and context control level. Findings from the first study suggest that an explicit visual form that directly connects the collaborators' shared gaze to the contextual conversation is preferred. The second study indicates that the gaze-speech modality shortens the coordination time to attend to the shared interest, making the communication more natural and the collaboration more effective.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1417">A Comparison of Spatial Augmented Reality Predictive Cues and their Effects on Sleep Deprived Users</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1417" class="wrap-collabsible"> <input id="collapsibleC1417" class="toggle" type="checkbox" /> <label for="collapsibleC1417" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Spatial Augmented Reality (SAR) is a useful tool for procedural tasks as virtual instructions can be co-located with the physical task. Existing research has shown SAR predictive cues to further enhance performance. However, this research has been conducted while the user is not fatigued. This paper investigates predictive cues under a sub-optimal scenario by depriving users of sleep. Results from a 62-hour sleep deprivation experiment demonstrate that SAR predictive cues are beneficial to sleep deprived users. From the predictive cues outlined in this paper, the line cue maintained the best performance and was least impacted by early morning performance declines.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    

    <h4 id="C1432">Distortion-free Mid-air Image Inside Refractive Surface and on Reflective Surface</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1432" class="wrap-collabsible"> <input id="collapsibleC1432" class="toggle" type="checkbox" /> <label for="collapsibleC1432" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We propose an approach to display a distortion-free mid-air image inside a transparent refractive object and on a curved reflective surface. We compensate for the distortion by generating a light source image that cancels the distortions in the mid-air image caused by refraction and reflection. The light source image is generated via ray-tracing simulation by transmitting the desired view image to the mid-air imaging system, and by receiving the transmitted image at the light source position. Finally, we present the results of an evaluation of our method performed in an actual optical system.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="I1003">HaptoMapping: Visuo-Haptic Augmented Reality by Embedding User-Imperceptible Tactile Display Control Signals in a Projected Image</h4>
    <p><strong><small>Invited Journal</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
            
                <p><i>Yamato Miyatake, Takefumi Hiraki, Daisuke Iwai, Kosuke Sato</i></p>
            
            
                <p><small>URL: <a href="https://doi.org/10.1109/TVCG.2021.3136214" target="_blank">https://doi.org/10.1109/TVCG.2021.3136214</a></small></p>
            
            
                <div id="I1003" class="wrap-collabsible"> <input id="collapsibleI1003" class="toggle" type="checkbox" /> <label for="collapsibleI1003" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This paper proposes HaptoMapping, a projection-based visuo-haptic augmented reality (VHAR) system, that can render visual and haptic content independently and present consistent visuo-haptic sensations on physical surfaces. HaptoMapping controls wearable haptic displays by embedded control signals that are imperceptible to the user in projected images using a pixel-level visible light communication technique. The prototype system is comprised of a high-speed projector and three types of haptic devicesfinger worn, stylus, and arm mounted. The finger-worn and stylus devices present vibrotactile sensations to a users fingertips. The arm-mounted device presents stroking sensations on a users forearm using arrayed actuators with a synchronized hand projection mapping. We identified that the developed systems maximum latency of haptic from visual sensations was 93.4 ms. We conducted user studies on the latency perception of our VHAR system. The results revealed that the developed haptic devices can present haptic sensations without user-perceivable latencies, and the visual-haptic latency tolerance of our VHAR system was 100, 159, 500 ms for the finger-worn, stylus, and arm-mounted devices, respectively. Another user study with the arm-mounted device discovered that the visuo-haptic stroking system maintained both continuity and pleasantness when the spacing between each substrate was relatively sparse, such as 20 mm, and significantly improved both the continuity and pleasantness at 80 and 150 mm/s when compared to the haptic only stroking system. Lastly, we introduced four potential applications in daily scenes. Our system methodology allows for a wide range of VHAR application design without concern for latency and misalignment effects.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    
    
    

    <h2 id="1.6">Session: Machine Learning</h2>
    

    
    
    <p><strong>Tuesday, March 15, 18:00, NZDT UTC+13</strong></p>
    
    
        <!-- TAKE ME TO THE EVENT START -->
    <!--
    
    
    
    
    
    
    
    
    
    
    
    
    <div class="notice--info" style="background-color: $theme-yellow ! important; color: $theme-text ! important;">
        <strong style="padding-bottom: 5px;">Take me to the event:</strong>
        <p>
            <strong style="color: black;">Virbela Location:</strong> Auditorium B (<a href="/2021/attend/virbela-instructions/#map">MAP</a>)

            
            <br />
            
            <strong style="color: black;">Watch the recorded video stream:</strong> <a href="https://youtu.be/ekxFESRAt5c?t=3733" target="_blank">HERE</a>
            
            
            
            <br />
            <strong style="color: black;">Discord Channel:</strong> <a href="https://discord.com/channels/785628120471699507/825746537501032448" target="_blank">Open in Browser</a>, <a href="discord://discord.com/channels/785628120471699507/825746537501032448">Open in App</a> (Participants only)
            
            
        </p>
    </div>
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    -->
    <!-- TAKE ME TO THE EVENT END-->
    
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1273">Continuous Transformation Superposition for Visual Comfort Enhancement of Casual Stereoscopic Photography</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1273" class="wrap-collabsible"> <input id="collapsibleC1273" class="toggle" type="checkbox" /> <label for="collapsibleC1273" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>In this paper, we present a novel visual comfort enhancement method for casual stereoscopic photography via reinforcement learning based on continuous transformation superposition. To achieve the continuous transformation superposition, we prepare continuous transformation models for translation, rotation, and perspective transformations. Then we train a policy model to determine an optimal transformation chain to recurrently handle the geometric constraints and disparity adjustment. We further propose an attention-based stereo feature fusion module that enhances and integrates the binocular information. Experimental results on three datasets demonstrate that our method achieves superior performance to state-of-the-art methods.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1389">SPAA: Stealthy Projector-based Adversarial Attacks on Deep Image Classifiers</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1389" class="wrap-collabsible"> <input id="collapsibleC1389" class="toggle" type="checkbox" /> <label for="collapsibleC1389" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We propose to use spatial augmented reality (SAR) techniques to fool image classifiers by altering the physical light condition with a projector. The main challenge is how to generate robust and stealthy projections. For the first time, we formulate this problem as an end-to-end differentiable process and propose Stealthy Projector-based Adversarial Attack (SPAA). In SPAA, we approximate the real Project-and-Capture process using a neural network named PCNet, then we include PCNet in the optimization of projector-based attacks. Finally, we propose an algorithm that alternates between the adversarial loss and stealthiness loss optimization. Experiments show that SPAA clearly outperforms other methods.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1471">360 Depth Estimation in the Wild - the Depth360 Dataset and the SegFuse Network</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1471" class="wrap-collabsible"> <input id="collapsibleC1471" class="toggle" type="checkbox" /> <label for="collapsibleC1471" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Single-view depth estimation from omnidirectional images has gained popularity with various applications. Although data-driven methods demonstrate significant potential in this field, scarce training data and ineffective 360 estimation algorithms are still two key limitations hindering accurate estimation across diverse domains. In this work, we first establish a large-scale dataset with varied settings by exploring the use of a plenteous source of data, internet 360 videos, with a test-time training method. We then propose an end-to-end multi-task network, SegFuse, to effectively learn from the dataset and estimate depth maps from diverse images. Experimental results show favorable performance against the state-of-the-art methods.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="J1023">ScanGAN360: A Generative Model of Realistic Scanpaths for 360&#176; Images</h4>
    <p><strong><small>Journal</small></strong></p>

    

    

    

    
        
    
        
    
        
            
                <p><i>Daniel Martin, Ana Serrano, Alexander William Bergman, Gordon Wetzstein, Belen Masia</i></p>
            
            
                <p><small>URL: <a href="https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150502" target="_blank">https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150502</a></small></p>
            
            
                <div id="J1023" class="wrap-collabsible"> <input id="collapsibleJ1023" class="toggle" type="checkbox" /> <label for="collapsibleJ1023" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We present ScanGAN360, a new generative adversarial approach to address this problem. We propose a novel loss function based on dynamic time warping and tailor our network to the specifics of 360&#176; images. The quality of our generated scanpaths outperforms competing approaches by a large margin, and is almost on par with the human baseline. ScanGAN360 allows fast simulation of large numbers of virtual observers, whose behavior mimics real users, enabling a better understanding of gaze behavior, facilitating experimentation, and aiding novel applications in virtual reality and beyond.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="J1154">A Virtual Reality Based System for the Screening and Classification of Autism</h4>
    <p><strong><small>Journal</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Marta Robles, Negar Namdarian Nosratabadi, Julia Otto, Evelin Wassiljew, Nassir Navab, Christine Falter Wagner, Daniel Roth</i></p>
            
            
                <p><small>URL: <a href="https://doi.org/10.1109/TVCG.2022.3150489" target="_blank">https://doi.org/10.1109/TVCG.2022.3150489</a></small></p>
            
            
                <div id="J1154" class="wrap-collabsible"> <input id="collapsibleJ1154" class="toggle" type="checkbox" /> <label for="collapsibleJ1154" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Autism Spectrum Disorders (ASD) - also known solely as Autism or Autism Spectrum Conditions (ASC) - are a neurodevelopmental disorder (ICD-11 6A02) that is associated with characteristic deficits in social interaction and altered communicative behavior patterns. As a consequence, many autistic individuals may struggle in everyday life, which sometimes manifests in depression, unemployment, or addiction. One crucial problem in patient support and treatment is the long waiting time to diagnosis, which was approximated to 13 months on average. Yet, the earlier an intervention can take place the better the patient can be supported, which was identified as a crucial factor. We propose a system to support the screening of ASD based on a virtual reality (VR) social interaction, namely a shopping experience, with an embodied agent. During this everyday interaction, behavioral responses are tracked and recorded. We analyze this behavior with machine learning approaches to classify participants from an ASD group in comparison to a typically developed (TD) individuals control sample with high accuracy, demonstrating the feasibility of the approach. We believe that such tools can strongly impact the way mental disorders are assessed and may help to further find objective criteria and categorization.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    

    <h4 id="J1244">Online Projector Deblurring Using a Convolutional Neural Network</h4>
    <p><strong><small>Journal</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Yuta Kageyama, Daisuke Iwai, Kosuke Sato</i></p>
            
            
                <p><small>URL: <a href="https://doi.org/10.1109/TVCG.2022.3150465" target="_blank">https://doi.org/10.1109/TVCG.2022.3150465</a></small></p>
            
            
                <div id="J1244" class="wrap-collabsible"> <input id="collapsibleJ1244" class="toggle" type="checkbox" /> <label for="collapsibleJ1244" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Projector deblurring is an important technology for dynamic projection mapping (PM), where the distance between a projector and a projection surface changes in time. However, conventional deblurring techniques do not support dynamic PM because they need to project calibration patterns to estimate the amount of defocus blur each time the surface moves. We present a deep neural network that can compensate for defocus blur in dynamic PM without projecting calibration patterns. We also propose a pseudo-projection technique for synthesizing physically plausible training data. Both simulation and physical PM experiments showed that our technique alleviated the defocus blur in dynamic PM.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h2 id="2.4">Session: Presence</h2>
    

    
    
    <p><strong>Tuesday, March 15, 8:30, NZDT UTC+13</strong></p>
    
    
        <!-- TAKE ME TO THE EVENT START -->
    <!--
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    -->
    <!-- TAKE ME TO THE EVENT END-->
    
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1289">Exploring Presence, Avatar Embodiment, and Body Perception with a Holographic Augmented Reality Mirror</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1289" class="wrap-collabsible"> <input id="collapsibleC1289" class="toggle" type="checkbox" /> <label for="collapsibleC1289" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>The use of full-body illusions in virtual reality has been shown promise in enhancing the user's mental health. In our work, we developed a holographic augmented reality (AR) mirror to extend these advances by real-world interaction and evaluated its user experience. Twenty-seven participants provided predominantly positive qualitative feedback on the technical implementation and rated the system regarding presence, embodiment, and body weight perception. Our comparison with more immersive systems shows that AR systems can provide full-body illusions of similar quality. However, future work is essential to determine the significance of our findings in the context of mental health.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1503">Kicking in Virtual Reality: The Influence of Foot Visibility on the Shooting Experience and Accuracy</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1503" class="wrap-collabsible"> <input id="collapsibleC1503" class="toggle" type="checkbox" /> <label for="collapsibleC1503" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Foot interaction is crucial in many disciplines when playing sports in virtual reality. We investigated how the visibility of the foot influences penalty shooting in soccer. In a between-group experiment, we asked 28 players to hit eight targets with a virtual ball. We measured the performance, task load, presence, ball control, and body ownership of inexperienced to advanced soccer players. In one condition, the players saw a visual representation of their tracked foot which improved the accuracy of the shots significantly. Players with invisible foot needed 58% more attempts. Further, with foot visibility the self-reported body ownership was higher.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="J1129">Augmenting Immersive Telepresence Experience with a Virtual Body</h4>
    <p><strong><small>Journal</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Nikunj Arora, Markku Suomalainen, Matti Pouke, Evan G Center, Katherine J. Mimnaugh, Alexis P Chambers, Pauli Sakaria Pouke, Steven LaValle</i></p>
            
            
            
                <div id="J1129" class="wrap-collabsible"> <input id="collapsibleJ1129" class="toggle" type="checkbox" /> <label for="collapsibleJ1129" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We propose augmenting immersive telepresence by adding a virtual body, representing the user's own arm motions, as realized through a head-mounted display and a 360-degree camera. We conducted a study where participants were telepresent through a head-mounted display at a researcher's physical location, who interacted with them and prompted for reactions. The results showed contradiction between pilot and confirmatory studies, with at best weak evidence in increase of presence and preference of the virtual body. Further analysis suggests that the quality and style of the virtual arms led to individual differences, which subsequently moderated feelings of presence.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="J1331">Breaking Plausibility Without Breaking Presence - Evidence For The Multi-Layer Nature Of Plausibility</h4>
    <p><strong><small>Journal</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Larissa Br&uuml;bach, Franziska Westermeier, Carolin Wienrich, Marc Erich Latoschik</i></p>
            
            
                <p><small>URL: <a href="https://doi.org/10.1109/TVCG.2022.3150496" target="_blank">https://doi.org/10.1109/TVCG.2022.3150496</a></small></p>
            
            
                <div id="J1331" class="wrap-collabsible"> <input id="collapsibleJ1331" class="toggle" type="checkbox" /> <label for="collapsibleJ1331" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Recently, a novel theoretical model introduced coherence and plausibility as the essential conditions of XR experiences. Plausibility results from multi-layer (cognitive, perceptual, and sensational) coherence activation. We utilized breaks in plausibility (analogous to breaks in presence) by introducing incoherence on the perceptual and cognitive layer. A simulation of gravity-defying objects, i.e., the perceptual manipulation, broke plausibility, however, not presence. Simultaneously, the cognitive manipulation, presented as storyline framing, was too weak to counteract the strong bottom-up inconsistencies. Both results confirm the predictions of the novel model, incorporating well-known top-down and bottom-up rivalries and a theorized increased independence between plausibility and presence.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    

    
    
    
    
    
    

    
    
    

    <h2 id="2.5">Session: Perception in AR</h2>
    

    
    
    <p><strong>Tuesday, March 15, 11:00, NZDT UTC+13</strong></p>
    
    
        <!-- TAKE ME TO THE EVENT START -->
    <!--
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    -->
    <!-- TAKE ME TO THE EVENT END-->
    
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1146">The Influence of Environmental Lighting on Size Variations in Optical See-through Tangible Augmented Reality</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1146" class="wrap-collabsible"> <input id="collapsibleC1146" class="toggle" type="checkbox" /> <label for="collapsibleC1146" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>It is not possible to use a physical replica for each virtual object interacted with in Tangible Augmented Reality. Investigations to what extent a physical object can differ from its virtual counterpart are necessary. Since perception in optical see-through Augmented Reality is strongly influenced by the ambient lighting, we examined under three different indoor lighting conditions how much the physical object can differ in size from its virtual representation. As illuminance increases, usability and presence decrease, but the size ranges in which a physical object can deviate without having a strong negative impact on usability, presence and performance increase.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1568">Depth Perception in Augmented Reality: The Effects of Display, Shadow, and Position</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1568" class="wrap-collabsible"> <input id="collapsibleC1568" class="toggle" type="checkbox" /> <label for="collapsibleC1568" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Although it is commonly accepted that depth perception in AR displays is distorted, we have yet to isolate which properties of AR affect people's ability to correctly perceive virtual objects in real spaces. In this research, we evaluate absolute measures of distance perception in the Microsoft HoloLens 2, an optical see-through (OST) display, and the Varjo XR-3, a video see-through (VST) display. Our work is the first to evaluate either device using absolute distance measures. Our results suggest that current VST displays may induce more distance underestimation than their OST counterparts. We also find differences in depth judgments to grounded versus floating virtual objects--a difference that prevails even when cast shadows are present.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="I1011">Multisensory Proximity and Transition Cues for Improving Target Awareness in Narrow Field of View Augmented Reality Displays</h4>
    <p><strong><small>Invited Journal</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Christina Trepkowski, Alexander Marquardt, Tom David Eibich, Yusuke Shikanai, Jens Maiero, Kiyoshi Kiyokawa, Ernst Kruijff, Johannes Schoning, Peter Konig</i></p>
            
            
                <p><small>URL: <a href="https://doi.org/10.1109/TVCG.2021.3116673" target="_blank">https://doi.org/10.1109/TVCG.2021.3116673</a></small></p>
            
            
                <div id="I1011" class="wrap-collabsible"> <input id="collapsibleI1011" class="toggle" type="checkbox" /> <label for="collapsibleI1011" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Augmented reality applications allow users to enrich their real surroundings with additional digital content. However, due to the limited field of view of augmented reality devices, it can sometimes be difficult to become aware of newly emerging information inside or outside the field of view. Typical visual conflicts like clutter and occlusion of augmentations occur and can be further aggravated especially in the context of dense information spaces. In this article, we evaluate how multisensory cue combinations can improve the awareness for moving out-of-view objects in narrow field of view augmented reality displays. We distinguish between proximity and transition cues in either visual, auditory or tactile manner. Proximity cues are intended to enhance spatial awareness of approaching out-of-view objects while transition cues inform the user that the object just entered the field of view. In study 1, user preference was determined for 6 different cue combinations via forced-choice decisions. In study 2, the 3 most preferred modes were then evaluated with respect to performance and awareness measures in a divided attention reaction task. Both studies were conducted under varying noise levels. We show that on average the Visual-Tactile combination leads to 63% and Audio-Tactile to 65% faster reactions to incoming out-of-view augmentations than their Visual-Audio counterpart, indicating a high usefulness of tactile transition cues. We further show a detrimental effect of visual and audio noise on performance when feedback included visual proximity cues. Based on these results, we make recommendations to determine which cue combination is appropriate for which application.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    

    <h4 id="J1024">The Effect of Context Switching, Focal Switching Distance, Binocular and Monocular Viewing, and Transient Focal Blur on Human Performance in Optical See-Through Augmented Reality</h4>
    <p><strong><small>Journal</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
            
                <p><i>Mohammed Safayet Arefin, Nate Phillips, Alexander Plopski, Joseph L Gabbard, J. Edward Swan</i></p>
            
            
                <p><small>URL: <a href="https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150503" target="_blank">https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150503</a></small></p>
            
            
                <div id="J1024" class="wrap-collabsible"> <input id="collapsibleJ1024" class="toggle" type="checkbox" /> <label for="collapsibleJ1024" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>In optical see-through augmented reality (AR), information is often distributed between real and virtual contexts, and often appears at different distances from the user. To integrate information, users must repeatedly switch context and change focal distance. Previously, Gabbard, Mehra, and Swan (2018) examined these issues, using a text-based visual search task on a monocular, optical see-through AR display. Our study partially replicated and extended this task on a custom-built AR Haploscope for both monocular and binocular viewings. Results establishes that context switching, focal distance switching, and transient focal blur remain important AR user interface design issues.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="J1099">Stereopsis Only: Validation of a Monocular Depth Cues Reduced Gamified Virtual Reality with Reaction Time Measurement</h4>
    <p><strong><small>Journal</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Wolfgang Andreas Mehringer, Markus Wirth, Daniel Roth, Georg Michelson, Bjoern M Eskofier</i></p>
            
            
                <p><small>URL: <a href="https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150486" target="_blank">https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150486</a></small></p>
            
            
                <div id="J1099" class="wrap-collabsible"> <input id="collapsibleJ1099" class="toggle" type="checkbox" /> <label for="collapsibleJ1099" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Visuomotor task performance is limited in absence of binocular cues, e.g. when the visual system is affected by a disorder like amblyopia. Conventional amblyopia treatment occludes the healthy eye, however, resulting in poor stereopsis improvements. Therefore, binocular treatments equilibrate both eyes' visual input. Most approaches use divided stimuli which do not account for loss of stereopsis. We created a Virtual Reality with reduced monocular depth cues in which a stereoscopic task is shown to both eyes simultaneously. In a study with 18 participants, the number of correct responses reduced from 90% under binocular vision to 50% under monocular vision.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    
    
    

    <h2 id="2.6">Session: Locomotion (Asia-Pacific)</h2>
    

    
    
    <p><strong>Tuesday, March 15, 13:00, NZDT UTC+13</strong></p>
    
    
        <!-- TAKE ME TO THE EVENT START -->
    <!--
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    -->
    <!-- TAKE ME TO THE EVENT END-->
    
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1302">Effects of Virtual Room Size and Objects on Relative Translation Gain Thresholds in Redirected Walking</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1302" class="wrap-collabsible"> <input id="collapsibleC1302" class="toggle" type="checkbox" /> <label for="collapsibleC1302" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This paper investigates how the size of virtual space and objects affect the threshold range of relative translation gains, a Redirected Walking (RDW) technique that scales the user's movement in virtual space in different ratios for width and depth. We estimate the relative translation gain thresholds in six spatial conditions configured by three room sizes and the presence of virtual objects, which were set according to differing Angles of Declination (AoDs) between eye-gaze and the forward-gaze. Results show that both size and virtual objects significantly affect the threshold range, it being greater in the large-sized condition and furnished condition.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1336">RedirectedDoors: Redirection While Opening Doors in Virtual Reality</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1336" class="wrap-collabsible"> <input id="collapsibleC1336" class="toggle" type="checkbox" /> <label for="collapsibleC1336" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We propose RedirectedDoors, a novel technique for redirection in VR focused on door-opening behavior. This technique manipulates the user's walking direction by rotating the entire virtual environment at a certain angular ratio of the door being opened, while the virtual door's position is kept unmanipulated to ensure door-opening realism. Results of a user study using two types of door-opening interfaces (with and without a passive haptic prop) revealed that the estimated detection thresholds generally showed a higher space efficiency of redirection. Following the results, we derived usage guidelines for our technique that provide lower noticeability and higher acceptability.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1450">PseudoJumpOn: Jumping onto Steps in Virtual Reality</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1450" class="wrap-collabsible"> <input id="collapsibleC1450" class="toggle" type="checkbox" /> <label for="collapsibleC1450" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We propose PseudoJumpOn, a novel locomotion technique using a common VR setup that allows the user to experience virtual step-up jumping motion by applying viewpoint manipulation to the physical jump on a flat floor. The core idea is to replicate the physical characteristics of ascending jumps, and thus we designed two viewpoint manipulation methods: gain manipulation, which differentiates the ascent and descent height&#59; and peak shifting, which delays the peak timing. The experimental results showed that the combination of these methods allowed the participants to feel adequate reality and naturalness of actually jumping onto steps, even knowing no physical steps existed.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    

    <h4 id="C1463">Solitary Jogging with A Virtual Runner using Smartglasses</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1463" class="wrap-collabsible"> <input id="collapsibleC1463" class="toggle" type="checkbox" /> <label for="collapsibleC1463" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Group exercise is more effective for gaining motivation than exercising alone, but it can be difficult to always find such partners. In this paper, we explore the experiences that joggers have with a virtual partner instead of a human partner and report on the results of two controlled experiments evaluating our approach. In Study 1, we investigated how participants felt and how their behavior changed when they jogged indoors with a human partner or with a virtual partner compared to solitary jogging. The virtual partner was represented either as a full-body, limb-only, or a point-light avatar displayed on smartglasses. In Study 2, we investigated the differences between the three representations as virtual partners in an outdoor setting.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    

    <h4 id="C1470">Optimal Target Guided Redirected Walking with Pose Score Precomputation</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1470" class="wrap-collabsible"> <input id="collapsibleC1470" class="toggle" type="checkbox" /> <label for="collapsibleC1470" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Most of the previous redirected walking (RDW) methods do not consider future possibilities of collisions after imperceptibly redirecting users. This paper combines the subtle RDW methods and reset strategy in our method design and proposes a novel solution for RDW. We discretize the representation of possible user positions and orientations by a series of standard poses and rate them based on the possibilities of hitting obstacles of their reachable poses. A transfer path algorithm is proposed to measure the accessibility of the poses. Our method can redirect VR users imperceptibly to the best pose among all the reachable poses. Experiments demonstrate that it outperforms state-of-the-art methods in various environment settings.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="J1026">FrictShoes: Providing Multilevel Nonuniform Friction Feedback on Shoes in VR</h4>
    <p><strong><small>Journal</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
            
                <p><i>Chih-An Tsao, Tzu-Chun Wu, Hsin-Ruey Tsai, Tzu-Yun Wei, Fang-Ying Liao, Sean Chapman, Bing-Yu Chen</i></p>
            
            
                <p><small>URL: <a href="https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150492" target="_blank">https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150492</a></small></p>
            
            
                <div id="J1026" class="wrap-collabsible"> <input id="collapsibleJ1026" class="toggle" type="checkbox" /> <label for="collapsibleJ1026" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We propose a wearable device, FrictShoes a pair of foot accessories, to provide multilevel nonuniform friction feedback to feet. By independently controlling six brakes on six wheels underneath each FrictShoe, the friction levels of the wheels from each could be either matched or to vary. We conducted user studies to understand users' distinguishability of friction force magnitudes (or levels), realize how users adjust and map the multilevel nonuniform friction patterns to common VR terrains or ground textures, and evaluate the performance of the proposed feedback to the feet as if walking on different terrains or ground textures in VR.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    
    
    
    
    
    
    
    
    
    
    

    <h2 id="3.3">Session: Interaction Design</h2>
    

    
    
    <p><strong>Tuesday, March 15, 8:30, NZDT UTC+13</strong></p>
    
    
        <!-- TAKE ME TO THE EVENT START -->
    <!--
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <div class="notice--info" style="background-color: $theme-yellow ! important; color: $theme-text ! important;">
        <strong style="padding-bottom: 5px;">Take me to the event:</strong>
        <p>
            <strong style="color: black;">Virbela Location:</strong> Auditorium A (<a href="/2021/attend/virbela-instructions/#map">MAP</a>)

            
            <br />
            
            <strong style="color: black;">Watch the recorded video stream:</strong> <a href="https://youtu.be/ijfvmKMLpQg?t=16222" target="_blank">HERE</a>
            
            
            
            <br />
            <strong style="color: black;">Discord Channel:</strong> <a href="https://discord.com/channels/785628120471699507/825746592992591902" target="_blank">Open in Browser</a>, <a href="discord://discord.com/channels/785628120471699507/825746592992591902">Open in App</a> (Participants only)
            
            
        </p>
    </div>
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    -->
    <!-- TAKE ME TO THE EVENT END-->
    
    

    
    
    
    
    
    

    <h4 id="C1042">Exploring and Selecting Supershapes in Virtual Reality with Line, Quad, and Cube Shaped Widgets</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1042" class="wrap-collabsible"> <input id="collapsibleC1042" class="toggle" type="checkbox" /> <label for="collapsibleC1042" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>In this work, we propose VR shape widgets that allow users to probe and select supershapes from a multitude of solutions. Our designs take leverage on thumbnails, mini-maps, haptic feedback and spatial interaction, while supporting 1-D, 2-D and 3-D supershape parameter spaces. We conducted a user study (N = 18) and found that VR shape widgets are effective, more efficient, and natural than conventional VR 1-D sliders while also usable for users without prior knowledge on supershapes. We also found that the proposed VR widgets provide a quick overview of the main supershapes, and users can easily reach the desired solution without having to perform fine-grain handle manipulations.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1303">The Potential of Augmented Reality for Digital Twins: A Literature Review</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1303" class="wrap-collabsible"> <input id="collapsibleC1303" class="toggle" type="checkbox" /> <label for="collapsibleC1303" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Implementing digital twins with the help of mixed/augmented reality is a promising approach, yet still a novel area of research. So we conducted a PRISMA-based literature review of scientific articles and book chapters dealing with the use of MR and AR for digital twins. 25 papers were analyzed, sorted and compared by different categories like research topic, domain, paper type, evaluation type, used hardware, as well as the different outcomes. The major finding of this research survey is the predominant focus of the reviewed papers on the technology itself and the neglect of factors regarding the users.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="I1008">The Effect of Exploration Mode and Frame of Reference in Immersive Analytics</h4>
    <p><strong><small>Invited Journal</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Jorge Wagner, Wolfgang Stuerzlinger, Luciana Nedel</i></p>
            
            
                <p><small>URL: <a href="https://doi.org/10.1109/TVCG.2021.3060666" target="_blank">https://doi.org/10.1109/TVCG.2021.3060666</a></small></p>
            
            
                <div id="I1008" class="wrap-collabsible"> <input id="collapsibleI1008" class="toggle" type="checkbox" /> <label for="collapsibleI1008" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>The design space for user interfaces for Immersive Analytics applications is vast. Designers can combine navigation and manipulation to enable data exploration with ego- or exocentric views, have the user operate at different scales, or use different forms of navigation with varying levels of physical movement. This freedom results in a multitude of different viable approaches. Yet, there is no clear understanding of the advantages and disadvantages of each choice. Our goal is to investigate the affordances of several major design choices, to enable both application designers and users to make better decisions. In this work, we assess two main factors, exploration mode and frame of reference, consequently also varying visualization scale and physical movement demand. To isolate each factor, we implemented nine different conditions in a Space-Time Cube visualization use case and asked 36 participants to perform multiple tasks. We analyzed the results in terms of performance and qualitative measures and correlated them with participants' spatial abilities. While egocentric room-scale exploration significantly reduced mental workload, exocentric exploration improved performance in some tasks. Combining navigation and manipulation made tasks easier by reducing workload, temporal demand, and physical effort.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="J1012">Synthesizing Personalized Construction Safety Training Scenarios for VR Training</h4>
    <p><strong><small>Journal</small></strong></p>

    

    

    

    
        
    
        
            
                <p><i>Wanwan Li, Haikun Huang, Tomay Solomon, Behzad Esmaeili, Lap-Fai Yu</i></p>
            
            
                <p><small>URL: <a href="https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150510" target="_blank">https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150510</a></small></p>
            
            
                <div id="J1012" class="wrap-collabsible"> <input id="collapsibleJ1012" class="toggle" type="checkbox" /> <label for="collapsibleJ1012" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Construction industry has the largest number of preventable fatal injuries, providing effective safety training practices can play a significant role in reducing the number of fatalities. Building on recent advancements in virtual reality-based training, we devised a novel approach to synthesize construction safety training scenarios to train users on how to proficiently inspect the potential hazards on construction sites in virtual reality. Given the training specifications such as individual training preferences and target training time, we synthesize personalized VR training scenarios through an optimization approach. We validated our approach by conducting user studies where users went through our personalized VR training, general VR training, or conventional slides training. The results show that our personalized VR training approach can more effectively train users to improve their construction hazard inspection skills.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="J1082">PoVRPoint: Authoring Presentations in Mobile Virtual Reality</h4>
    <p><strong><small>Journal</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Verena Biener, Travis Gesslein, Daniel Schneider, Felix Kawala, Alexander Otte, Per Ola Kristensson, Michel Pahud, Eyal Ofek, Cuauhtli Campos, Matjazz Kljun, Klen &#x10C;opi&#x10D; Pucihar, Jens Grubert</i></p>
            
            
            
                <div id="J1082" class="wrap-collabsible"> <input id="collapsibleJ1082" class="toggle" type="checkbox" /> <label for="collapsibleJ1082" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We propose PoVRPoint - a set of tools coupling pen- and touch-based editing of presentations on mobile touch devices with the interaction capabilities afforded by VR. We study the utility of extended display space to assist users in identifying target slides, supporting spatial manipulation of slide-content, creating animations, and facilitating arrangements of multiple, possibly occluded objects. Among other things, our results indicate that the three-dimensional view in VR enables significantly faster object reordering in the presence of occlusion compared to two baseline interfaces. A user study further confirmed that our interaction techniques were found to be usable and enjoyable.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    
    
    

    <h2 id="3.4">Session: Virtual Humans and Agents</h2>
    

    
    
    <p><strong>Tuesday, March 15, 11:00, NZDT UTC+13</strong></p>
    
    
        <!-- TAKE ME TO THE EVENT START -->
    <!--
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    -->
    <!-- TAKE ME TO THE EVENT END-->
    
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1262">The Stare-in-the-Crowd Effect in Virtual Reality</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1262" class="wrap-collabsible"> <input id="collapsibleC1262" class="toggle" type="checkbox" /> <label for="collapsibleC1262" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>In this study we explore the stare-in-the crowd effect, an asymmetry in the gaze behaviour of a human observing directed and averted gaze present in a crowd. In virtual reality we have analysed gazes of a group of 30 users, asked to observe a crowd of 11 virtual agents performing 4 different gaze patterns. Results show that the stare-in-the-crowd effect is preserved in VR. We have additionally explored how user gaze behaviour can be affected by social anxiety. Such results are encouraging for the development of expressive and reactive virtual humans, which can be animated to express natural interactive behaviour.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    

    <h4 id="C1268">Virtual Humans with Pets and Robots: Exploring the Influence of Social Priming on One's Perception of a Virtual Human</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1268" class="wrap-collabsible"> <input id="collapsibleC1268" class="toggle" type="checkbox" /> <label for="collapsibleC1268" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Social priming is the idea that observations of a virtual human (VH) engaged in short social interactions with a real or virtual human bystander can positively influence users' subsequent interactions with that VH. In this paper, we investigate the question of whether the positive effects of social priming are limited to interactions with humanoid entities through a human-subjects experiment. Our mixed-methods analysis revealed positive influences of social priming with humanoid and non-humanoid entities, such as increasing participants' perception of the VH's affective attraction and enhancing participants' quality of experience.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    

    <h4 id="C1278">Investigating the Effects of Leader and Follower Behaviors of Virtual Humans in Collaborative Fine Motor Tasks in Virtual Reality</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1278" class="wrap-collabsible"> <input id="collapsibleC1278" class="toggle" type="checkbox" /> <label for="collapsibleC1278" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We examined the effects on users during collaboration with two types of virtual human (VH) agents in object transportation. The two types of VHs we examined are leader and follower agents. The goal of the users is to interact with the agents using natural language and carry objects from initial locations to destinations. The follower agent follows a user's instructions to perform actions to manipulate the object. The leader agent determines the appropriate actions that the agent and the user should perform. We conducted a within-subjects study to evaluate the user behaviors when interacting with the two VHs.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    

    <h4 id="C1279">An Evaluation of Native versus Foreign Communicative Interactions on Users' Behavioral Reactions towards Affective Virtual Crowds</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1279" class="wrap-collabsible"> <input id="collapsibleC1279" class="toggle" type="checkbox" /> <label for="collapsibleC1279" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This investigation compared the impact on the users' non-verbal behaviors elicited by interacting with a crowd of emotional virtual humans (VHs) in native and non-native language settings. In a between-subject experiment, we collected objective metrics regarding the users' behaviors during interaction with a crowd. The language conditions were collected in the USA and under two different conditions in Taiwan. The participants in the USA group interacted with the VHs in English (a native language for the USA setting)&#59; and two different groups in Taiwan interacted with the VHs in either a foreign (English) or native (Mandarin) language, respectively.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1329">The Effect of Virtual Humans Making Verbal Communication Mistakes on Learner's Perspective of their Credibility, Reliability, and Trustworthiness</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1329" class="wrap-collabsible"> <input id="collapsibleC1329" class="toggle" type="checkbox" /> <label for="collapsibleC1329" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>In this work, we performed a 2x2 mixed design user study that had learners (n = 80) attempt to identify verbal communication mistakes made by a virtual human acting as a nurse in a virtual desktop environment. We found that learners struggle to identify infrequent virtual human verbal communication mistakes. Additionally, learners with lower initial trustworthiness ratings are more likely to overlook potentially life-threatening mistakes, and virtual human mistakes temporarily lower learner credibility, reliability, and trustworthiness ratings of virtual humans.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="J1310">The One-Man-Crowd: Single User Generation of Crowd Motions Using Virtual Reality</h4>
    <p><strong><small>Journal</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Tairan Yin, Ludovic Hoyet, Marc Christie, Marie-Paule R. Cani, Julien Pettr&eacute;</i></p>
            
            
                <p><small>URL: <a href="https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150507" target="_blank">https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150507</a></small></p>
            
            
                <div id="J1310" class="wrap-collabsible"> <input id="collapsibleJ1310" class="toggle" type="checkbox" /> <label for="collapsibleJ1310" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Crowd motion data is fundamental for understanding and simulating crowd behaviours. Such data is usually collected through controlled experiments and is scarce due to difficulties involved in its gathering. In this work, we propose a novel Virtual Reality based approach for the acquisition of crowd motion data, which immerses a single user in virtual scenarios to act each crowd member. We validate our approach by replicating three real experiments, and compare the results. Using our approach, realistic collective behaviours can naturally emerge, even though with lower behavioural variety. These results provide valuable insights to virtual crowd experiences, and reveal key directions for further improvements.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    

    
    
    

    <h2 id="3.5">Session: Perception</h2>
    

    
    
    <p><strong>Tuesday, March 15, 13:00, NZDT UTC+13</strong></p>
    
    
        <!-- TAKE ME TO THE EVENT START -->
    <!--
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    -->
    <!-- TAKE ME TO THE EVENT END-->
    
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1159">Effects of Field of View on Dynamic Out-of-View Target Search in Virtual Reality</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1159" class="wrap-collabsible"> <input id="collapsibleC1159" class="toggle" type="checkbox" /> <label for="collapsibleC1159" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We present a study of the effects of field of view (FOV), target movement, and number of targets on visual search performance in virtual reality. We compared visual search tasks in two FOVs (&#126;65&#176;, &#126;32.5&#176;) under two target movement speeds (static, dynamic) while varying the visible target count, with targets potentially out of the user's view. We examined the expected linear relationship between search time and number of items, to explore how moving and/or out-of-view targets affected this relationship. Overall, search performance increased with a wide FOV, but decreased when targets were moving and with more visible targets. Neither FOV nor target movement meaningfully altered the linear relationship between search time and number of items.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1223">The Smell Engine: A system for artificial odor synthesis in virtual environments</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1223" class="wrap-collabsible"> <input id="collapsibleC1223" class="toggle" type="checkbox" /> <label for="collapsibleC1223" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We devise a Smell Engine that presents users with a real-time odor synthesis of smells from a virtual environment. Using the Smell Composer framework, developers can configure odor sources in virtual space, then the Smell Mixer component dynamically estimates the odor mix that the user would smell, and a Smell Controller coordinates an olfactometer to physically present an approximation of the odor mix to the user's mask. Through a three-part user study, we found that the Smell Engine can help measure a subject's olfactory detection threshold and improve their ability to precisely localize odors in the virtual environment.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1525">All Shook Up: The Impact of Floor Vibration in Symmetric and Asymmetric Immersive Multi-user VR Gaming Experiences</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1525" class="wrap-collabsible"> <input id="collapsibleC1525" class="toggle" type="checkbox" /> <label for="collapsibleC1525" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This paper investigates the influence of floor-vibration tactile feedback on immersed users under symmetric and asymmetric tactile sensory cue conditions. With our custom-built, computer-controlled vibration floor, we implemented a cannonball shooting game for two physically-separated players. In the VR game, the two players shoot cannonballs to destroy their opponent's protective wall and cannon, while the programmed floor platform generates vertical vibrations depending on the experimental condition. We concluded that vibration provided to a pair of game players in immersive VR can significantly enhance the VR experience, but sensory symmetry does not guarantee improved gaming performance.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="I1010">Shedding Light on Cast Shadows: An Investigation of Perceived Ground Contact in AR and VR</h4>
    <p><strong><small>Invited Journal</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Haley Adams, Jeanine Stefanucci, Sarah H Creem-Regehr, Grant Pointon, William B Thompson, Bobby Bodenheimer</i></p>
            
            
                <p><small>URL: <a href="https://doi.org/10.1109/TVCG.2021.3097978" target="_blank">https://doi.org/10.1109/TVCG.2021.3097978</a></small></p>
            
            
                <div id="I1010" class="wrap-collabsible"> <input id="collapsibleI1010" class="toggle" type="checkbox" /> <label for="collapsibleI1010" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Virtual objects in augmented reality (AR) often appear to float atop real world surfaces, which makes it difficult to determine where they are positioned in space. This is problematic as many applications for AR require accurate spatial perception. In the current study, we examine how the way we render cast shadows--which act as an important monocular depth cue for creating a sense of contact between an object and the surface beneath it--impacts spatial perception. Over two experiments, we evaluate people's sense of surface contact given both traditional and non-traditional shadow shading methods in optical see-through augmented reality (OST AR), video see-through augmented reality (VST AR), and virtual reality (VR) head-mounted displays. Our results provide evidence that nontraditional shading techniques for rendering shadows in AR displays may enhance the accuracy of one's perception of surface contact. This finding implies a possible tradeoff between photorealism and accuracy of depth perception, especially in OST AR displays. However, it also supports the use of more stylized graphics like non-traditional cast shadows to improve perception and interaction in AR applications.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="J1167">Effects of Transparency on Perceived Humanness: Implications for Rendering Skin Tones Using Optical See-Through Displays</h4>
    <p><strong><small>Journal</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Tabitha C. Peck, Jessica J Good, Austin Erickson, Isaac M Bynum, Gerd Bruder</i></p>
            
            
                <p><small>URL: <a href="https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150521" target="_blank">https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150521</a></small></p>
            
            
                <div id="J1167" class="wrap-collabsible"> <input id="collapsibleJ1167" class="toggle" type="checkbox" /> <label for="collapsibleJ1167" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Current optical see-through displays in the field of augmented reality are limited in their ability to display colors with low lightness in the hue, saturation, lightness (HSL) color space, causing such colors to appear transparent. This hardware limitation may add unintended bias into scenarios with virtual humans. We present an exploratory user study investigating whether differing opacity levels result in dehumanizing avatar and human faces. Results support that dehumanization occurs as opacity decreases. This suggests that in similar lighting, low lightness skin tones (e.g., Black faces) will be viewed as less human than high lightness skin tones (e.g., White faces). </p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    
    
    

    <h2 id="3.6">Session: Medical and Health Care</h2>
    

    
    
    <p><strong>Tuesday, March 15, 18:00, NZDT UTC+13</strong></p>
    
    
        <!-- TAKE ME TO THE EVENT START -->
    <!--
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    -->
    <!-- TAKE ME TO THE EVENT END-->
    
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1199">Inducing Emotional Stress from the ICU Context in VR</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1199" class="wrap-collabsible"> <input id="collapsibleC1199" class="toggle" type="checkbox" /> <label for="collapsibleC1199" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Nurses in ICUs can suffer from emotional stress, which may severely effect their health and impact the quality of care. Stress Inoculation Training is a promising method to learn coping strategies. It involves stressors in a controlled environment at increasing intensity. In this work, using VR, we implement an emotional (moral) stressor in three intensity levels, based on literature reviews and expert interviews. In a user study, subjects deal with virtual characters while experiencing the stressor of having to comply with the patient's family wishes against ones own beliefs. Resulting stress was measured in objective and subjective ways. We show that emotional stress can be induced using VR and artificial characters.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1242">2D versus 3D: A Comparison of Needle Navigation Concepts between Augmented Reality Display Devices</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1242" class="wrap-collabsible"> <input id="collapsibleC1242" class="toggle" type="checkbox" /> <label for="collapsibleC1242" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Only an insufficient amount of fundamental research has focused on the design and hardware selection of such augmented reality needle navigation systems for surgical procedures. We contribute to this area by comparing three state-of-the-art concepts displayed by an optical see-through HMD and stereoscopic projectors. A 2D glyph resulted in higher accuracy but required more insertion time. Punctures guided by 3D see-through vision were less accurate but faster. A static representation of the correctly positioned needle, showed too high target errors. Future work should focus on improving the accuracy of the see-through vision concept. Until then, the glyph visualization is recommended.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1374">Supporting Playful Rehabilitation in the Home using Virtual Reality Headsets and Force Feedback Gloves</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1374" class="wrap-collabsible"> <input id="collapsibleC1374" class="toggle" type="checkbox" /> <label for="collapsibleC1374" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Virtual Reality (VR) is a promising platform for home rehabilitation with the potential to completely immerse users within a playful experience. To explore this area we design, implement, and evaluate a system that uses a VR headset in conjunction with force feedback gloves to present users with a playful experience for home rehabilitation. The system immerses the user within a virtual cat bathing simulation that allows users to practice fine motor skills by progressively completing three cat-care tasks. The study results demonstrate the positive role that playfulness may play in the user experience of VR rehabilitation.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    

    <h4 id="C1378">Evaluating Perceptional Tasks for Medicine: A Comparative User Study Between a Virtual Reality and a Desktop Application</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1378" class="wrap-collabsible"> <input id="collapsibleC1378" class="toggle" type="checkbox" /> <label for="collapsibleC1378" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>One way to improve the performance of precision-based medical VR applications is to provide suitable visualizations. Today, these &quot;suitable&quot; visualizations are mostly transferred from desktop to VR without considering their spatial and temporal performance might change in VR. This may not lead to an optimal solution, which can be crucial for precision-based tasks. Misinterpretation of shape or distance in a surgical or pre-operative simulation can affect the chosen treatment and thus directly impact the outcome. Therefore, we evaluate the performance differences of multiple visualizations for 3D surfaces based on their shape and distance estimation for desktop and VR applications.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    
    
    
    
    
    
</div>

<div>
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h2 id="1.7">Session: Audio in VR</h2>
    

    
    
    <p><strong>Wednesday, March 16, 8:30, NZDT UTC+13</strong></p>
    
    
        <!-- TAKE ME TO THE EVENT START -->
    <!--
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    -->
    <!-- TAKE ME TO THE EVENT END-->
    
    

    
    
    
    

    <h4 id="C1039">Investigating how speech and motion realism influence the perceived personality of virtual characters and agents.</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1039" class="wrap-collabsible"> <input id="collapsibleC1039" class="toggle" type="checkbox" /> <label for="collapsibleC1039" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>The portrayed personality of virtual characters and agents influences how we perceive and engage with digital applications. Understanding the influence of speech and animation allows us to design more personalized and engaging characters. Using performance capture data from multiple datasets, we contrast performance-driven characters to those portrayed by generated gestures and synthesized speech, analysing how the features of each influence personality according to the Big Five personality traits. Our results highlight motion as dominant for portraying extraversion and speech for communicating agreeableness and emotional stability, supporting the development of virtual characters, social agents and 3DUI agents with targeted personalities.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1510">Spatial Updating in Virtual Reality - Auditory and Visual Cues in a Cave Automatic Virtual Environment</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1510" class="wrap-collabsible"> <input id="collapsibleC1510" class="toggle" type="checkbox" /> <label for="collapsibleC1510" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>While moving through a real environment, egocentric location representations are effortlessly and automatically updated. But in synthetic environments, spatial updating is often disrupted or incomplete due to a lack of body-based movement information. To prevent disorientation, the support of spatial updating via other sensory movement cues is necessary. In the presented experiment, participants performed a spatial updating task inside a CAVE, with either no orientation cues, three visible distant landmarks, or one continuous auditory cue present. The data showed improved task performance when an orientation cue was present, with auditory cues providing at least as much improvement as visual cues.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1553">Audience Experiences of a Volumetric Virtual Reality Music Video</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1553" class="wrap-collabsible"> <input id="collapsibleC1553" class="toggle" type="checkbox" /> <label for="collapsibleC1553" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Modern music videos apply various media capture techniques and creative postproduction technologies to provide a myriad of stimulating and artistic approaches to audience entertainment and engagement for viewing across multiple devices. Within this domain, volumetric technologies are becoming a popular means of recording and reproducing musical performances for new audiences to access via traditional 2D screens and emergent virtual reality platforms. However, the precise impact of volumetric video in virtual reality music video entertainment has yet to be fully explored from a user's perspective. Here we show how users responded to volumetric representations of music performance in virtual reality.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="J1089">Comparing Direct and Indirect Methods of Audio Quality Evaluation in Virtual Reality Scenes of Varying Complexity</h4>
    <p><strong><small>Journal</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Thomas Robotham, Olli S. Rummukainen, Miriam Kurz, Marie Eckert, Emanu&euml;l A. P. Habets</i></p>
            
            
                <p><small>URL: <a href="https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150491" target="_blank">https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150491</a></small></p>
            
            
                <div id="J1089" class="wrap-collabsible"> <input id="collapsibleJ1089" class="toggle" type="checkbox" /> <label for="collapsibleJ1089" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This study uses four subjective audio quality evaluation methods (viz. multiple-stimulus with and without reference for direct scaling, and rank-order elimination and pairwise comparison for indirect scaling) to investigate the contributing factors present in multi-modal 6-DoF VR on quality ratings of real-time audio rendering. Five scenes were designed for evaluation with various amounts of user interactivity and complexity. Our results show rank-order elimination proved to be the fastest method, required the least amount of repetitive motion, and yielded the highest discrimination between spatial conditions. Ratings across scenes indicate complex scenes and interactive aspects of 6-DoF VR can impede quality judgments.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    
    
    

    <h2 id="1.8">Session: Rendering</h2>
    

    
    
    <p><strong>Wednesday, March 16, 13:00, NZDT UTC+13</strong></p>
    
    
        <!-- TAKE ME TO THE EVENT START -->
    <!--
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    -->
    <!-- TAKE ME TO THE EVENT END-->
    
    

    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1059">SivsFormer: Parallax-Aware Transformers for Single-image-based View Synthesis</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1059" class="wrap-collabsible"> <input id="collapsibleC1059" class="toggle" type="checkbox" /> <label for="collapsibleC1059" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Single-image-based view synthesis is challenging as it requires inferring contents beyond what is immediately visible. The generated views suffer from visually unpleasant holes, deformations, and artifacts by previous methods. We propose SivsFormer for high-quality and realistic view synthesis. In particular, a warping and occlusion handing module is designed to reduce the influence of parallax. Subsequently, a disparity alignment module captures the long-range information over the scene and ensures that pixels move correctly. Moreover, we present a parallax-aware loss function to improve the quality of the synthetic images, which explicitly quantifies the magnitude of parallax. Our approach achieves superior performance.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1300">Reconstructing 3D Virtual Face with Eye Gaze from a Single Image</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1300" class="wrap-collabsible"> <input id="collapsibleC1300" class="toggle" type="checkbox" /> <label for="collapsibleC1300" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Reconstructing 3D virtual face from a single image has a wide range of applications in virtual reality. In this paper, we propose to reconstruct 3D virtual face with eye gaze information from a single image. In detail, we design the 3D face reconstruction with precise eye region to retain correct eye gaze information and we propose eye contact guided facial-rotation to perform both eye contact and gaze estimation simultaneously. Extensive experiments on different tasks demonstrate the significant gain of the proposed approach.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1361">Interactive Mixed Reality Rendering on Holographic Pyramid</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1361" class="wrap-collabsible"> <input id="collapsibleC1361" class="toggle" type="checkbox" /> <label for="collapsibleC1361" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Currently, ray tracing and image-based lighting (IBL) have shortcomings when rendering the metallic virtual object displayed in the holographic pyramid in mixed reality. We propose a mixed reality rendering method to render glossy and specular reflection effects on metallic virtual objects displayed in the holographic pyramid based on the surrounding real environment at interactive frame rates. Compared with IBL and screen-space ray tracing, our method can generate the rendering results closer to the ground truth at the same time cost. Compared with Monte Carlo path tracing, our method is 2.5-4.5 times faster in generating rendering results of the comparable quality.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1543">Rectangular Mapping-based Foveated Rendering</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1543" class="wrap-collabsible"> <input id="collapsibleC1543" class="toggle" type="checkbox" /> <label for="collapsibleC1543" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>With the speedy increase of display resolution and the demand for interactive frame rate, rendering acceleration is becoming more critical for a wide range of virtual reality applications. Foveated rendering addresses this challenge by rendering with a non-uniform resolution for the display. Motivated by the non-linear optical lens equation, we present rectangular mapping-based foveated rendering (RMFR), a simple yet effective implementation of foveated rendering framework. RMFR supports varying level of foveation according to the eccentricity and the scene complexity. Compared with traditional foveated rendering methods, RMFR provides a superior level of perceived visual quality while consuming minimal rendering cost.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="I1012">Dynamic Projection Mapping for Robust Sphere Posture Tracking Using Uniform/Biased Circumferential Markers</h4>
    <p><strong><small>Invited Journal</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Yuri Mikawa, Tomohiro Sueishi, Yoshihiro Watanabe, Masatoshi Ishikawa</i></p>
            
            
                <p><small>URL: <a href="https://doi.ieeecomputersociety.org/10.1109/TVCG.2021.3111085" target="_blank">https://doi.ieeecomputersociety.org/10.1109/TVCG.2021.3111085</a></small></p>
            
            
                <div id="I1012" class="wrap-collabsible"> <input id="collapsibleI1012" class="toggle" type="checkbox" /> <label for="collapsibleI1012" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>In spatial augmented reality, a widely dynamic projection mapping system has been developed as a novel approach to graphics presentation for widely moving objects in dynamic situations. However, this method necessitates a novel tracking marker design that is resistant to random and complex occlusion and out-of-focus blurring, which conventional markers have not achieved. This study presents a uniform circumferential marker that becomes an ellipse in perspective projection and expresses geometric information. It can track the relative posture of a dynamically moving sphere with high speed, high accuracy, and robustness owing to continuous contour lines, thereby supporting both wide-range movement in the depth direction and human interaction. Moreover, a biased circumferential marker is proposed to embed unique coding, where the absolute posture is decoded with a novel recognition algorithm. Moreover, rough initialization using the geometry of multiple ellipses is proposed for both markers to start the automatic and precise tracking. Real-time rotation visualization onto the surface of a moving sphere is made possible with the high-speed, widely dynamic projection mapping system. The tracking performance is demonstrated to exhibit sufficient basic tracking performance as well as robustness against blurring and occlusion compared to conventional dot-based markers.</p>
                        </div>
                    </div>
                </div>
            
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="J1111">Dynamic Multi-Projection Mapping Based on Parallel Intensity Control</h4>
    <p><strong><small>Journal</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Takashi Nomoto, Wanlong Li, Hao-Lun Peng, Yoshihiro Watanabe</i></p>
            
            
                <p><small>URL: <a href="https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150488" target="_blank">https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150488</a></small></p>
            
            
                <div id="J1111" class="wrap-collabsible"> <input id="collapsibleJ1111" class="toggle" type="checkbox" /> <label for="collapsibleJ1111" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Projection mapping using multiple projectors is promising for spatial augmented reality&#59; however, it is difficult to apply it to dynamic scenes. This is because it is hard for the conventional method to reduce the latency from motion to projection. To mitigate this, we propose a novel method of controlling the intensity based on a pixel-parallel calculation for each projector with low latency. Additionally, our pixel-parallel calculation method allows a distributed system configuration, such that the number of projectors can be increased to form a network. We demonstrate a seamless mapping into dynamic scenes at 360 fps with a 9.5-ms latency using ten cameras and four projectors.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    
    
    

    <h2 id="1.9">Session: Advanced UI</h2>
    

    
    
    <p><strong>Wednesday, March 16, 14:00, NZDT UTC+13</strong></p>
    
    
        <!-- TAKE ME TO THE EVENT START -->
    <!--
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    -->
    <!-- TAKE ME TO THE EVENT END-->
    
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1089">Galea: A physiological sensing system for behavioral research in virtual environments</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1089" class="wrap-collabsible"> <input id="collapsibleC1089" class="toggle" type="checkbox" /> <label for="collapsibleC1089" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>The pairing of Virtual Reality technology with Physiological Sensing has gained much interest in clinical settings and beyond. We present Galea, a device which measures physiological responses when experiencing virtual content, enabling behavioral, affective computing, and human-computer interaction research access to data from the Parasympathetic nervous system and Sympathetic nervous system simultaneously. We provide design considerations and circuit characterization results of in-vivo recordings, and present two examples to help contextualize how these signals can be used in VR settings. We also discuss the importance and contributions of this work and future challenges that should be considered.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1322">Validating the Benefits of Glanceable and Context-Aware Augmented Reality for Everyday Information Access Tasks</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1322" class="wrap-collabsible"> <input id="collapsibleC1322" class="toggle" type="checkbox" /> <label for="collapsibleC1322" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Glanceable Augmented Reality interfaces have the potential to provide fast and efficient information access for the user. However, the virtual content's placement and accessibility depends on the user context. We designed a Context-Aware AR interface that can intelligently adapt for two different contexts: solo and social. We evaluated information access using Context-Aware AR compared to current mobile phones and non-adaptive Glanceable AR interfaces. Our results indicate the advantages of Context-Aware AR interface for information access efficiency, avoiding negative effects on primary tasks or social interactions, and overall user experience.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1584">User Preference for Navigation Instructions in Mixed Reality</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1584" class="wrap-collabsible"> <input id="collapsibleC1584" class="toggle" type="checkbox" /> <label for="collapsibleC1584" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Mixed Reality (MR) holds the promise of integrating navigation instructions directly in users' visual field, making them less obtrusive and more expressive. Current solutions, however, focus on using conventional designs such as arrows, and do not fully leverage the technological possibilities of MR. We contribute a remote survey and an in-person Virtual Reality study, showing that while familiar designs such as arrows are well received, novel navigation aids such as avatars or desaturation of non-target areas are viable alternatives. We distill the results into a set of guidelines for MR content creators and future context-aware MR navigation systems.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    

    
    
    

    <h4 id="C1594">Tangiball: Foot-Enabled Embodied Tangible Interaction with a Ball in Virtual Reality</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1594" class="wrap-collabsible"> <input id="collapsibleC1594" class="toggle" type="checkbox" /> <label for="collapsibleC1594" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Interaction with tangible user interfaces in virtual reality (VR) is known to offer several benefits. In this study, we explored foot-enabled embodied interaction in VR through a room-scale tangible soccer game (Tangiball). Users interacted with a physical ball with their feet in real time by seeing its virtual counterpart inside a VR head mounted display. A user study was performed with 40 participants, in which Tangiball was compared with the control condition of foot-enabled embodied interaction with a purely virtual ball. The results revealed that tangible interaction improved user performance and presence significantly, while no difference in terms of motion sickness was detected between the tangible and virtual versions.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    

    <h4 id="C1611">Redirecting Desktop Interface Input to Animate Cross Reality Avatars</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1611" class="wrap-collabsible"> <input id="collapsibleC1611" class="toggle" type="checkbox" /> <label for="collapsibleC1611" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We present and evaluate methods to redirect desktop inputs such as eye gaze and mouse pointing to a VR-embedded avatar. We use these methods to build a novel interface that allows a desktop user to give presentations in remote meetings such as VR-based conferences or classrooms with a more engaging &quot;cross-reality&quot; avatar capable of gestures similar to those performed by standard immersed avatars. We compare desktop avatar control to headset-based control, suggesting users consider the enhanced desktop avatar to be comparably human-like to the VR headset condition, implying that our methods could be useful for future cross-reality remote learning tools.</p>
                        </div>
                    </div>
                </div>
            
        
    

    
    
    
    
    
    
    

    <h4 id="I1002">UrbanRama: Navigating Cities in Virtual Reality</h4>
    <p><strong><small>Invited Journal</small></strong></p>

    

    

    

    
        
    
        
    
        
            
                <p><i>Shaoyu Chen, Fabio Miranda, Nivan Ferreira, Marcos Lage, Harish Doraiswamy, Corinne Brenner, Connor Defanti, Michael Koutsoubis, Luc Wilson, Kenneth Perlin, Claudio T Silva</i></p>
            
            
                <p><small>URL: <a href="https://doi.org/10.1109/TVCG.2021.3099012" target="_blank">https://doi.org/10.1109/TVCG.2021.3099012</a></small></p>
            
            
                <div id="I1002" class="wrap-collabsible"> <input id="collapsibleI1002" class="toggle" type="checkbox" /> <label for="collapsibleI1002" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Exploring large virtual environments, such as cities, is a central task in several domains, such as gaming and urban planning. VR systems can greatly help this task by providing an immersive experience&#59; however, a common issue with viewing and navigating a city in the traditional sense is that users can either obtain a local or a global view, but not both at the same time, requiring them to continuously switch between perspectives, losing context and distracting them from their analysis. In this paper, our goal is to allow users to navigate to points of interest without changing perspectives. To accomplish this, we design an intuitive navigation interface that takes advantage of the strong sense of spatial presence provided by VR. We supplement this interface with a perspective that warps the environment, called UrbanRama, based on a cylindrical projection, providing a mix of local and global views. The design of this interface was performed as an interactive process in collaboration with architects and urban planners. We conducted a qualitative and a quantitative pilot user study to evaluate UrbanRama and the results indicate the effectiveness of our system in reducing perspective changes, while ensuring that the warping doesnt affect distance and orientation perception.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h2 id="2.7">Session: Haptics</h2>
    

    
    
    <p><strong>Wednesday, March 16, 8:30, NZDT UTC+13</strong></p>
    
    
        <!-- TAKE ME TO THE EVENT START -->
    <!--
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    -->
    <!-- TAKE ME TO THE EVENT END-->
    
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1101">Tapping with a Handheld Stick in VR: Redirection Detection Thresholds for Passive Haptic Feedback</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1101" class="wrap-collabsible"> <input id="collapsibleC1101" class="toggle" type="checkbox" /> <label for="collapsibleC1101" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This paper investigates providing grounded passive haptic feedback to a user of a VR application through a handheld stick with which the user taps virtual objects. Two haptics redirection methods are proposed: the DriftingHand method, which alters the position of the user's virtual hand, and the VariStick method, which alters the length of the virtual stick. Detection thresholds were measured in a user study (N = 60) for multiple stick lengths and multiple distances from the user to the real object. VariStick and DriftingHand provide an undetectable range of offsets of &#91;-20cm,+13cm&#93; and &#91;-11cm, +11cm&#93;, respectively.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    

    <h4 id="C1145">STROE: An Ungrounded String-Based Weight Simulation Device</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1145" class="wrap-collabsible"> <input id="collapsibleC1145" class="toggle" type="checkbox" /> <label for="collapsibleC1145" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We present STROE, a new ungrounded string-based weight simulation device. STROE is worn as an add-on to a shoe that in turn is connected to the user's hand via a controllable string. A motor is pulling the string with a force according to the weight to be simulated. The design of STROE allows the users to move more freely than other state-of-the-art devices for weight simulation. We conducted a user study that empirically shows that STROE is able to simulate the weight of various objects and, in doing so, increases users' perceived realism and immersion of VR scenes.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    

    <h4 id="C1150">LevelEd SR: A Substitutional Reality Level Design Workflow</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1150" class="wrap-collabsible"> <input id="collapsibleC1150" class="toggle" type="checkbox" /> <label for="collapsibleC1150" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We present LevelEd SR, a substitutional reality level design workflow that combines AR and VR systems and is built for consumer devices. The system enables passive haptics through the inclusion of physical objects from within a space into a virtual world. A validation study produced quantitative data that suggests players benefit from passive haptics in VR games with an improved game experience and increased levels of presence. An evaluation found that participants were accepting of the system, rating it positively using the System Usability Scale questionnaire and would want to use it again to experience substitutional reality.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1206">Body Warping Versus Change Blindness Remapping: A Comparison of Two Approaches to Repurposing Haptic Proxies for Virtual Reality</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1206" class="wrap-collabsible"> <input id="collapsibleC1206" class="toggle" type="checkbox" /> <label for="collapsibleC1206" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This paper details a study comparing two techniques for repurposing haptic proxies&#59; namely haptic retargeting based on body warping and change blindness remapping. Participants performed a simple button-pressing task, and 24 virtual buttons were mapped onto four haptic proxies with varying degrees of misalignment. Body warping and change blindness remapping were used to realign the real and virtual buttons, and the results indicate that users failed to reliably detect realignment of up to 7.9 cm for body warping and up to 9.7 cm for change blindness remapping. Moreover, change blindness remapping yielded significantly higher self-reported agency, and marginally higher ownership.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1298">&quot;Let;s Meet and Work It Out&quot;: Understanding and Mitigating Encountered-Type of Haptic Devices Failure Modes in VR</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1298" class="wrap-collabsible"> <input id="collapsibleC1298" class="toggle" type="checkbox" /> <label for="collapsibleC1298" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Encountered-type of Haptic devices (ETHD) are robotic interfaces physically overlaying virtual counterparts prior to a user interaction in Virtual Reality. They theoretically reliably provide haptics in Virtual environments, yet they raise several intrinsic design challenges to properly display rich haptic feedback and interactions in VR applications. In this paper, we use a Failure Mode and Effects Analysis (FMEA) approach to identify, organise and analyse the failure modes and their causes in the different stages of an ETHD scenario and highlight appropriate solutions from the literature to mitigate them. We help justify these interfaces' lack of deployment, to ultimately identify guidelines for future ETHD designers.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1492">Exploring Pseudo-Weight in Augmented Reality Extended Displays</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1492" class="wrap-collabsible"> <input id="collapsibleC1492" class="toggle" type="checkbox" /> <label for="collapsibleC1492" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Augmented reality (AR) allows us to wear virtual displays that are registered to our bodies and devices. Such virtually extendable displays, or AR extended displays (AREDs), are personal and free from physical restrictions. Existing work has explored the new design space for improved user performance. Contrary to this direction, we focus on the weight that the user perceives from AREDs, even though they are virtual and have no physical weight. Our results show evidence that AREDs can be a source of pseudo-weight. We also systematically evaluate the perceived weight changes depending on the layout and delay in the visualization system.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    
    
    

    <h2 id="2.8">Session: Computer Vision</h2>
    

    
    
    <p><strong>Wednesday, March 16, 13:00, NZDT UTC+13</strong></p>
    
    
        <!-- TAKE ME TO THE EVENT START -->
    <!--
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    -->
    <!-- TAKE ME TO THE EVENT END-->
    
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1309">Real-Time Gaze Tracking with Event-Driven Eye Segmentation</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1309" class="wrap-collabsible"> <input id="collapsibleC1309" class="toggle" type="checkbox" /> <label for="collapsibleC1309" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Gaze tracking is an essential component in X-Reality. Modern gaze tracking algorithms are heavyweight&#59; they operate at most 5 Hz on mobile processors despite that cameras can operate at a real-time rate (&gt; 30 Hz). This paper presents a real-time eye tracking algorithm that can operate at 30 Hz on a mobile processor, achieves 0.1&#176;-0.5&#176; gaze accuracies, all the while requiring one to two orders of magnitude smaller parameters than state-of-the-art eye tracking algorithms. The key is an Auto ROI mode, which continuously predicts and processes only the Regions of Interest (ROIs) of near-eye images. In particular, we discuss how to accurately predict ROI by emulating an event camera without requiring special hardware support.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1416">Structured Light of Flickering Patterns Having Different Frequencies for a Projector-Event-Camera System</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1416" class="wrap-collabsible"> <input id="collapsibleC1416" class="toggle" type="checkbox" /> <label for="collapsibleC1416" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Our objective is to realize a projector-camera system that combines the event camera with a projector under the strong ambient light. Specifically, this study proposes a new structured light that combines different frequencies of flickers to acquire the correspondence between the image pixels of the event camera and the projector. This method does not rely on the co-axial frame-based measurement and synchronization mechanism between projector and camera and is thus applicable to most general event cameras. Experiments confirm that the proposed method obtains the correspondence robustly with reasonable accuracy in a bright room (up to 2,600 lux) under general indoor lighting and additional light projection.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="J1151">Instant Reality: Gaze-Contingent Perceptual Optimization for 3D Virtual Reality Streaming</h4>
    <p><strong><small>Journal</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Shaoyu Chen, Budmonde Duinkharjav, Xin Sun, Li-Yi Wei, Stefano Petrangeli, Jose Echevarria, Claudio Silva, Qi Sun</i></p>
            
            
                <p><small>URL: <a href="https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150522" target="_blank">https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150522</a></small></p>
            
            
                <div id="J1151" class="wrap-collabsible"> <input id="collapsibleJ1151" class="toggle" type="checkbox" /> <label for="collapsibleJ1151" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>To advance VR applications in a cloud-edge setting, we propose a perceptually-optimized progressive 3D streaming method for spatial quality and temporal consistency. Our model schedules the streaming assets for optimal spatial-temporal quality based on human visual mechanisms. Subjective studies and objective analysis demonstrate the framework's enhanced visual quality and temporal consistency than alternative solutions. We envision our framework allowing future efficient immersive streaming applications without compromising high visual quality and interactivity, such as those in esports and teleconference.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    

    <h4 id="J1232">Prepare for Ludicrous Speed: Marker-based Instantaneous Binocular Rolling Shutter Localization</h4>
    <p><strong><small>Journal</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Juan Carlos Dibene Simental, Yazmin Maldonado, Leonardo Trujillo, Enrique Dunn</i></p>
            
            
                <p><small>URL: <a href="https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150485" target="_blank">https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150485</a></small></p>
            
            
                <div id="J1232" class="wrap-collabsible"> <input id="collapsibleJ1232" class="toggle" type="checkbox" /> <label for="collapsibleJ1232" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We propose a marker-based geometric framework for the high-frequency absolute pose estimation of a binocular camera system by using the data captured during the exposure of a single rolling shutter scanline. We leverage the projective invariants of a planar pattern to define a geometric reference and determine 2D-3D correspondences from edge measurements in individual scanlines. To tackle the ensuing multi-view estimation problem, achieve real-time operation, and minimize latency, we develop a pair of custom solvers leveraging our geometric setup. We demonstrate the effectiveness of our approach with an FPGA implementation achieving a localization throughput of 129.6 KHz with 1.5us latency.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    

    <h4 id="J1239">Robust Tightly-Coupled Visual-Inertial Odometry with Pre-built Maps in High Latency Situations</h4>
    <p><strong><small>Journal</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Hujun Bao, Weijian Xie, Quanhao Qian, Danpeng Chen, Shangjin Zhai, Nan Wang, Guofeng Zhang</i></p>
            
            
                <p><small>URL: <a href="https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150495" target="_blank">https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150495</a></small></p>
            
            
                <div id="J1239" class="wrap-collabsible"> <input id="collapsibleJ1239" class="toggle" type="checkbox" /> <label for="collapsibleJ1239" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>In this paper, we present a novel monocular visual-inertial odometry system with the pre-built maps deployed on the remote server. By coupling VIO with geometric priors from pre-built maps, our system can tolerate the high latency and low frequency of global localization service. Firstly, sparse point clouds are obtained from the dense mesh according to the localization results. The sparse point clouds are directly used for feature tracking and state update of VIO to suppress the drift accumulation. Both the experiments on datasets and the real-time AR demo show that our method outperforms the state-of-the-art methods.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h2 id="3.7">Session: Locomotion (Europe)</h2>
    

    
    
    <p><strong>Wednesday, March 16, 8:30, NZDT UTC+13</strong></p>
    
    
        <!-- TAKE ME TO THE EVENT START -->
    <!--
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    -->
    <!-- TAKE ME TO THE EVENT END-->
    
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1176">Foldable Spaces: An Overt Redirection Approach for Natural Walking in Virtual Reality</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1176" class="wrap-collabsible"> <input id="collapsibleC1176" class="toggle" type="checkbox" /> <label for="collapsibleC1176" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We propose Foldable Spaces, a novel overt redirection approach that dynamically 'folds' the geometry of the virtual environment to enable natural walking. Based on this approach, we developed three techniques: (1) Horizontal, which folds virtual space like the pages in a book&#59; (2) Vertical, which rotates virtual space along a vertical axis&#59; and (3) Accordion, which corrugates virtual space to bring faraway places closer to the user. In a within-subjects study, we compared our foldable techniques along with a base condition, Stop &amp; Reset. Our findings show that Accordion was the most continuous and preferred technique for experiencing natural walking.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1215">Design and Evaluation of Travel and Orientation Techniques for Desk VR</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1215" class="wrap-collabsible"> <input id="collapsibleC1215" class="toggle" type="checkbox" /> <label for="collapsibleC1215" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Typical VR interactions can be tiring, resulting in decreased comfort and session duration compared with traditional non-VR interfaces, which may, in turn, reduce productivity. Desk VR experiences provide the convenience and comfort of a desktop experience and the benefits of VR immersion. We explore travel and orientations techniques targeted at desk VR users, using both controllers and a large multi-touch surface. Results revealed advantages for a continuous controller-based travel method and a trend for a dragging-based orientation technique. Also, we identified possible trends towards task focus affecting overall cybersickness symptomatology.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1368">Eye Tracking-based LSTM for Locomotion Prediction in VR</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1368" class="wrap-collabsible"> <input id="collapsibleC1368" class="toggle" type="checkbox" /> <label for="collapsibleC1368" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Virtual Reality allows users to perform natural movements such as walking in virtual environments. This comes with the need for a large tracking space. To optimise use of the physical space, prediction models for upcoming behavior are helpful. Here we examined whether eye movements can improve such predictions. Eighteen participants walked through a virtual environment while performing different tasks. The recorded data were used to train an LSTM model. We found that positions 2.5s into the future can be predicted with an average error of 65cm. The benefit of eye movement data depended on task and environment. Situations with changes in walking speed benefited from the inclusion of eye data.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    

    <h4 id="C1386">The Chaotic Behavior of Redirection - Revisiting Simulations in Redirected Walking</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1386" class="wrap-collabsible"> <input id="collapsibleC1386" class="toggle" type="checkbox" /> <label for="collapsibleC1386" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Redirected Walking is a common technique to allow real walking for exploring large virtual environments in constrained physical spaces. Many existing approaches were evaluated in simulation only, and researchers argued that the findings would translate to real scenarios to motivate the effectiveness of their algorithms. In this paper, we argue that simulation-based evaluations require critical reflection. We demonstrate simulations that show the chaotic process fundamental to RDW, in which altering the initial user's position by mere millimeters can drastically change the resulting steering behavior. This insight suggests that redirection is more sensitive to underlying data than previously assumed.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1512">How to Take a Brake from Embodied Locomotion - Seamless Status Control Methods for Seated Leaning Interfaces</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1512" class="wrap-collabsible"> <input id="collapsibleC1512" class="toggle" type="checkbox" /> <label for="collapsibleC1512" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Embodied locomotion, especially leaning, has one major problem. Effectively the regular functionality of the utilized body parts is overwritten. Thus, in this work, we propose 6 different status control methods that seamlessly switch off (brake) a seated leaning locomotion interface. Different input modalities, such as a physical button, voice, and gestures/metaphors, are used and evaluated against a baseline condition and a leaning interface with a bilateral transfer function. In a user study, the most diegetic interface, a hoverboard metaphor, was the most preferred. The overall results are more heterogeneous and the interfaces vary in their suitability for different scenarios.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    
    
    

    <h2 id="3.8">Session: Negative Effects</h2>
    

    
    
    <p><strong>Wednesday, March 16, 13:00, NZDT UTC+13</strong></p>
    
    
        <!-- TAKE ME TO THE EVENT START -->
    <!--
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    -->
    <!-- TAKE ME TO THE EVENT END-->
    
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1110">Asymmetric Lateral Field-of-View Restriction to Mitigate Cybersickness During Virtual Turns</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1110" class="wrap-collabsible"> <input id="collapsibleC1110" class="toggle" type="checkbox" /> <label for="collapsibleC1110" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We propose and evaluate a novel variant of field-of-view restriction that uses an asymmetric mask to obscure only one side region of the periphery during rotation and laterally shifts the center of restriction towards the direction of the turn. A between-subjects study was conducted to compare the side restrictor, a traditional symmetric restrictor, and a control condition without restriction. The side restrictor was effective in mitigating cybersickness, reducing discomfort, improving subjective visibility, and enabling longer immersion time. These results indicate that side field-of-view restriction is an effective cybersickness mitigation technique for virtual environments with frequent turns.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1320">You're in for a Bumpy Ride! Uneven Terrain Increases Cybersickness While Navigating with Head Mounted Displays</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1320" class="wrap-collabsible"> <input id="collapsibleC1320" class="toggle" type="checkbox" /> <label for="collapsibleC1320" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Cybersickness poses a challenge to the broader adoption of virtual reality technologies. In this study, we examine the impact of uneven virtual terrain traversal on cybersickness in VR. We recruited 38 participants to navigate with three virtual terrain variants: flat surface, regular bumps, and terrain generated from Perlin noise. We collected cybersickness data using the Fast Motion Sickness Scale, Simulator Sickness Questionnaire, and galvanic skin response. Our results indicate that users felt greater cybersickness when traversing uneven terrain than they did with flat geometry. We recommend that designers exercise caution when incorporating uneven terrain into their virtual experiences.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1482">Answering With Bow and Arrow: Questionnaires and VR Blend Without Distorting the Outcome</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1482" class="wrap-collabsible"> <input id="collapsibleC1482" class="toggle" type="checkbox" /> <label for="collapsibleC1482" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Negative effects generated by transitioning between physical and virtual reality or time spent between the actual sensation and its measurement when using questionnaires might affect subjective measurements. This motivates research on innovative questionnaire moalities. Using the same interaction technique, this study integrates the answering of the questionnaire into the actual task. We made a bow and arrow game where the player shoots at random targets as quickly as possible. The player then had to answer questionnaires by firing at a target representing the rating. Notably, the presence (SUS-PQ), satisfaction (ASQ), and workload (SMEQ) evaluations did not change across questionnaires presented in VR, text panel VR, or desktop PC version.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    

    <h4 id="C1486">Systematic Design Space Exploration of Discrete Virtual Rotations in VR</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1486" class="wrap-collabsible"> <input id="collapsibleC1486" class="toggle" type="checkbox" /> <label for="collapsibleC1486" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Continuous virtual rotation is one of the biggest contributors to cybersickness, while simultaneously being necessary for many VR scenarios where the user is limited in physical body rotation. A solution is discrete virtual rotation. We classify existing work in discrete virtual rotation and systematically investigate the two dimensions target (rotation) acquisition (selection vs. directional) and body-based (yes vs. no) regarding their impact on the performance in a naive and a primed rotational search task, spatial orientation, and usability. We find rotation selection most successful in both search tasks and no difference in the factor body-based on spatial orientation.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="C1559">Auditory Feedback for Standing Balance Improvement in Virtual Reality</h4>
    <p><strong><small>Conference</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Author information coming soon</i></p>
            
            
            
                <div id="C1559" class="wrap-collabsible"> <input id="collapsibleC1559" class="toggle" type="checkbox" /> <label for="collapsibleC1559" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Virtual Reality (VR) users often experience postural instability which could be a major barrier to universal usability for all, especially for persons with balance impairments. We recruited 42 participants (balance impairments: 21, without balance impairments: 21) to investigate the impact of several auditory techniques on balance in VR. Participants performed standing visual exploration and standing reach and grasp tasks. Results showed that each auditory technique improved balance in VR for all. Spatial and CoP audio improved balance significantly more than other audios. The techniques presented in this research could be used in future virtual environments to improve standing balance.</p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    <h4 id="J1302">Omnidirectional Galvanic Vestibular Stimulation in Virtual Reality</h4>
    <p><strong><small>Journal</small></strong></p>

    

    

    

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <p><i>Colin Groth, Jan-Philipp Tauscher, Nikkel Heesen, Max Hattenbach, Susana Castillo, Marcus Magnor</i></p>
            
            
                <p><small>URL: <a href="https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150506" target="_blank">https://doi.ieeecomputersociety.org/10.1109/TVCG.2022.3150506</a></small></p>
            
            
                <div id="J1302" class="wrap-collabsible"> <input id="collapsibleJ1302" class="toggle" type="checkbox" /> <label for="collapsibleJ1302" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Cybersickness often taints virtual experiences. Its source can be associated to the perceptual mismatch happening when our eyes tell us we are moving while we are, in fact, at rest. We reconcile the signals from our senses by using omnidirectional galvanic vestibular stimulation (GVS), stimulating the vestibular canals behind our ears with low-current electrical signals specifically attuned to the visually displayed camera motion. We describe how to calibrate and generate the appropriate GVS signals in real-time for pre-recorded omnidirectional videos exhibiting ego-motion in all three spatial directions, and prove that it significantly reduces discomfort for cybersickness-susceptible VR users. </p>
                        </div>
                    </div>
                </div>
            
        
    
        
    
        
    
        
    
        
    
        
    

    
    
    
    
    
    
    
    
    
    
    
    

    
    
</div>

<!--
<div id="S1">
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
</div>
-->



                </div>
                <div class="column center" style=""></div>

                <!-- Main Content -->
                <div class="column right" style="text-align: center">
            
                    <!-- <img class="sidebar-header-icon" src="/assets/images/ieeevr-logo.jpg">-->
                    <!-- <a class="twitter-timeline" data-height="450" href="https://twitter.com/IEEEVR?ref_src=twsrc%5Etfw">Tweets by IEEEVR</a> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> -->
                    <div>
                        <ul class="social-icons">
                            <li><a href="https://facebook.com/ieeevr"><img src=/assets/images/social/circle-color/Facebook.png alt="Facebook" /></a></li>
                            <li><a href="http://www.twitter.com/ieeevr"><img src=/assets/images/social/circle-color/Twitter.png alt='Twitter' /></a></li>
                        </ul>
                    </div>

                    <div class="confsponsors" id="sponsors">
                        <h4 style="border-bottom: 1px solid black;padding-bottom: 5px;">Conference Sponsors</h4>

                        <div style="background-color: #fec10d; border-radius: 7px;">
                            <b style="color: #363636">Diamond</b>
                        </div>
                        <br>
                            <a href="https://www.virbela.com/" target="_blank">
                                <img class="conf-icon" style="width: 90%;" src=/assets/images/sponsors/Virbela.png alt="Virbela Logo. Their name next to a stylised green, red, and blue circle."></a>
                            <br>
                        <br>
                        <div style="background-color: #fec10d; border-radius: 7px;">
                            <b style="color: #363636">Gold</b>
                        </div>
                        <br>
                            <a href="https://www.christchurchnz.com/" target="_blank">
                            <img class="conf-icon" style="width: 90%;" src=/assets/images/sponsors/ChristchurchNZ_Logo.png alt="ChristchurchNZ Logo. Their name is written in a red font."></a>
                            <br>
                            <br>
                            <a href="https://immersivelrn.org/" target="_blank">
                            <img class="conf-icon" style="width: 90%;" src=/assets/images/sponsors/iLRN.png alt="iLRN Logo. Their name is written in an orange font."></a>
                            <br>
                            <a href="https://www.canterbury.ac.nz/" target="_blank">
                            <img class="conf-icon" style="width: 90%;" src=/assets/images/sponsors/UCRed_RGB.jpg alt="University of Canterbury Logo. Their name is written in a red font."></a>
                            <br>
                         <br>
                        <div style="background-color: #fec10d; border-radius: 7px;">
                            <b style="color: #363636">Silver</b>
                        </div>
                        <br>
                            <a href="https://www.qualcomm.com/research/extended-reality" target="_blank">
                            <img class="conf-icon" style="width: 90%;" src=/assets/images/sponsors/Qualcomm.png alt="The Qualcomm logo. Their name is written in a blue font."></a>
                            <br>
                        <br>
                        <div style="background-color: #fec10d; border-radius: 7px;">
                            <b style="color: #363636">Bronze</b>
                        </div>
                        <br>
                            <a href="https://www.hitlabnz.org/" target="_blank">
                                <img class="conf-icon" style="width: 90%;" src=/assets/images/sponsors/HITL-AIGI.jpg alt="HITLab NZ logo. A Kiwi wearing a VR headset.">
                            </a>
                            <br>
                        <br>

                        <h4 style="border-bottom: 1px solid black;padding-bottom: 5px;">Supporters</h4>
                            <a href="https://www.mdpi.com/journal/mti" target="_blank">
                                <img class="conf-icon" style="width: 90%;" src=/assets/images/sponsors/MTI.png alt="Multimodal Technologies and Interaction logo. Their name is written next to a stylised blue M.">
                            </a>
                        <br>
                        <!--<a href="https://gpcg.pt/website/en/" target="_blank">
                            <img class="conf-icon" style="width: 95%;padding-top: 5px;" src=/assets/images/sponsors/CGILogo.pngalt="GPCG Logo">
                        </a>
                        <br>-->

                        <h4 style="border-bottom: 1px solid black;padding-bottom: 5px;">Doctoral Consortium Sponsors</h4>
                        <!--<a href="https://www.nsf.gov" target="_blank">
                            <img class="conf-icon" style="width: 65%;" src=/assets/images/sponsors/nsf.jpg alt="NSF Logo">
                        </a>
                        <br>-->

                        <h4 style="border-bottom: 1px solid black;padding-bottom: 5px;">Conference Partner</h4>
                        <!--<a href="https://www.cioapplicationseurope.com" target="_blank">
                            <img class="conf-icon" style="width: 90%;" src=/assets/images/sponsors/cio-applications-europe.png alt="CIO Applications Europe Website">
                        </a>
                        <br>
                        <br>-->
                    </div>

                </div>
            </div>

            <!--  footer -->
            <div class="ieeevrfooter">
                <hr>
                <small><a href=/attend/code-of-conduct/>Code of Conduct</a></small><br/>
                <p style="text-align:center ! important;">© IEEEVR Conference</p>
            </div>
        </section>

    </article>
</div>

    </div>

      
      

    

    

  </body>
</html>
